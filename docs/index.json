[{"uri":"https://gardener.cloud/documentation/home/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/api-server/","title":"Gardener API Server","tags":[],"description":"","content":"Gardener API server The Gardener API server is a Kubernetes-native extension based on its aggregation layer. It is registered via an APIService object and designed to run inside a Kubernetes cluster whose API it wants to extend.\nAfter registration, it exposes the following resources:\nCloudProfiles CloudProfiles are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc. Each shoot has to reference a CloudProfile to declare the environment it should be created in. In a CloudProfile the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions he wants to offer, etc. End-users can read CloudProfiles to see these values, but only operators can change the content or create/delete them. When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced CloudProfile.\nAdditionally, a CloudProfile may contain a providerConfig which is a special configuration dedicated for the infrastructure provider. Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.\nPlease see this example manifest and consult the documentation of your provider extension controller to get information about its providerConfig.\nSeeds Seeds are resources that represent seed clusters. Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.11 and passes the Kubernetes conformance tests. The Gardener operator has to either deploy the Gardenlet into the cluster he wants to use as seed (recommended, then the Gardenlet will create the Seed object itself after bootstrapping), or he provides the kubeconfig to the cluster inside a secret (that is referenced by the Seed resource) and creates the Seed resource himself.\nPlease see this, this(, and optionally this) example manifests.\nQuotas In order to allow end-user not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by him that he allows to be used for trial clusters. Trial clusters can be put under quota such that they don\u0026rsquo;t consume too many resources (resulting in costs), and so that one user cannot consume all resources on his own. These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.\nPlease see this example manifest.\nProjects The first thing before creating a shoot cluster is to create a Project. A project is used to group multiple shoot clusters together. End-users can invite colleagues to the project to enable collaboration, and they can either make them admin or viewer. After an end-user has created a project he will get a dedicated namespace in the garden cluster for all his shoots.\nPlease see this example manifest.\nSecretBindings Now that the end-user has a namespace the next step is registering his infrastructure provider account.\nPlease see this example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.\nAfter the secret has been created the end-user has to create a special SecretBinding resource that binds this secret. Later when creating shoot clusters he will reference such a binding.\nPlease see this example manifest.\nShoots Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end. As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well. Such configurations are not evaluated by Gardener (because it doesn\u0026rsquo;t know/understand them), but they are only transported to the respective extension controller.\n:warning: This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).\nPlease see this example manifest and consult the documentation of the provider extension controller to get information about its spec.provider.controlPlaneConfig, .spec.provider.infrastructureConfig, and .spec.provider.workers[].providerConfig.\n(Cluster)OpenIDConnectPresets Please see this separate documentation file.\n"},{"uri":"https://gardener.cloud/documentation/concepts/architecture/","title":"Architecture","tags":[],"description":"","content":"Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it\u0026rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps:\n Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active  Overview Architecture Diagram Note: The kubelet as well as the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/ctrl-registration/","title":"Extension controller registration","tags":[],"description":"","content":"Registering Extension Controllers Extensions are registered in the garden cluster via ControllerRegistration resources. Gardener is evaluating the registrations and creates ControllerInstallation resources which describe the request \u0026ldquo;please install this controller X to this seed Y\u0026rdquo;.\nSimilar to how CloudProfile or Seed resources get into the system, the Gardener administrator must deploy the ControllerRegistration resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).\nThe specification mainly describes which of Gardener\u0026rsquo;s extension CRDs are managed, for example:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:os-coreosspec:resources:- kind:OperatingSystemConfigtype:coreosprimary:trueThis information tells Gardener that there is an extension controller that can handle OperatingSystemConfig resources of type coreos.\nAlso, it specifies that this controller is the primary one responsible for the lifecycle of the OperatingSystemConfig resource. Setting primary to false would allow to register additional, secondary controllers that may also watch/react on the OperatingSystemConfig/coreos resources, however, only the primary controller may change/update the main status of the extension object (that are used to \u0026ldquo;communicate\u0026rdquo; with the Gardenlet). Particularly, only the primary controller may set .status.lastOperation, .status.lastError, .status.observedGeneration, and .status.state. Secondary controllers may contribute to the .status.conditions[] if they like, of course.\nSecondary controllers might be helpful in scenarios where additional tasks need to be completed which are not part of the reconciliation logic of the primary controller but separated out into a dedicated extension.\n⚠️ There must be exactly one primary controller for every registered kind/type combination. Also, please note that the primary field cannot be changed after creation of the ControllerRegistration.\nDeploying Extension Controllers Submitting above ControllerRegistration will create a ControllerInstallation resource:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerInstallationmetadata:name:os-coreosspec:registrationRef:apiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationname:os-coreosseedRef:apiVersion:core.gardener.cloud/v1beta1kind:Seedname:aws-eu1This resource expresses that Gardener requires the os-coreos extension controller to run on the aws-eu1 seed cluster.\nThe Gardener Controller Manager does automatically determine which extension is required on which seed cluster and will only create ControllerInstallation objects for those. Also, it will automatically delete ControllerInstallations referencing extension controllers that are no longer required on a seed (e.g., because all shoots on it have been deleted). There are additional configuration options, please see this section.\nHow do extension controllers get deployed to seeds? After Gardener has written the ControllerInstallation resource some component must satisfy this request and start deploying the extension controller to the seed. Depending on the complexity of the controllers lifecycle management, configuration, etc. there are two possible scenarios:\nScenario 1: Deployed by Gardener In many cases the extension controllers are easy to deploy and configure. It is sufficient to simply create a Helm chart (standardized way of packaging software in the Kubernetes context) and deploy it together with some static configuration values. Gardener supports this scenario and allows to provide arbitrary deployment information in the ControllerRegistration resource\u0026rsquo;s .spec section:\n...spec:...deployment:type:helmproviderConfig:chart:H4sIFAAAAAAA/yk...values:foo:barIf .spec.deployment.type=helm then Gardener itself will take over the responsibility the deployment. It base64-decodes the provided Helm chart (.spec.deployment.providerConfig.chart) and deploys it with the provided static configuration (.spec.deployment.providerConfig.values). The chart and the values can be updated at any time - Gardener will recognize and re-trigger the deployment process.\nIn order to allow extensions to get information about the garden and the seed cluster Gardener does mix-in certain properties into the values (root level) of every deployed Helm chart:\ngardener:garden:identifier:\u0026lt;uuid-of-gardener-installation\u0026gt; seed:identifier:\u0026lt;seed-name\u0026gt; region: europespec:\u0026lt;complete-seed-spec\u0026gt;Extensions can use this information in their Helm chart in case they require knowledge about the garden and the seed environment. The list might be extended in the future.\n:information_source: Gardener uses the UUID of the garden Namespace object in the .gardener.garden.identifier property.\nScenario 2: Deployed by a (non-human) Kubernetes operator Some extension controllers might be more complex and require additional domain-specific knowledge wrt. lifecycle or configuration. In this case, we encourage to follow the Kubernetes operator pattern and deploy a dedicated operator for this extension into the garden cluster. The ControllerResource's .spec.deployment.type field would then not be helm, and no Helm chart or values need to be provided there. Instead, the operator itself knows how to deploy the extension into the seed. It must watch ControllerInstallation resources and act one those referencing a ControllerRegistration the operator is responsible for.\nIn order to let Gardener know that the extension controller is ready and running in the seed the ControllerInstallation's .status field supports two conditions: RegistrationValid and InstallationSuccessful - both must be provided by the responsible operator:\n...status:conditions:- lastTransitionTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;message:Chartcouldberenderedsuccessfully.reason:RegistrationValidstatus:\u0026#34;True\u0026#34;type:Valid- lastTransitionTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;message:Installationofnewresourcessucceeded.reason:InstallationSuccessfulstatus:\u0026#34;True\u0026#34;type:InstalledAdditionally, the .status field has a providerStatus section into which the operator can (optionally) put any arbitrary data associated with this installation.\nExtensions in the garden cluster itself The Shoot resource itself will contain some provider-specific data blobs. As a result, some extensions might also want to run in the garden cluster, e.g., to provide ValidatingWebhookConfigurations for validating the correctness of their provider-specific blobs:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:johndoe-awsnamespace:garden-devspec:...cloud:type:awsregion:eu-west-1providerConfig:apiVersion:aws.cloud.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:# specify either \u0026#39;id\u0026#39; or \u0026#39;cidr\u0026#39;# id: vpc-123456cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1a...In the above example, Gardener itself does not understand the AWS-specific provider configuration for the infrastructure. However, if this part of the Shoot resource should be validated then you should run an AWS-specific component in the garden cluster that registers a webhook. You can do it similarly if you want to default some fields of a resource (by using a MutatingWebhookConfiguration).\nAgain, similar to how Gardener is deployed to the garden cluster, these components must be deployed and managed by the Gardener administrator.\nExtension resource configurations The Extension resource allows to inject arbitrary steps into the shoot reconciliation flow that are unknown to Gardener. Hence, it is slightly special and allows further configuration when registering it:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-foospec:resources:- kind:Extensiontype:fooprimary:truegloballyEnabled:truereconcileTimeout:30sThe globallyEnabled=true option specifies that the Extension/foo object shall be created by default for all shoots (unless they opted out by setting .spec.extensions[].enabled=false in the Shoot spec).\nThe reconcileTimeout tells Gardener how long it should wait during its shoot reconciliation flow for the Extension/foo's reconciliation to finish.\nDeployment configuration options The .spec.deployment resource allows to configure a deployment policy. There are the following policies:\n OnDemand (default): Gardener will demand the deployment and deletion of the extension controller to/from seed clusters dynamically. It will automatically determine (based on other resources like Shoots) whether it is required and decide accordingly. Always: Gardener will demand the deployment of the extension controller to seed clusters independent of whether it is actually required or not. This might be helpful if you want to add a new component/controller to all seed clusters by default. Another use-case is to minimize the durations until extension controllers are deployed and ready in case you have highly fluctuating seed clusters.  Also, the .spec.deployment.seedSelector allows to specify a label selector for seed clusters. Only if it matches the labels of a seed then it will be deployed to it. Please note that a seed selector can only be specified for secondary controllers (primary=false for all .spec.resources[]).\n"},{"uri":"https://gardener.cloud/documentation/concepts/core-components/scheduler/","title":"Gardener Scheduler","tags":[],"description":"","content":"Gardener scheduler The Gardener scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them. Conceptually, the task of the Gardener scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.\nEither the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating. The following sections explain the configuration and flow in greater detail.\nWhy is the Gardener scheduler needed? 1. Decoupling Previously, an admission plugin in the Gardener API server conducted the scheduling decisions. This implies changes to the API server whenever adjustments of the scheduling are needed. Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.\n2. Extensibility It should be possible to easily extend and tweak the scheduler in the future. Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions. It should be also possible to completely replace the standard Gardener scheduler with a custom implementation.\nConfiguration The Gardener scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag. Here is an example scheduler configuration.\nMost of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, \u0026hellip;). However, the Gardener scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.\nThe scheduling strategy is defined in the candidateDeterminationStrategy and can have the possible values SameRegion and MinimalDistance. The SameRegion strategy is the default strategy.\n1. Same Region strategy The Gardener scheduler reads the spec.provider.type and .spec.region fields from the Shoot resource. It tries to find a Seed that has the identical .spec.provider.type and .spec.provider.region fields set. If it cannot find a suitable Seed, it adds an event to the shoot stating, that it is unschedulable.\n2. Minimal Distance strategy As a first step, the SameRegion strategy is being executed. If no seed in the same region could be found, the scheduler uses a lexicographical approach to determine a suitable seed cluster. This leverages the fact that most cloud providers (except from Azure) use geographically aligned region names. The scheduler takes into consideration the region names of all available seeds in the cluster of the desired infrastructure and picks the regions that match lexicographically the best (starting from the left letter to right letter of the region name). E.g. if the shoots wants a cluster in AWS eu-north-1, the scheduler picks all Seeds in region AWS eu-central-1, because at least the continent “eu-“ matches (even better with region instances like AWS ap-southeast-1 and AWS ap-southeast-2).\nIn the last step, the scheduler picks the one seed having the least shoots currently deployed.\nIn order to put the scheduling decision into effect, the scheduler sends an update request for the Shoot resource to the API server. After validation, the Gardener Aggregated API server updates the shoot to have the spec.seedName field set. Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.\nSpecial handling based on shoot cluster purpose Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see this document for more information).\nIn case the shoot has the testing purpose then the scheduler only reads the .spec.provider.type from the Shoot resource and tries to find a Seed that has the identical .spec.provider.type. The region does not matter, i.e., testing shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.\nFiltering to determine the best candidate The section above has explained which strategies are used to determine the potential seed candidates. Once this list has been computed the scheduler tries to find the best one out of them to which, eventually, the shoot gets assigned to. It filters out Seeds\n whose networks have intersections with the Shoot's networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint) that are tainted with the seed.gardener.cloud/disable-dns taint (only if the shoot specifies a DNS domain or does not use the unmanaged DNS provider) whose labels don\u0026rsquo;t match the .spec.seedSelector field of the CloudProfile that is used in the Shoot (there might be multiple environments for the same provider type, e.g., you might have multiple OpenStack systems connected to Gardener)  After this filtering process the least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the .spec.seedName field of the Shoot.\nseedSelector field in the Shoot specification Similar to the .spec.nodeSelector field in Pods, the Shoot specification has an optional .spec.seedSelector field. It allows the user to provide a label selector that must match the labels of Seeds in order to be scheduled to one of them. The labels on Seeds are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves. If provided, the Gardener scheduler will only consider those seeds as \u0026ldquo;suitable\u0026rdquo; whose labels match those provided in the .spec.seedSelector of the Shoot.\nFailure to determine a suitable seed In case the scheduler fails to find a suitable seed, the operation is being retried with an exponential backoff - starting with the retrySyncPeriod (default of 15s).\nCurrent Limitation / Future Plans  Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the MinimalRegion strategy with a more suitable one in the future. Currently, shoots can only scheduled to seeds from the same cloud provider (spec.provider.type), however that is not a technical limitation and might be changed in the future.  "},{"uri":"https://gardener.cloud/documentation/guides/client_tools/","title":"Set Up Client Tools","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/seed-admission-ctrl/","title":"Gardener Seed Admission Controller","tags":[],"description":"","content":"Gardener Seed Admission Controller The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster. It\u0026rsquo;s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed\u0026rsquo;s API server.\nWhat is it doing concretely? Validating Webhooks Unconfirmed Deletion Prevention As part of Gardener\u0026rsquo;s extensibility concepts a lot of CustomResourceDefinitions are deployed to the seed clusters that serve as extension points for provider-specific controllers. For example, the Infrastructure CRD triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster. Consequently, these extension CRDs have a lot of power and control large portions of the end-user\u0026rsquo;s shoot cluster. Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.\nTogether with the deployment of the Gardener seed admission controller a ValidatingWebhookConfiguration for CustomResourceDefinitions and most (custom) resources in the extensions.gardener.cloud/v1alpha1 API group is registered. It prevents DELETE requests for those CustomResourceDefinitions labeled with gardener.cloud/deletion-protected=true, and for all mentioned custom resources if they were not previously annotated with the confirmation.gardener.cloud/deletion=true. This prevents that undesired kubectl delete \u0026lt;...\u0026gt; requests are accepted.\nMutating Webhooks It doesn\u0026rsquo;t serve any mutating webhooks yet.\n"},{"uri":"https://gardener.cloud/documentation/concepts/","title":"Concepts","tags":[],"description":"Explore the concepts on which Gardener is built","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/","title":"Core Components","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/gardenlet/","title":"Gardenlet","tags":[],"description":"","content":"Gardenlet Right from the beginning of the Gardener Project we started implementing the operator pattern: We have a custom controller-manager that acts on our own custom resources. Now, when you start thinking about the Gardener architecture, you will recognize some interesting similarity with respect to the Kubernetes architecture: Shoot clusters can be compared with pods, and seed clusters can be seen as worker nodes. Guided by this observation we introduced the gardener-scheduler (#356). Its main task is to find an appropriate seed cluster to host the control-plane for newly ordered clusters, similar to how the kube-scheduler finds an appropriate node for newly created pods. By providing multiple seed clusters for a region (or provider) and distributing the workload, we reduce the blast-radius of potential hick-ups as well.\nYet, there was still a significant difference between the Kubernetes and the Gardener architectures: Kubernetes runs a primary \u0026ldquo;agent\u0026rdquo; on every node, the kubelet, which is mainly responsible for managing pods and containers on its particular node. Gardener used its single controller-manager which was responsible for all shoot clusters on all seed clusters, and it was performing its reconciliation loops centrally from the garden cluster.\nWhile this works well at scale for thousands of clusters today, our goal is to enable true scalability following the Kubernetes principles (beyond the capacity of a single controller-manager): We have now worked on distributing the logic (or the Gardener operator) into the seed cluster and introduced a corresponding component, adequately named the Gardenlet. It is Gardener\u0026rsquo;s primary \u0026ldquo;agent\u0026rdquo; on every seed cluster and is only responsible for shoot clusters located in its particular seed cluster.\nThe gardener-controller-manager still kept its control loops for other resources of the Gardener API, however, it does no longer talk to seed/shoot clusters.\nReversing the control flow will even allow placing seed/shoot clusters behind firewalls without the necessity of direct accessibility (via VPN tunnels) anymore.\nTLS Bootstrapping Kubernetes does not manage worker nodes itself, and it is also not responsible for the lifecycle of the kubelet running on the workers. Similarly, Gardener does not manage seed clusters itself (although, it is possible that you create a shoot cluster that you later register a seed again, of course), hence, Gardener is also not responsible for the lifecycle of the Gardenlet running on the seeds.\nAs explained in the above motivation, the Gardenlet can be compared with the kubelet. After you depoyed it yourself into your seed clusters, it initializes a bootstrapping process that is very similar to the Kubelet\u0026rsquo;s TLS bootstrapping:\n Gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest resources After the CSR is signed it downloads the created client certificate, creates a new kubeconfig with it, and stores it inside a Secret in the seed cluster It deletes the bootstrap kubeconfig secret and starts up with its new kubeconfig  Basically, you can follow the Kubernetes documentation regarding the process. The gardener-controller-manager runs a control loop that automatically signs CSRs created by Gardenlets.\n:warning: Certificate rotation is not yet implemented but will follow very soon.\nOptionally, if you don\u0026rsquo;t want to run this bootstrap process, then you can create a Kubeconfig pointing to the Garden cluster for the Gardenlet yourself, and simply provide it to it.\nSeed Config vs. Seed Selector While the kubelet has to run on the worker node directly in order to talk to the container runtime and to manage the lifecycle of containers, the Gardenlet can potentially run outside of the seed cluster as long as it can talk to the seed\u0026rsquo;s API server. Also, it\u0026rsquo;s possible that the Gardenlet controls more than one seed. Theoretically, if network connectivity is available, a Gardenlet installed inside the Garden cluster could be responsible all seed clusters in the system, just like the gardener-controller-manager back in previous Gardener versions. However, this scenario has been mainly implemented for development purposes.\nFor production use, though, mainly motivated with scalability arguments and a better distribution of responsibilities, it is recommend to run one Gardenlet per seed inside the seed cluster itself.\nIf you want the Gardenlet in the standard way then please provide a seedConfig that contains information about the seed cluster itself, see the example Gardenlet configuration. Once bootstrapped, the Gardenlet will create and update its Seed object itself.\nIf you want the Gardenlet to manage multiple seeds then please provide a seedSelector that incorporates a label selector for the targeted Seeds, see the example Gardenlet configuration. In this case, you have to create the Seed objects (together with a kubeconfig pointing to the cluster) yourself (like with previous Gardener versions).\nComponent Configuration You can find an example configuration file here.\nBasically, it is possible to define settings for the Kubernetes clients interacting with the various clusters, settings for the control loops inside the Gardenlet, settings for leader election and log levels, feature gates, and seed selection/configuration.\nMost of the configuration options are similar to what the gardener-controller-manager offered with previous Gardener versions.\nHeartbeats Similar to how Kubernetes is meanwhile using Lease objects for node heart beats (see KEP), the Gardenlet is using Leases objects for seed heart beats. Every two seconds it is checking its connectivity to the seed and then it renews its lease. The status will be reported in the GardenletReady condition in its Seed object(s). Similarly to the node-lifecycle-controller inside the kube-controller-manager, the gardener-controller-manager features a seed-lifecycle-controller that will set the GardenletReady condition to Unknown in case the Gardenlet stops sending its heartbeat signals. Carrying on, this will make the gardener-scheduler not considering this seed for newly created shoots anymore.\n/healthz Endpoint gardener/gardener#2309 has enhanced the Gardenlet with a HTTPS server that serves a /healthz endpoint. It is used as a liveness probe in the Deployment of the Gardenlet. If the Gardenlet fails trying to renew its lease then the endpoint will return 500 Internal Server Error, otherwise it will return 200 OK.\n⚠️ In case the Gardenlet is managing mutliple seeds (i.e., a seed selector is used) then the /healthz will report 500 Internal Server Error if there is at least one seed for which it could not renew its lease. Only if it can renew the lease for all seeds then it will report 200 OK.\nShooted Seeds If the Gardenlet manages a shoot cluster that has been marked to be used as seed then it will automatically deploy itself into the cluster, unless you prevent this by using the no-gardenlet configuration in the shoot.gardener.cloud/use-as-seed annotation (in this case, you have to deploy the Gardenlet on your own into the seed cluster).\nExample: Annotate the shoot with shoot.gardener.cloud/use-as-seed=\u0026quot;true,no-gardenlet,invisible\u0026quot; to mark it as invisible (meaning, that the gardener-scheduler won\u0026rsquo;t consider it) and to express your desire to deploy the gardenlet into the cluster on your own.\nFor automatic deployments, the Gardenlet will use the same version and the same configuration for the clone it deploys.\nMigrating from previous Gardener versions You have to make sure that your Garden cluster is exposed in a way that it is reachable from all your seed clusters. Otherwise, you cannot upgrade Gardener to v1. Also, you should at least Gardener 0.31 before upgrading to v1.\nApart from these constraints, there is no special migration task required. With previous Gardener versions, you had deployed the Gardener Helm chart (incorporating the API server, controller-manager, and scheduler). With v1, this will stay the same, but you now have to deploy the Gardenlet Helm chart as well (into all of your seed (if they are not shooted, see above)).\nPlease follow the general deployment guide for all instructions.\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/","title":"Install Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/","title":"Administer Client (Shoot) Clusters","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/backup-restore/","title":"Backup and Restore","tags":[],"description":"","content":"Backup and restore Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.\nGardener uses etcd-backup-restore component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via etcd-druid. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer GEP-06 and documentation on individual repository.\nBucket provisioning Refer the backup bucket extension document to know details about configuring backup bucket.\nBackup Policy etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:\n Full Snapshot Schedule:  Daily, 24hr interval. For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.   Delta Snapshot schedule:  At 5min interval. If aggregated events size since last snapshot goes beyond 100Mib.   Backup History / Garbage backup deletion policy:  Gardener configure backup restore to have Exponential garbage collection policy. As per policy, following backups are retained. All full backups and delta backups for the previous hour. Latest full snapshot of each previous hour for the day. Latest full snapshot of each previous day for 7 days. Latest full snapshot of the previous 4 weeks. Garbage Collection is configured at 12hr interval.   Listing:  Gardener don\u0026rsquo;t have any API to list out the backups. To find the backup list, admin can checkout the BackupEntry resource associated with Shoot which holds the bucket and prefix details on object store.    Restoration Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen\u0026rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the doc. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.\nAgain, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster\u0026rsquo;s etcd.\n"},{"uri":"https://gardener.cloud/documentation/guides/","title":"How-To Guides","tags":[],"description":"Learn how to execute concrete tasks","content":""},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/","title":"Monitor and Troubleshoot","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/networking/network-policies/","title":"Network Policies","tags":[],"description":"","content":"Network Policies in Gardener As Seed clusters can host the Kubernetes control planes of many Shoot clusters, it is necessary to isolate the control planes from each other for security reasons. Besides deploying each control plane in its own namespace, Gardener creates network policies to also isolate the networks. Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to. As such, network policies are an important part of Gardener\u0026rsquo;s tenant isolation.\nGardener deploys network policies into\n each namespace hosting the Kubernetes control plane of the Shoot cluster. the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called garden and contains e.g. the Gardenlet. the kube-system namespace in the Shoot.  The aforementioned namespaces in the Seed contain a deny-all network policy that denies all ingress and egress traffic. This secure by default setting requires pods to whitelist network traffic. This is done by pods having labels matching to the selectors of the network policies deployed by Gardener.\nMore details on the deployed network policies can be found in the development and usage sections.\n"},{"uri":"https://gardener.cloud/documentation/tutorials/","title":"Tutorials","tags":[],"description":"Walkthroughs of common use case implementations and goals that require a set of tasks to accomplish","content":""},{"uri":"https://gardener.cloud/documentation/guides/applications/","title":"Applications","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/extensions/","title":"Extensions","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/documentation/references/","title":"API Reference","tags":[],"description":"Reference documentation for the Gardener API","content":"API Reference Documentation Bellow you can find reference documentation of the API for Gardener.\nGardener Gardener APIs are Kubernetes-style APIs for the life-cycle of a Kubernetes Cluster.\nThose APIs are divided into different groups:\n Core Extensions Settings  "},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/","title":"Monitoring","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/contribute/","title":"Contribute","tags":[],"description":"Contributors guides for code and documentation","content":"graph TB; B(Contributor) B -- C{I want to..} C --|Code| D(\"fa:fa-code-fork \u0026lt;a href\u0026#61;\u0026#39;./code\u0026#39;\u0026gt;Code Contribute\u0026lt;/a\u0026gt;\") C --|Docs| E(\"fa:fa-paragraph \u0026lt;a href\u0026#61;\u0026#39;./docs\u0026#39;\u0026gt;Doc Contribute\u0026lt;/a\u0026gt;\")  Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nIndividual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project\u0026rsquo;s license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Add tests relevant to the fixed bug or new feature.\n  Issues and Planning We use GitHub issues to track bugs and enhancement requests and ZenHub for planning.\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\n"},{"uri":"https://gardener.cloud/","title":"Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/news/","title":"News","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/news/004/00/","title":"What&#39;s Gardener Project to SAP?","tags":[],"description":"","content":"What\u0026rsquo;s Gardener Project to SAP? An New Article on about SAP, Gardener and OSS. Recently, Thomas Hertz, CNCF board member and head of Development Experience at SAP, published an overview article at the TheNewStack where he discusses projct Gardener and its place in SAP and the open source community, and what\u0026rsquo;s the future ahead of it.\n\u0026ldquo;Gardener is an SAP-driven open source project that tackles real-world demands for hyperscale Kubernetes services, regardless of infrastructure.\u0026quot;, Thomas says disucssing the benefits from Gardener, yielding analogy with Borg, born to address real-world problems and eventually becomming Kubernetes. Further stressing upon the project\u0026rsquo;s Kubernetes DNA he outlines the unique, vendor-neutral approach of Gardener to offer a lock-in free solution and lists a number of SAP solutions that already benefit from that far and wide.\n Gardener provides a neutral toolbox for the technology stack of today, and we designed it to be sufficiently extensible so that — with relatively low effort — it can additionally adapt for the tools and infrastructures that come next. No one can say which direction the Kubernetes ecosystem will take, but Gardener is designed to keep things open and flexible.\n \u0026ldquo;We are determined to be transparent with Gardener, by developing everything in the public space and then adopting it with minor SAP specific integrations in-house. It’s always been imperative to keep it vendor-neutral and to stick to upstream Kubernetes practices, design and processes.\u0026quot; Thomas then elaborates on SAP\u0026rsquo;s comitment to transparently drive the project in vendor-neutral manner and in a trully transparent for the community community and collaborative manner.\nHe then discusses various use cases of community members using Gardener in differnet domains and concludes with the benefits of the open source project for SAP and how it catalizes a cultural change\n Within SAP, Gardener is influencing and catalyzing change. We already have great inner sourcing examples, with internal stakeholders contributing directly to the open source software project and doing almost everything in the public.\n \u0026ldquo;We developed Gardener to provide our customers with a single, consistent Kubernetes feature set that abstracts resources and underlying infrastructure, and can be used by SAP solutions anywhere. We are seeing an uptake by those developing applications and deploying them across multiple clouds, and look forward to working with the community to extend Gardener and deliver hyperscale Kubernetes services for the tools and infrastructures of the future.\u0026quot; Thomas says at the end of the article, inviting contributors and adopters to the growing Gardener community for a collaboraiton to meet the challenges today and in future.\nRead more about that in the original article at the .\n"},{"uri":"https://gardener.cloud/blog/2020-05/00/","title":"PingCAP’s Experience in Implementing their Managed TiDB Service with Gardener","tags":[],"description":"","content":"Gardener is showing successful collaboration with its growing community of contributors and adopters. With this come some success stories, including PingCAP using Gardener to implement its managed service.\nAbout PingCAP and its TiDB Cloud PingCAP started in 2015, when three seasoned infrastructure engineers working at leading Internet companies got sick and tired of the way databases were managed, scaled and maintained. Seeing no good solution on the market, they decided to build their own - the open-source way. With the help of a first-class team and hundreds of contributors from around the globe, PingCAP is building a distributed NewSQL, hybrid transactional and analytical processing (HTAP) database.\nIts flagship project, TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project.\nPingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers. As a result, the company turned to Gardener to build their managed TiDB cloud service offering.\nTiDB Cloud Beta Preview Limitations with other public managed Kubernetes services Previously, PingCAP encountered issues while using other public managed K8s cluster services, to develop the first version of its TiDB Cloud. Their worst pain point was that they felt helpless when encountering certain malfunctions. PingCAP wasn’t able to do much to resolve these issues, except waiting for the providers’ help. More specifically, they experienced problems due to cloud-provider specific Kubernetes system upgrades, delays in the support response (which could be avoided in exchange of a costly support fee), and no control over when things got fixed.\nThere was also a lot of cloud-specific integration work needed to follow a multi-cloud strategy, which proved to be expensive both to produce and maintain. With one of these managed K8s services, you would have to integrate the instance API, as opposed to a solution like Gardener, which provides a unified API for all clouds. Such a unified API eliminates the need to worry about cloud specific-integration work altogether.\nWhy PingCAP chose Gardener to build TiDB Cloud  “Gardener has similar concepts to Kubernetes. Each Kubernetes cluster is just like a Kubernetes pod, so the similar concepts apply, and the controller pattern makes Gardener easy to manage. It was also easy to extend, as the team was already very familiar with Kubernetes, so it wasn’t hard for us to extend Gardener. We also saw that Gardener has a very active community, which is always a plus!”- Aylei Wu, (Cloud Engineer) at PingCAP\n At first glance, PingCAP had initial reservations about using Gardener - mainly due to its adoption level (still at the beginning) and an apparent complexity of use. However, these were soon eliminated as they learned more about the solution. As Aylei Wu mentioned during the last Gardener community meeting, “a good product speaks for itself”, and once the company got familiar with Gardener, they quickly noticed that the concepts were very similar to Kubernetes, which they were already familiar with.\nThey recognized that Gardener would be their best option, as it is highly extensible and provides a unified abstraction API layer. In essence, the machines can be managed via a machine controller manager for different cloud providers - without having to worry about the individual cloud APIs.\nThey agreed that Gardener’s solution, although complex, was definitely worth it. Even though it is a relatively new solution, meaning they didn’t have access to other user testimonials, they decided to go with the service since it checked all the boxes (and as SAP was running it productively with a huge fleet). PingCAP also came to the conclusion that building a managed Kubernetes service themselves would not be easy. Even if they were to build a managed K8s service, they would have to heavily invest in development and would still end up with an even more complex platform than Gardener’s. For all these reasons combined, PingCAP decided to go with Gardener to build its TiDB Cloud.\nHere are certain features of Gardener that PingCAP found appealing:\n Cloud agnostic: Gardener’s abstractions for cloud-specific integrations dramatically reduce the investment in supporting more than one cloud infrastructure. Once the integration with Amazon Web Services was done, moving on to Google Cloud Platform proved to be relatively easy. (At the moment, TiDB Cloud has subscription plans available for both GCP and AWS, and they are planning to support Alibaba Cloud in the future.) Familiar concepts: Gardener is K8s native; its concepts are easily related to core Kubernetes concepts. As such, it was easy to onboard for a K8s experienced team like PingCAP’s SRE team. Easy to manage and extend: Gardener’s API and extensibility are easy to implement, which has a positive impact on the implementation, maintenance costs and time-to-market. Active community: Prompt and quality responses on Slack from the Gardener team tremendously helped to quickly onboard and produce an efficient solution.  How PingCAP built TiDB Cloud with Gardener On a technical level, PingCAP’s set-up overview includes the following:\n A Base Cluster globally, which is the top-level control plane of TiDB Cloud A Seed Cluster per cloud provider per region, which makes up the fundamental data plane of TiDB Cloud A Shoot Cluster is dynamically provisioned per tenant per cloud provider per region when requested A tenant may create one or more TiDB clusters in a Shoot Cluster  As a real world example, PingCAP sets up the Base Cluster and Seed Clusters in advance. When a tenant creates its first TiDB cluster under the us-west-2 region of AWS, a Shoot Cluster will be dynamically provisioned in this region, and will host all the TiDB clusters of this tenant under us-west-2. Nevertheless, if another tenant requests a TiDB cluster in the same region, a new Shoot Cluster will be provisioned. Since different Shoot Clusters are located in different VPCs and can even be hosted under different AWS accounts, TiDB Cloud is able to achieve hard isolation between tenants and meet the critical security requirements for our customers.\nTo automate these processes, PingCAP creates a service in the Base Cluster, known as the TiDB Cloud “Central” service. The Central is responsible for managing shoots and the TiDB clusters in the Shoot Clusters. As shown in the following diagram, user operations go to the Central, being authenticated, authorized, validated, stored and then applied asynchronously in a controller manner. The Central will talk to the Gardener API Server to create and scale Shoot clusters. The Central will also access the Shoot API Service to deploy and reconcile components in the Shoot cluster, including control components (TiDB Operator, API Proxy, Usage Reporter for billing, etc.) and the TiDB clusters.\nTiDB Cloud on Gardener Architecture Overview What’s next for PingCAP and Gardener With the initial success of using the project to build TiDB Cloud, PingCAP is now working heavily on the stability and day-to-day operations of TiDB Cloud on Gardener. This includes writing Infrastructure-as-Code scripts/controllers with it to achieve GitOps, building tools to help diagnose problems across regions and clusters, as well as running chaos tests to identify and eliminate potential risks. After benefiting greatly from the community, PingCAP will continue to contribute back to Gardener.\nIn the future, PingCAP also plans to support more cloud providers like AliCloud and Azure. Moreover, PingCAP may explore the opportunity of running TiDB Cloud in on-premise data centers with the constantly expanding support this project provides. Engineers at PingCAP enjoy the ease of learning from Gardener’s kubernetes-like concepts and being able to apply them everywhere. Gone are the days of heavy integrations with different clouds and worrying about vendor stability. With this project, PingCAP now sees broader opportunities to land TiDB Cloud on various infrastructures to meet the needs of their global user group.\nStay tuned, more blog posts to come on how Gardener is collaborating with its contributors and adopters to bring fully-managed clusters at scale everywhere! If you want to join in on the fun, connect with our community.\n"},{"uri":"https://gardener.cloud/blog/","title":"Blogs","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/news/002/00/","title":"Gardener Website 2.0","tags":[],"description":"","content":" .news-item .green-power { color: #0f674e; font-weight: 700; } .news-item .green-power img { display: inline-block; width: 32px; vertical-align: middle; margin: 0 5px 0; } @media (min-width: 750px) { .news-item .green-power img { width: 48px; } } .news-item .title-2 { font-weight: 700; background-color: unset!important; color: #222!important; }  New Website, Same Green Power! The Gardener Project just got a brand new website with multiple improvements and we are all very excited!\n Go ahead and explore around now and help us spread the word: https://gardener.cloud Tweet  Read more about changes and plans in the blog post.\n"},{"uri":"https://gardener.cloud/blog/2020_week_20/00/","title":"New Website, Same Green Flower","tags":[],"description":"","content":"The Gardener project website just received a serious facelift. Here are some of the highlights:\n A completely new landing page, emphasizing both on Gardener\u0026rsquo;s value proposition and the open community behind it. The Community page was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the communty. A completely new News section and content type available at /documentation/news. Use metadata such as publishdate and archivedate to schedule for news publish and archive automatically, regardless of when you contributed them. You can now track what\u0026rsquo;s happening from the landing page or in the dedicated News section on the website and share. Improved blogs layout. One-click sharing options are available starting with simple URL copy link and twitter button and others will closely follow up. While we are at it, give it a try. Spread the word.  Website builds also got to a new level with:\n Containerization. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the /documentation repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing make serve in your local /documentation clone. Numerous improvements to the buld scripts. More configuration options, authenticated requests, fault tollerance and performance. Good news for Windows WSL users who will now nejoy a significantly support. See the updated README for details on that. A number of improvements in layouts styles, site assets and hugo site-building techniques.  But hey, THAT\u0026rsquo;S NOT ALL!\nStay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and \u0026hellip; let\u0026rsquo;s cut the spoilers here.\nI hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on Twitter and Slack, or in case of issues - on GitHub.\nGo ahead and help us spread the word: https://gardener.cloud\n  "},{"uri":"https://gardener.cloud/news/003/release/","title":"Gardener 1.4.0 Released","tags":[],"description":"","content":"Gardener Release 1.4 ​ The Gardener release 1.4 is mainly focused on stability improvments and optimizations. For example, we twitched some configurations (e.g CPU and memory limits), and improved the monitoring and healtchechks. And there are some new features too. The list below is an overview of the most notable changes. Explore the full release notes in GitHub Release 1.4. ​​\nNotable Changes Now you decide when the reconciliation happens if you change your Shoot specification ​ Previously whenever you change a configuration of your Shoot.yaml specification, a reconciliation process was triggered enabling your desired state. Now using .spec.maintenance.confineSpecUpdateRollout=true you confine those changes to be updated in the individual maintenance time window. This is helpful if you want to update your Shoot but don\u0026rsquo;t want the changes to be applied immediately. One example use-case would be a Kubernetes version upgrade that you want to roll out during the maintenance time window. It is important to note that if you change the maintenance window itself, then it will only be effective after the upcoming maintenance. Of course, there is an exception with the .spec.hibernation.enabled field, which changes are taken under consideration immediately. If you hibernate or wake-up your Shoots then Gardener gets active right away. ​\nNew Grafana dashboard to monitor CoreDNS\u0026rsquo;s stats ​ We exposed this Dashboard to give you insight over some CoreDNS metrics of your cluster, like: DNS Requests, DNS Lookups, Cache Hits/Misses. ​\nBetter way of describing Shoot reconciliation errors ​ The Shoot health check controller has been improved to produce error codes (if applicable) to the .status.conditions[].codes that help to categorize observed problems. Also, there are two new error codes: ERR_INFRA_RESOURCES_DEPLETED indicates that the underlying infrastructure does not have enough resources anymore, and ERR_CONFIGURATION_PROBLEM indicates that the user has misconfigured something and should double-check the specification.\nGardener will now block removal of Kubernetes and machine image versions from the CloudProfile which are still in use ​ The Gardener API Server now validates the changes of CloudProfile against Shoots that are using it. And will block removal of in use versions of Kubernetes and machine images from the CloudProfile. This is part of the \u0026ldquo;Gardener Versioning Policy\u0026rdquo; proposal that you can find at GEP-5 ​\nForceful Shoot clusters updates ​ You can specify for which Kubernetes and machine image versions you can forcefully upgrade to newer ones when expired. This is part of the \u0026ldquo;Gardener Versioning Policy\u0026rdquo; proposal that you can find at GEP-5\nGardener supports metadata.generateName as alternative to metadata.name Create unlimited count of worker groups per Shoot Previously it was not possible to create more that 20-25 worker groups, because the number of worker\u0026rsquo;s cloud configs were limited by the size of the Secret. We divided the cloud config to dedicated managed resources to eliminate this limitation.\nCommon library for simple validating webhooks ​ We introduced new common library in the extension package that will help you to develop simple validating or mutating webhooks for different K8s types with different handlers. Please have a look at PR#69 to see an implementation in action.\nImprovements Kubernetes Dashboard addon version v2.0.0 ​ We bumped the already presented addon for Kubernetes Dashboard to v2.0.0\nGardener validates Pod/Service — Service/Pod network intersection between the Shoot and the Seed ​ Previosuly an overlap of these networks resulted in a broken state, so we added such validation to our admission plugin. ​\nThe infrastructure reconciliation for hibernated shoots is now skipped Endpoint managed by 3rd party operators doesn\u0026rsquo;t block the Shoot\u0026rsquo;s hibernation ​ Now, gardener checks if the Endpoints object is reconciled by kube-controller-manager, otherwise it ignores and does not block the hibernation. ​\nFixed a bug in the healthcheck that prevents checks after a Shoot has been woken up from hibernation If you have implemented custom extension controller for your cluster, you can vendor Gardener v1.4.0 to benefit from the fix.\nDetect outdated health check reports ​ The Gardenlet detects outdated health check reports on extension CRDs with a default threshold of 5 minutes in case Gardener extensions stop performing health checks. For backwards-compatibility reasons, the gardenlet does not check for stale extension reports per default. To enable, the field controllers.shootCare.staleExtensionHealthCheckThreshold in the Gardenlet configuration file should be set. ​\nUpdates on Grafana Dashboard ​ Removed the cluster overview dashboard since metrics used in this dashboard were removed. Other dashboards are changed to no longer show data on a \u0026ldquo;Pod level\u0026rdquo; since pod level metrics have a high cardinality and have been mostly removed from the aggregate-prometheus. ​\nETCD Encryption data is persisted in the ShootState ​ We now replicate the ETCD encryption into the ShootState for future restoration purposes. ​\nImproved Shoot operations ​ We fixed a race condition that led to incomplete maintenance operations for shoot clusters and fixed a bug that prevented the Shoot reconciliation to wait for the deletion of Extension CRDs. ​\nExtension controllers now support Migrate and Restore operations ​ The Actuator interface for the Infrastructure, ControlPlane, Network, Worker, OperationSystemConfig, BackupEntry extension controllers were extended to support migrate and restore operations. ​\nAutoscaler has configurable delay for Pod age before considering scaling-up​ HVPA now properly scales ETCD containers Enhnaced etcd-backup-restore API for delta and full snapshots ​ Triggering full or delta snapshots now returns metadata for the snapshot taken in the response body. Also, new endpoint was introduces for fetching details of the latest full and delta snapshots.\n"},{"uri":"https://gardener.cloud/news/001/release/","title":"Gardener 1.3.0 Released","tags":[],"description":"","content":"Gardener 1.3.0 Released See what\u0026rsquo;s new and noteworthy presented by the team on the regular community meeting recording.\nBookmark the agenda for upcomming community meetings and stay up to date with what\u0026rsquo;s going on in the project.\nFor a detailed list of changes refer to the release notes.\nEnjoy!\n"},{"uri":"https://gardener.cloud/blog/2019_week_21/","title":"Cluster Overprovisioning","tags":[],"description":"","content":"This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work load that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n..read some more on Cluster Overprovisioning.\n"},{"uri":"https://gardener.cloud/blog/2019_week_21_2/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"","content":"Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nPossible Use Cases\n turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  ..read some more on Feature Flags for App.\n"},{"uri":"https://gardener.cloud/blog/2019_week_06/","title":"Manually adding a node to an existing cluster","tags":[],"description":"","content":"Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\n This tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\n..read some more on Adding Nodes to a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2019_week_02/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n  What happens if your kubeconfig file of your production cluster is leaked or published by accident?\n Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if it is has leaked - delete the cluster.\n..learn more on Work with kubeconfig files.\n"},{"uri":"https://gardener.cloud/blog/2018_week_40/","title":"Hibernate a Cluster to save money","tags":[],"description":"","content":"You want to experiment with Kubernetes or have set up a customer scenario, but you don\u0026rsquo;t want to run the cluster 24 / 7 for reasons of cost?\n The Gardener gives you the possibility to scale your cluster down to zero nodes.\n..read some more on Hibernate a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2018_week_22/","title":"Anti Patterns","tags":[],"description":"","content":" Running as root user Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user.\nStoring data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n"},{"uri":"https://gardener.cloud/blog/2018_week_46/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"","content":"In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\n Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\n..read some more on Auditing Kubernetes for Secure Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_07/","title":"Big things come in small packages","tags":[],"description":"","content":"Microservices tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - size of the technology stack.\nGeneral purpose technology stack There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight technology stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\nAdditionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize and deploy a microservice without impacting other subsystems.\n"},{"uri":"https://gardener.cloud/blog/2018_week_51/","title":"Cookies are dangerous...","tags":[],"description":"","content":"\u0026hellip;they mess up the figure.\nFor a team event during the Christmas season we decided to completely reinterpret the topic cookies\u0026hellip; since the vegetables have gone on a well-deserved vacation. :-)\nGet recipe on Gardener Cookies.\n"},{"uri":"https://gardener.cloud/blog/2018_week_17/","title":"Frontend HTTPS","tags":[],"description":"","content":" For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nCreate a secret in the namespace of the ingress containing the TLS private key and certificate. Then configure the secret name in the TLS configuration section of the ingress specification.\n..read on HTTPS - Self Signed Certificates how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_50/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"The Gardener project team has analyzed the impact of the Gardener CVE-2018-2475 and the Kubernetes CVE-2018-1002105 on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.\nRead more on Hardening the Gardener Community Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_06/","title":"Kubernetes is available in Docker for Mac 17.12 CE","tags":[],"description":"","content":"    Kubernetes is only available in Docker for Mac 17.12 CE and higher on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see general configuration.     Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes. The Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n\u0026hellip;see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n"},{"uri":"https://gardener.cloud/blog/2018_week_09/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may chose to configure Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another  ..read on Namespace Isolation how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_08_2/","title":"Namespace Scope","tags":[],"description":"","content":"Should I use:\n❌ one namespace per user/developer? ❌ one namespace per team? ❌ one per service type? ❌ one namespace per application type? 😄 one namespace per running instance of your application?  Apply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don\u0026rsquo;t provide:\n Network isolation Access Control Audit Logging on user level  "},{"uri":"https://gardener.cloud/blog/2018_week_27/","title":"ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS","tags":[],"description":"","content":"The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  When you have application running on multiple nodes which require shared access to a file system When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS supports encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about vendor lock-in and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (roughly twice the price of EBS storage) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don’t use EFS, use it for the files which can\u0026rsquo;t be stored in a CDN. Don’t use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.  ..read some more on ReadWriteMany.\n"},{"uri":"https://gardener.cloud/blog/2018_week_10/","title":"Shared storage with S3 backend","tags":[],"description":"","content":"The storage is definitely the most complex and important part of an application setup, once this part is completed, one of the most problematic parts could be solved.\nMounting a S3 bucket into a pod using FUSE allows to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_08/","title":"Watching logs of several pods","tags":[],"description":"","content":"One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment and you don\u0026rsquo;t have setup a log viewer service like Kibana.\nkubetail comes to the rescue, it is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n"},{"uri":"https://gardener.cloud/installer/","title":"","tags":[],"description":"","content":"gardener-installerWe're sorry but gardener-installer doesn't work properly without JavaScript enabled. Please enable it to continue."},{"uri":"https://gardener.cloud/readme/","title":"","tags":[],"description":"","content":"This folder contains the content of the public facing gardener landing page http://gardener.cloud\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/access_pod_from_local/","title":"Access a port of a pod locally","tags":[],"description":"","content":"Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to Access my service\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps:\n Run kubectl get pods Note down the name of the pod in question as \u0026lt;your-pod-name\u0026gt; Run kubectl port-forward \u0026lt;your-pod-name\u0026gt; \u0026lt;local-port\u0026gt;:\u0026lt;your-app-port\u0026gt; Run a web browser or curl locally and enter the URL http(s)://localhost:\u0026lt;local-port\u0026gt;  In addition, kubectl port-forward allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward. Find more details in the Kubernetes documentation.\nThe main drawback of this approach is that the pod\u0026rsquo;s name will change as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.\nSolution 2: Using apiserver proxy There are several different proxies used with Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. Different from the first solution, a service is required for this solution .\nUse the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read Discovering builtin services\nhttps://\u0026lt;cluster-master\u0026gt;/api/v1/namespace/\u0026lt;namespace\u0026gt;/services/\u0026lt;service\u0026gt;:\u0026lt;service-port\u0026gt;/proxy/\u0026lt;service-endpoint\u0026gt;\nExample:\n   cluster-master namespace service yservice-port service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / url   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 url    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the port-forward approach described above.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/service-access/","title":"Access service from outside Kubernetes cluster","tags":[],"description":"Is there an ingress deployed and how is it configured","content":"TL;DR To expose your application / service for access from outside the cluster, following options exist:\n Kubernetes Service of type LoadBalancer Kubernetes Service of type \u0026lsquo;NodePort\u0026rsquo; + Ingress  This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there are many examples, here is one brief example.\nService Types A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.\nServices can be exposed in different ways by specifying a type in the service spec, and different types determine accessibility from inside and outside of cluster.\n ClusterIP NodePort LoadBalancer  Type ExternalName is a special case of service and not discussed here.\nType ClusterIP A service of type ClusterIP exposes a service on an internal IP in the cluster, which makes the service only reachable from within the cluster. This is the default value if no type is specified.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:ClusterIP# use ClusterIP as type hereports:- port:80selector:app:nginx-appExecute following commands to create deployment and service\nkubectl create -f \u0026lt;Your yaml file name\u0026gt; Checking the service status\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc ClusterIP 100.66.125.61 \u0026lt;none\u0026gt; 80/TCP 45m As shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file. You can test the service like this:\n# list all existing pods in cluster $ kubectl get pods NAME READY STATUS RESTARTS AGE docker-nodejs-app-76b77494-vwv4d 1/1 Running 0 11d nginx-deployment-74d949bf69-nvdzs 1/1 Running 0 1h privileged-pod 1/1 Running 0 11d # test service from within the cluster on the same pod $ kubectl exec -it nginx-deployment-74d949bf69-nvdzs curl 100.66.125.61:80 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 612 100 612 0 0 1006k 0 --:--:-- --:--:-- --:--:-- 597k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; ...   Tip\n The service is also accessible from any other container (even from different pods) within the same cluster, e.g. kubectl -it exec \u0026lt;another POD_NAME\u0026gt; curl \u0026lt;YourServiceClusterIP:YourPort\u0026gt;. You need to make sure command curl is installed in the container. You can also find out the dns name of the ClusterIP by command kubectl exec -it \u0026lt;POD_NAME\u0026gt; nslookup \u0026lt;ClusterIP\u0026gt;, replace the IP address with the resolved name in your test. The resolved name typically looks like nginx-svc.default.svc.cluster.local where nginx-svc is the name of your service defined in the configuration file.   Type NodePort Follow the previous example, just replace the type with NodePort\n...spec:type:NodePortports:- port:80...A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates a port in the range 30000–32767 and opens this port on every node (thus the name “NodePort”). Connections to this port are forwarded to the service’s cluster IP. If we create the service above and run kubectl get svc \u0026lt;your-service\u0026gt;, we can see the NodePort that has been allocated for it.\nNote that in the in following example, in addition to port 80, port 32521 has been opened as well on the node, in contrast to the output of \u0026ldquo;ClusterIP\u0026rdquo; case where only port 80 is opened.\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc NodePort 100.70.105.182 \u0026lt;none\u0026gt; 80:32521/TCP 16m Therefore you can access the service from within the cluster in two ways:\n Access via ClusterIP:port  #via ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80 #via internal name of ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80  Access via NodeIP:NodePort  # First find out the Node IP address $ kubectl describe node Name: ip-10-250-20-203.eu-central-1.compute.internal Roles: node Addresses: InternalIP: 10.250.20.203 InternalDNS: ip-10-250-20-203.eu-central-1.compute.internal Hostname: ip-10-250-20-203.eu-central-1.compute.internal ... #via NodeIP:NodePort kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521 #via internal name of NodeIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521 Type LoadBalancer The LoadBalancer type is the simplest approach, which is created by specifying type as LoadBalancer.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:LoadBalancer# use LoadBalancer as type hereports:- port:80selector:app:nginx-appOnce the service is created, it has an external IP address as shown here:\n$ kubectl get services -l app=nginx-app -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-svc LoadBalancer 100.67.182.148 a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com 80:31196/TCP 9m app=nginx-app A service of type LoadBalancer combines the capabilities of a NodePort with the ability to setup a complete ingress path.\nHence the service can be accessible from outside the cluster without the need for additional components like an Ingress.\nTo test the external IP run this curl command from your local machine:\n$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;... RawContent : HTTP/1.1 200 OK ... Obviously the service can also is accessed from within the cluster. You can test this in the same way as described in section NodePort.\nLoadBalancer vs. Ingress As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster. However this approach has its own limitation. You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced for this purpose.\nWhy an Ingress LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a a separate resource that configures a LoadBalancer in a more flexible way. The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected pods which is more efficient.\nTypically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them you already pay 400$ per month just for load balancing.\nHow to use the ingress? In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:\n k8s-hana.ondemand.com  \u0026lt;gardener_cluster_name\u0026gt;.\u0026lt;gardener_project_name\u0026gt;.shoot.canary.k8s-hana.ondemand.com.\nBoth \u0026lt;gardener_cluster_name\u0026gt; and \u0026lt;gardener_project_name\u0026gt; are defined in Gardener which can be determined on Gardener dashboard.\nThis results in the following default DNS endpoints:\n api.\u0026lt;cluster_domain\u0026gt; Kubernetes API *.ingress.\u0026lt;cluster_domain\u0026gt; Internal nginx ingress  Example: Configure an Ingress resource with Service type: NodePort With the configuration below you can reach your service nginx-svc with:\nhttp://test.ingress.\u0026amp;lt;GARDENER-CLUSTER-NAME\u0026amp;gt;.\u0026amp;lt;GARDENER-PROJECT-NAME\u0026amp;gt;.shoot.canary.k8s-hana.ondemand.com\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:NodePortports:- port:80selector:app:nginx-app---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:nginxsvc-ingressspec:rules:- host:nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:nginx-svcservicePort:80Show the newly created ingress and test it :\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginxsvc-ingress nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com 10.250.20.203 80 29s $ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; ... Reference:  Concepts: Kubernetes Service Concepts: Connecting Applications with Services Tutorial: Using a Service to Expose Your App Tutorial: Using Source IP Kubernetes Networking Accessing Kubernetes Pods from Outside of the Cluster  "},{"uri":"https://gardener.cloud/using-gardener/administrator/","title":"Administrator","tags":[],"description":"","content":"Learning Material Everything you need to know to operate gardener.    "},{"uri":"https://gardener.cloud/adopter/","title":"Adopters","tags":[],"description":"","content":" See who is using Gardener Gardener adopters in production environments that have publicly shared details of their usage.       b’nerd uses Gardener as the core technology for its own managed Kubernetes as a Service solution and operates multiple Gardener installations for several cloud hosting service providers.    SAP uses Gardener to deploy and manage Kubernetes clusters at scale in a uniform way across infrastructures (AWS, Azure, GCP, Alicloud, OpenStack). Workloads include databases (SAP HANA), Big Data (SAP Data Hub), IoT, AI, and Machine Learning (SAP Leonardo), Serverless and diverse business workloads.    ScaleUp Technologiesruns Gardener within their public Openstack Clouds (Hamburg, Berlin, Düsseldorf). Their clients run all kinds of workloads on top of Gardener maintained Kubernetes clusters ranging from databases to Software-as-a-Service applications.    Finanz Informatik Technologie Services GmbHuses Gardener to offer k8s as a service for customers in the financial industry in Germany. It is built on top of a \"metal as a service\" infrastructure implemented from scratch for k8s workloads in mind. The result is k8s on top of bare metal in minutes.    PingCAP TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project. PingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers and they chose Gardener.    Beezlabs uses Gardener to deliver Intelligent Process Automation platform, on multiple cloud providers and reduce costs and lock-in risks.   If you’re using Gardener and you aren’t on this list, submit a pull request!    "},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/alerting/","title":"Alerting","tags":[],"description":"","content":"Alerting Gardener uses Prometheus to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an alertmanager. The alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:\n end-users/stakeholders/customers operators/administrators  Alerting for Users To receive email alerts as a user set the following values in the shoot spec:\nspec:monitoring:alerting:emailReceivers:- john.doe@example.comemailReceivers is a list of emails that will receive alerts if something is wrong with the shoot cluster. A list of alerts for users can be found here.\nAlerting for Operators Currently, Gardener supports two options for alerting:\n Email Alerting Sending Alerts to an external alertmanager  A list of operator alerts can be found here.\nEmail Alerting Gardener provides the option to deploy an alertmanager into each seed. This alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values alerting. See this on how to configure the Gardener\u0026rsquo;s SMTP secret. If the values are set, a secret with the label gardener.cloud/role: alerting will be created in the garden namespace of the garden cluster. This secret will be used by each alertmanager in each seed.\nExternal Alertmanager The alertmanager supports different kinds of alerting configurations. The alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external alertmanager. This external alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this alertmanager). To configure sending alerts to an external alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: gardener.cloud/role: alerting. This secret needs to contain a URL to the the external alertmanager and information regarding authentication. Supported authentication types are:\n No Authentication (none) Basic Authentication (basic) Mutual TLS (certificate)  Remote Alertmanager Examples Note: the url value cannot be prepended with http or https.\n# No AuthenticationapiVersion:v1kind:Secretmetadata:labels:gardener.cloud/role:alertingname:alerting-authnamespace:gardendata:# No Authenticationauth_type:base64(none)url:base64(external.alertmanager.foo)# Basic Authauth_type:base64(basic)url:base64(extenal.alertmanager.foo)username:base64(admin)password:base64(password)# Mutual TLSauth_type:base64(certificate)url:base64(external.alertmanager.foo)ca.crt:base64(ca)tls.crt:base64(certificate)tls.key:base64(key)# Email Alerts (internal alertmanager)auth_type:base64(smtp)auth_identity:base64(internal.alertmanager.auth_identity)auth_password:base64(internal.alertmanager.auth_password)auth_username:base64(internal.alertmanager.auth_username)from:base64(internal.alertmanager.from)smarthost:base64(internal.alertmanager.smarthost)to:base64(internal.alertmanager.to)type:OpaqueConfiguring your External Alertmanager Please refer to the alertmanager documentation on how to configure an alertmanager.\nWe recommend you use at least the following inhibition rules in your alertmanager configuration to prevent excessive alerts:\ninhibit_rules:# Apply inhibition if the alert name is the same.- source_match:severity:criticaltarget_match:severity:warningequal:[\u0026#39;alertname\u0026#39;,\u0026#39;service\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop all alerts for type=shoot if there are VPN problems.- source_match:service:vpntarget_match_re:type:shootequal:[\u0026#39;type\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop warning and critical alerts if there is a blocker - no workers nodes, no etcd main etc.- source_match:severity:blockertarget_match_re:severity:^(critical|warning)$equal:[\u0026#39;cluster\u0026#39;]# If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server.- source_match:service:kube-apiservertarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]# If API server is down inhibit kube-state-metrics alerts.- source_match:service:kube-apiservertarget_match_re:severity:infoequal:[\u0026#39;cluster\u0026#39;]# No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down.- source_match:service:kube-state-metrics-shoottarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]Below is a graph visualizing the inhibition rules:\n"},{"uri":"https://gardener.cloud/using-gardener/developer/topic/","title":"App Developer","tags":[],"description":"","content":"Learning Material Everything you need to know about running your software.      by Topic     by Experience Level    "},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/25_markup/attachments/","title":"Attachments","tags":[],"description":"The Attachments shortcode displays a list of files attached to a page.","content":"The Attachments shortcode displays a list of files attached to a page.\n  Attachments  \u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt; BachGavotteShort.mp3 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt; Carroll_AliceAuPaysDesMerveilles.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt; adivorciarsetoca00cape.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt; hugo.png \u0026lt;/a\u0026gt; (17 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt; movieselectricsheep-flock-244-32500-2.mp4 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;div\u0026gt;   Usage The shortcurt lists files found in a specific folder. Currently, it support two implementations for pages\n  If your page is a markdown file, attachements must be place in a folder named like your page and ending with .files.\n  content  _index.md page.files  attachment.pdf   page.md       If your page is a folder, attachements must be place in a nested \u0026lsquo;files\u0026rsquo; folder.\n  content  _index.md page  index.md files  attachment.pdf           Be aware that if you use a multilingual website, you will need to have as many folders as languages.\nThat\u0026rsquo;s all !\nParameters    Parameter Default Description     title \u0026ldquo;Attachments\u0026rdquo; List\u0026rsquo;s title   style \u0026quot;\u0026rdquo; Choose between \u0026ldquo;orange\u0026rdquo;, \u0026ldquo;grey\u0026rdquo;, \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;green\u0026rdquo; for nice style   pattern \u0026ldquo;.*\u0026rdquo; A regular expressions, used to filter the attachments by file name. The pattern parameter value must be regular expressions.    For example:\n To match a file suffix of \u0026lsquo;jpg\u0026rsquo;, use *.jpg (not *.jpg). To match file names ending in \u0026lsquo;jpg\u0026rsquo; or \u0026lsquo;png\u0026rsquo;, use .*(jpg|png)  Examples List of attachments ending in pdf or mp4 {{%attachments title=\u0026quot;Related files\u0026quot; pattern=\u0026quot;.*(pdf|mp4)\u0026quot;/%}}  renders as\n  Related files  \u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt; Carroll_AliceAuPaysDesMerveilles.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt; adivorciarsetoca00cape.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt; movieselectricsheep-flock-244-32500-2.mp4 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;div\u0026gt;   Colored styled box {{%attachments style=\u0026quot;orange\u0026quot; /%}}  renders as\n  Attachments  \u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt; BachGavotteShort.mp3 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt; Carroll_AliceAuPaysDesMerveilles.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt; adivorciarsetoca00cape.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt; hugo.png \u0026lt;/a\u0026gt; (17 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt; movieselectricsheep-flock-244-32500-2.mp4 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;div\u0026gt;   {{%attachments style=\u0026quot;grey\u0026quot; /%}}  renders as\n  Attachments  \u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt; BachGavotteShort.mp3 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt; Carroll_AliceAuPaysDesMerveilles.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt; adivorciarsetoca00cape.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt; hugo.png \u0026lt;/a\u0026gt; (17 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt; movieselectricsheep-flock-244-32500-2.mp4 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;div\u0026gt;   {{%attachments style=\u0026quot;blue\u0026quot; /%}}  renders as\n  Attachments  \u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt; BachGavotteShort.mp3 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt; Carroll_AliceAuPaysDesMerveilles.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt; adivorciarsetoca00cape.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt; hugo.png \u0026lt;/a\u0026gt; (17 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt; movieselectricsheep-flock-244-32500-2.mp4 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;div\u0026gt;   {{%attachments style=\u0026quot;green\u0026quot; /%}}  renders as\n  Attachments  \u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt; BachGavotteShort.mp3 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt; Carroll_AliceAuPaysDesMerveilles.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt; adivorciarsetoca00cape.pdf \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt; hugo.png \u0026lt;/a\u0026gt; (17 ko) \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;/documentation/contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt; movieselectricsheep-flock-244-32500-2.mp4 \u0026lt;/a\u0026gt; (0 ko) \u0026lt;/li\u0026gt; \u0026lt;div\u0026gt;   "},{"uri":"https://gardener.cloud/documentation/guides/applications/insecure-configuration/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"A few insecure configurations in Kubernetes","content":"Auditing Kubernetes for Secure Setup \u0026lt;object type=\u0026quot;image/svg+xml\u0026quot; data=\u0026quot;./images/teaser.svg\u0026quot; style=\u0026quot;;visibility:hidden; margin: 3rem auto;display: block;\u0026quot; class=\u0026quot;inline reveal-fast drop-shadow\u0026quot;\u0026gt;\u0026lt;/object\u0026gt;  Increasing the Security of all Gardener Stakeholders In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\nMajor Findings From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.\nAlban Crequy (Kinvolk) and Dirk Marwinski (SAP SE) gave a presentation entitled Hardening Multi-Cloud Kubernetes Clusters as a Service at KubeCon 2018 in Shanghai presenting some of the findings.\nHere is a summary of the findings:\n  Privilege escalation due to insecure configuration of the Kubernetes API server\n Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server. Risk: Users can get access to the API server. Recommendation: Always use different CAs.    Exploration of the control plane network with malicious HTTP-redirects\n  Root cause: See detailed description below.\n  Risk: Provoked error message contains full HTTP payload from an existing endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials.\n  Recommendation:\n Use the latest version of Gardener Ensure the seed cluster\u0026rsquo;s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn\u0026rsquo;t support network policies.      Reading private AWS metadata via Grafana\n Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control Risk: Users can get the \u0026ldquo;user-data\u0026rdquo; for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster Recommendation: Lockdown Grafana features to only what\u0026rsquo;s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints    Scenario 1: Privilege Escalation with Insecure API Server In most configurations, different components connect directly to the Kubernetes API server, often using a kubeconfig with a client certificate. The API server is started with the flag:\n/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ... The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.\n The API server can have many clients of various kinds  However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.\n--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group  API server clients can reach the API server through an authenticating proxy  So far, so good. But what happens if malicious user “Mallory” tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?\n What happens when a client bypasses the proxy, connecting directly to the API server?  With a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header “X-Remote-Group: system:masters”.\nYou only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.\nThe kubectl tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP requests manually.\nWe worked on improving the Kubernetes documentation to make clearer that this configuration should be avoided.\nScenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.\n The API server is mostly a component that receives requests  However, there are exceptions. Some kubectl commands will trigger the API server to open a new connection to the Kubelet. Kubectl exec is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a HTTP-302 redirection to the Container Runtime Interface (CRI). Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because the Kubelet and the CRI component run on the same worker node.\n But the API server also initiates some connections, for example, to worker nodes  It’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with “host” volumes.\nIn contrast, users — even those with “system:masters” permissions or “root” rights — are often not given access to the control plane. On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network in the control plane.\nWhat would happen if a user was tampering with the Kubelet to make it maliciously redirect kubectl exec requests to a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.\n The API server is tricked to connect to other components  The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the AWS metadata service) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different EC2 instance profile for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.\nWe have reported this issue to the Kubernetes Security mailing list and the public pull request that addresses the issue has been merged PR#66516. It provides a way to enforce HTTP redirect validation (disabled by default).\nBut there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with Kubernetes Network Policies, EC2 Security Groups or just iptables directly. Following the defense in depth principle, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.\nIn Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the announcements on the Gardener mailing list. This is tracked in CVE-2018-2475.\nTo be protected from this issue, stakeholders should:\n Use the latest version of Gardener Ensure the seed cluster’s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn’t support network policies.  Scenario 3: Reading Private AWS Metadata via Grafana For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.\n Prometheus and Grafana can be used to monitor worker nodes  Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.\n Credentials can be retrieved from the debugging console of Chrome   Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets  In that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.\nThere are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.\nConclusion The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.\n"},{"uri":"https://gardener.cloud/documentation/guides/client_tools/oidc-login/","title":"Authenticating with an Identity Provider","tags":[],"description":"Authenticating with an Identity Provider using OpenID Connect","content":"Authenticating In this blog you will learn how to:\n Configure an Identity Provider using OpenID Connect. Configure a local kubectl plugin to enable oidc-login . Configure the K8s API Server of Gardener managed Kubernetes cluster. Create an RBAC rule to authorize an authenticated user.  Motivation As a project owner of Gardener, I want my Kubernetes level user to be authenticated by an identity provider.\nPrerequisite Knowledge Please read the following background material on Authenticating\nInsights About Gardener The Gardener allows the administrator to modify every aspect of the control plane setup, e.g. all feature gateways and even configurations are programmatically accessible. In this way, every administrative user of the Gardener has full control of how the control plane should be parameterized. But with this power, the user can easily configure a control plane that is beyond any SLA that the Gardener team can arguably support. Therefore, use this power wisely! A configuration that enables experimental features for production becomes an operational responsibility of the cluster owner\u0026rsquo;s team.\nBut Gardener does not stop you from experimenting!\nThere are currently no default IdP parameters.\nConfigure an Identity Provider Create a tenant in an OpenID-Connect compatible Identity Provider. For sake of simplicity, we shall use Auth0, which has a free plan for experimentations.\nIn your tenant, setup a native client/application that will use the authentication: Configure the client to have a callback url of http://localhost:8000. This callback will connect to your local kubectl oidc-login plugin: Note down the following parameters:\n Domain or Issuer url. It must be an https-secured endpoint (In case of Auth0, notice the trailing / at the end). Client ID Client Secret  Verify that https://\u0026lt;Issuer\u0026gt;/.well-known/openid-configuration is reachable.\nNow create some users (or connect to a user store): Notice that the users must have a verified email address. In doubt, just override that setting manually.\nConfigure kubectl oidc-login Please install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\n$ kubectl krew install oidc-login Updated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login Prepare a kubeconfig for later use:\n$ cp ~/.kube/config ~/.kube/config-oidc Modify the configurations as follows:\napiVersion:v1kind:Config...contexts:- context:cluster:shoot--project--myclusteruser:my-oidcname:shoot--project--mycluster...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profilename:oidcEnsure that the modified context is the active context current-context: shoot--project--mycluster.\nConfigure the Gardener Shoot Spec Modify the Gardener shoot/cluster manifest as follows:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:myclusternamespace:garden-project...spec:kubernetes:kubeAPIServer:oidcConfig:clientID:\u0026lt;ClientID\u0026gt; issuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;usernameClaim:emailThis change of the shoot manifest triggers a reconciliation. Once the reconciliation is finished, your oidc configuration is applied. It does not invalidate other certificate based authentication methods. Wait for Gardener to reconcile the change. It can take upto 5min.\nAuthorize an authenticated user For simplicity, we will just authorize a single user with the all encompassing cluster role cluster-admin:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:cluster-admin-testroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- apiGroup:rbac.authorization.k8s.iokind:Username:test@test.comAs administrator, activate/apply the above cluster role binding for test@test.com.\nVerify the Result Now activate the prepared kubeconfig-oidc and perform a login:\n$ export KUBECONFIG=~/.kube/config-oidc $ kubectl oidc-login Open http://localhost:8000 for authentication The plugin opens a browser for an interctive authentication session, and in parallel serves a local webserver for the configured callback.\nIf you successfully verified your user, then the console will display the validity of your returned token:\nYou got a valid token until 2019-08-14 06:26:49 +0200 CEST Inspect the kubeconfig-oidc. You will find two additional parameters:\n...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profileid-token:eyJ0eX...4In0.QQKS...TTTwrefresh-token:LFt...0Skjname:oidcThe plugin persisted the id-token and refresh-token in your configuration file.\nVerify that your user actually has the cluster-admin role:\n$ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system blackbox-exporter-954dd954b-tk9vl 1/1 Running 0 7d5h kube-system calico-kube-controllers-5f4b46ffb5-ggb7z 1/1 Running 0 7d5h ... $ kubectl who-can create clusterrolebinding No subjects found with permissions to create clusterrolebinding assigned through RoleBindings CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group cluster-admin-test test@test.com User ... Congratulations, you have just configured your cluster to authenticate against an Identity Provider using OpenID Connect!\n"},{"uri":"https://gardener.cloud/documentation/guides/client_tools/kubectl-apiserver/","title":"Automated deployment","tags":[],"description":"Automated deployment with kubectl","content":"Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don\u0026rsquo;t want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites   Create a service account user\nkubectl create serviceaccount deploy-user -n default   Bind a role to the newly created serviceuser\n !!! Warning !!! In this example the preconfigured role edit and the namespace default is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\n kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default   Get the URL of your API-server\nAPISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;)   Get the service account\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})   Generate a token for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)   Usage You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\nkubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml "},{"uri":"https://gardener.cloud/components/bouquet/","title":"Bouquet","tags":[],"description":"","content":"Bouquet Bouquet is a draft addon manager for the Gardener. It incorporates some of the requested features of the community but not yet all of them.\n Caution: This software is early alpha. It is not meant for production use and shall (currently) only serve as a possible outlook of what is possible with pre-deployed software on Gardener Kubernetes clusters.\n Installation If you want to deploy Bouquet on a target Gardener cluster, run the following:\nhelm install charts/bouquet \\  --name gardener-bouquet \\  --namespace garden This will deploy Bouquet with the required permissions into your garden cluster.\nStructure As of now, Bouquet comes with two new custom resources: AddonManifest and AddonInstance.\nAn AddonManifest can be considered equivalent to a Helm template. The manifest itself only contains metadata (like the name, default values etc.). The actual content of a manifest is specified via its source attribute. Currently, the only available source is a ConfigMap.\nAn AddonInstance references an AddonManifest and a target Shoot. It may also contain value overrides in its spec. As soon as an AddonInstance is created, Bouquet will apply the values to the templates and then ensure that the objects exist in the target shoot. If an AddonInstance is deleted, Bouquet will also make sure that the created objects are deleted as well.\nExample use case Say you want your cluster to contain istio right from the start. How can you do that?\nFirst you need to get the .yaml files necessary to deploy istio into your cluster. Download an istio release as follows:\nwget -O istio.yaml https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/istio/noauth/istio.yaml This will fetch a .yaml file containing all necessary kubernetes objects of istio. To make this data available in your garden cluster, create a configmap in your cluster via\nkubectl -n garden create configmap istio-files --from-file ./istio.yaml Now you need to create an AddonManifest that references this file and push it to Kubernetes. The file could look like the following:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonManifest\u0026#34;metadata:name:\u0026#34;istio-0.0.1\u0026#34;spec:configMap:\u0026#34;istio-files\u0026#34;You can submit this manifest to Kubernetes via kubectl (given that you saved the file to addonmanifest.yaml:\nkubectl -n garden apply -f addonmanifest.yaml Once this is done, the only thing left to do is to create an AddonInstance referencing both your target Shoot and your AddonManifest. This AddonInstance has to be in the same namespace as your target Shoot:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonInstance\u0026#34;metadata:name:\u0026#34;example\u0026#34;finalizers:- \u0026#34;bouquet\u0026#34;spec:manifest:namespace:\u0026#34;garden\u0026#34;name:\u0026#34;istio\u0026#34;version:\u0026#34;0.0.1\u0026#34;target:shoot:\u0026#34;addon-test\u0026#34;And apply it via kubectl (given that you saved the file to addoninstance.yaml):\nkubectl -n garden-addon-test apply -f addoninstance.yaml Bouquet will then start deploying your objects to the target Shoot once it is ready.\nOutlook / Future Since this is just a tech-preview, features like value / chart updates, more efficient templating, company addon guidelines etc. are not yet implemented / yet to come / yet to be discussed. It is also not yet clear whether this should eventually move into the Gardener or remain as a stand-alone component.\nCore points that have to be tackled are:\n Fire and forget mode (only deploy objects once, don\u0026rsquo;t monitor afterwards) Reconciliation (currently, updating behavior is not correctly implemented) Updates of an addon (-\u0026gt; Update strategies) Dependent addons / dependency resolution / dependency lifecycle  As such, contributions and help on shaping this topic is highly appreciated.\n"},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/25_markup/button/","title":"Button","tags":[],"description":"Nice buttons on your page.","content":"A button is a just a clickable button with optional icon.\n{{% button href=\u0026#34;https://getgrav.org/\u0026#34; %}}Get Grav{{% /button %}} {{% button href=\u0026#34;https://getgrav.org/\u0026#34; icon=\u0026#34;fa fa-download\u0026#34; %}}Get Grav with icon{{% /button %}} {{% button href=\u0026#34;https://getgrav.org/\u0026#34; icon=\u0026#34;fa fa-download\u0026#34; icon-position=\u0026#34;right\u0026#34; %}}Get Grav with icon right{{% /button %}} Get Grav\n \nGet Grav with icon\n Get Grav with icon right\n\n "},{"uri":"https://gardener.cloud/using-gardener/developer/experience/","title":"By Skill","tags":[],"description":"","content":"Learning Material Everything you need to know about running your software.     by Topic      by Experience Level    "},{"uri":"https://gardener.cloud/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/components/cert-broker/","title":"Cert Broker","tags":[],"description":"","content":"cert-broker Cert-Broker is a complementary component for Cert-Manager. It enables certificate management for Kubernetes clusters which don\u0026rsquo;t operate their own (in-cluster) Cert-Manager, e.g. for organizational resposibilities the Cert-Manager is located in another cluster.\nConcept To use or contribute to Cert-Broker it is fundamental to understand its main concept of control and target cluster.\n Control cluster: The cluster which operates an instance of Cert-Manager. Target cluster: The cluster which demands TLS certificates by Cert-Manager through the Cert-Broker.  Cert-Broker replicates Ingress resources from the target cluster to a predefined namespace in the control cluster. After the matching TLS Secret resource was created by Cert-Manager, Cert-Broker copies it to the appropriate Namespace in the target cluster. This works similarily in case the certificate is renewed.\nInstallation To install Cert-Broker on the control cluster, fill out the place holders and run\nhelm install charts/cert-broker \\ --name cert-broker \\ --namespace \u0026lt;Namespace\u0026gt; \\ --set certbroker.targetClusterSecret=\u0026lt;Target cluster Kubeconfig\u0026gt; \\ --set certmanager.dns=\u0026#34;{\u0026#34;\u0026lt;Domain\u0026gt;\u0026#34;.\u0026#34;\u0026lt;DNS Provider\u0026gt;\u0026#34;, \u0026#34;\u0026lt;Domain\u0026gt;\u0026#34;.\u0026#34;\u0026lt;DNS Provider\u0026gt;\u0026#34;}\u0026#34; \\ --set certmanager.clusterissuer=\u0026#34;\u0026lt;Issuer Name\u0026gt;\u0026#34; Limitations In case Cert-Manager issues certificates for the target cluster with Let\u0026rsquo;s Encrypt, the domain\u0026rsquo;s ownership can only be proven by DNS records, i.e. DNS01 Challenges must be used.\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/14_cicd/","title":"CI/CD","tags":[],"description":"","content":"CI/CD As an execution environment for CI/CD workloads, we use Concourse. We however abstract from the underlying \u0026ldquo;build executor\u0026rdquo; and instead offer a Pipeline Definition Contract, through which components declare their build pipelines as required.\nIn order to run continuous delivery workloads for all components contributing to the Gardener project, we operate a central service.\nTypical workloads encompass the execution of tests and builds of a variety of technologies, as well as building and publishing container images, typically containing build results.\nWe are building our CI/CD offering around some principles:\n container-native - each workload is executed within a container environment. Components may customise used container images automation - pipelines are generated without manual interaction self-service - components customise their pipelines by changing their sources standardisation  Learn more on our: Build Pipeline Reference Manual\n"},{"uri":"https://gardener.cloud/documentation/tutorials/node-overprovisioning/","title":"Cluster Overprovisioning","tags":[],"description":"How to overprovision cluster nodes for quick scaling and failover","content":"Cluster Overprovisioning This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work loads that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n Overprovisioning: Allocating more computer resources than is strictly necessary\nhttps://en.wikipedia.org/wiki/Overprovisioning\n When does the autoscaler change the size of the cluster? Below is a description of how the cluster behaves when there is a requirement to scale.\nScaling without overprovisioning  load hits the cluster (or a node is crashed) cannot schedule application-pods due to insufficient resources, scaling fails 💀 cluster-autoscaler notices and begins to provision new instance wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the application-pods and will schedule them  Scaling with Overprovisioning  load hits the cluster (or a node is crashed) placeholder-pods are evicted, scaling of application-pod is immediately successful placeholder-pods cannot be scheduled due to insufficient resources wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the placeholder pods and will schedule them  You can apply the above scenario one-to-one to the case when a node of the Hyperscaler dies.\nReal Scenario Test We executed normal and overprovisioning tests on a gardener cluster on different infrastructure provider (aws, azure, gcp, alicloud). All of them tested the downtime of the application pod running in the cluster, when a node dies.\nThe test results for the different IaaS provider are shown below.\nResults The results provided should only show how long the downtimes can be approximately.\n The downtime results could vary +- 1 min, because the minimum request interval in UpTime is 1 minute\n Amazon Normal Overprovisioning Azure Normal Overprovisioning GCP Normal Overprovisioning AliCloud Normal Overprovisioning Summary of results Normal    Provider AWS Azure GCP AliCloud     Node deleted 08:56 09:32 09:39 09:53   Pod rescheduled 09:17 09:50 09:53 10:14   Downtime 21 min 18 min 14 min 21 min    Overprovisioning    Provider AWS Azure GCP AliCloud     Node deleted 14:20 06:00 06:05 08:23   Pod rescheduled 14:22 06:02 06:06 08:25   Downtime 2 min 2 min 1 min 2 min    Test Description We deployed a nginx web server and a service of type LoadBalancer to expose it. So we are able to call our endpoint with external tools like UpTime to check the availability of our nginx. It takes only a few seconds to deploy a nginx web server on kubernetes, so we could say: when your endpoint works, your node is up and running.\nWe wanted to test how much time it takes, when your node gets killed and your cluster has to create a new one to run your application on it.\nkubectl get nodes # select the node where your nginx is running on kubectl delete node \u0026lt;NGINX-HOSTED-NODE\u0026gt; The downtime is tested with UpTime, which does every minute a request to our endpoint. Further we checked manually, if the node startup time and the timestamps on UpTime are almost similar.\nNext, deploy the overprovisioned version of our demo application and kill the node with the NGINX. As you can see - the pod comes up very fast and can serve content again.\n"},{"uri":"https://gardener.cloud/contribute/code/","title":"Code","tags":[],"description":"","content":" Contributing Code How to Contribute to the Open Source Project Gardener    You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "},{"uri":"https://gardener.cloud/community/","title":"Community","tags":[],"description":"","content":"Gardener Community Follow - Engage - Contribute\n@GardenerProject  Follow the latest project updates on Twitter  Community Meetings  You are welcome on our community meetings where you can engage with other contributors in person. See calendar for schedule or watch past recordings to get the idea.  GitHub  Eveyone is welcome to contribute with what they can - an issue or a pull request. Check Gardener project there and our contributors guide to help you get started.   Gardener Project  Watch videos and community meetings recordings on our YouTube channel  #gardener  Discuss Gardener on our Slack channel in the Kubernetes workspace   COMMUNITY The Gardener development process is an open process. Here are the general communication channels we use to communicate. We work with the wider community to create a strong, vibrant codebase. 60+ Committer  1300+ Merged Pull Requests  1400+ Github Stars  500+ Closed Community Issues   We are cordially inviting interested parties to join our weekly meetings. Here you can address questions regarding the direction of the project, technical problems and support.   Our Slack Channel is the best way to contact the experts in all questions about Kubernetes and the Gardener and share your ideas with them or ask for support.   Find out more about the project and consider making a contribution..     "},{"uri":"https://gardener.cloud/documentation/contribute/10_code/15_conf_secrets/","title":"Configuration and Usage","tags":[],"description":"","content":"Gardener Configuration and Usage Gardener automates the full lifecycle of Kubernetes clusters as a service. Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle. As a consequence, there are several configuration options for the various custom resources that are partially required.\nThis document describes the\n configuration and usage of Gardener as operator/administrator. configuration and usage of Gardener as end-user/stakeholder/customer.  Configuration and Usage of Gardener as Operator/Administrator When we use the terms \u0026ldquo;operator/administrator\u0026rdquo; we refer to both the people deploying and operating Gardener. Gardener consists out of four components:\n gardener-apiserver, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like Seeds and Shoots), and a component that contains multiple admission plugins. gardener-controller-manager, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining Shoots, reconciling Plants, etc.). gardener-scheduler, a component that assigns newly created Shoot clusters to appropriate Seed clusters. gardenlet, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of Shoots).  Each of these components have various configuration options. The gardener-apiserver uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags. The two other components are using so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener scheduler The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file. Please take a look at this example configuration. Information about the concepts of the Gardener scheduler can be found here\nConfiguration file for Gardenlet The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file. Please take a look at this example configuration. Information about the concepts of the Gardenlet can be found here\nSystem configuration After successful deployment of the four components you need to setup the system. Let\u0026rsquo;s first focus on some \u0026ldquo;static\u0026rdquo; configuration. When the gardenlet starts it scans the garden namespace of the garden cluster for Secrets that have influence on its reconciliation loops, mainly the Shoot reconciliation:\n  Internal domain secret, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called \u0026ldquo;internal\u0026rdquo; DNS records for the Shoot clusters, please see this for an example.\n This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components. The DNS records are normal DNS records but called \u0026ldquo;internal\u0026rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters. It is forbidden to change the internal domain secret if there are existing shoot clusters.    Default domain secrets (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., example.com), please see this for an example.\n Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster. As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don\u0026rsquo;t specify their own domain.    :warning: Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not tainted with seed.gardener.cloud/disable-dns. Seeds with this taint don\u0026rsquo;t create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don\u0026rsquo;t need to create the domain secrets.\n  Alerting secrets (optional), contain the alerting configuration and credentials for the AlertManager to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this for an example.\n If email alerting is configured:  An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster. Gardener will inject the SMTP credentials into the configuration of the AlertManager. The AlertManager will send emails to the configured email address in case any alerts are firing.   If an external AlertManager is configured:  Each shoot has a Prometheus responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret. This external AlertManager is not managed by Gardener and can be configured however the operator sees fit. Supported authentication types are no authentication, basic, or mutual TLS.      OpenVPN Diffie-Hellmann Key secret (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this for an example.\n If you don\u0026rsquo;t specify a custom key then a default key is used, but for productive landscapes it\u0026rsquo;s recommend to create a landscape-specific key and define it.    Global monitoring secrets (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.\n These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.    Apart from this \u0026ldquo;static\u0026rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener. As an operator/administrator you have to configure some of them to make the system work.\nConfiguration and Usage of Gardener as End-User/Stakeholder/Customer As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team. You don\u0026rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed. Take a look at this document - it describes which resources are offered by Gardener. You may want to have a more detailed look for Projects, SecretBindings, Shoots, Plants, and (Cluster)OpenIDConnectPresets.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/missing-registry-permission/","title":"Container image not pulled","tags":[],"description":"Wrong Container Image or Invalid Registry Permissions","content":"Problem Two of the most common problems are specifying the wrong container image or trying to use private images without providing registry credentials.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let\u0026rsquo;s see an example. We\u0026rsquo;ll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456 the command prompt doesn\u0026rsquo;t return and you can press ctrl+c\nError analysis We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\n$ (minikube) kubectl get pods NAME READY STATUS RESTARTS AGE client-5b65b6c866-cs4ch 1/1 Running 1 1m fail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u0026lt;invalid\u0026gt; vuejs-578574b75f-5x98z 1/1 Running 0 1d $ (minikube) For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8 As you can see in the events section, your image can\u0026rsquo;t be pulled\nName:\tfail-6667d7685d-7v6w8 Namespace:\tdefault Node:\tminikube/192.168.64.10 Start Time:\tWed, 22 Nov 2017 10:01:59 +0100 Labels:\tpod-template-hash=2223832418 run=fail Annotations:\tkubernetes.io/created-by={\u0026#34;kind\u0026#34;:\u0026#34;SerializedReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ReplicaSet\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;fail-6667d7685d\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\u0026#34;,\u0026#34;a... . . . . Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube 1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-9fr6r\u0026#34; 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \u0026#34;tutum/curl:1.123456\u0026#34; 1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \u0026#34;tutum/curl:1.123456\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found 1m\t\u0026lt;invalid\u0026gt;\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod 1m\t\u0026lt;invalid\u0026gt;\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \u0026#34;tutum/curl:1.123456\u0026#34; Why couldn\u0026rsquo;t Kubernetes pull the image? There are three primary candidates besides network connectivity issues:\n The image tag is incorrect The image doesn\u0026rsquo;t exist Kubernetes doesn\u0026rsquo;t have permissions to pull that image  If you don\u0026rsquo;t notice a typo in your image tag, then it\u0026rsquo;s time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn\u0026rsquo;t have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u0026lt;username\u0026gt; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;email\u0026gt; If the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn\u0026rsquo;t exist. Go to the Docker registry and check which tags are available for this image.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/image-pull-policy/","title":"Container image not updating","tags":[],"description":"Updating Images in your cluster during development","content":"Preface A container image should use a fixed tag or the content hash of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Many Kubernetes users have run into this problem. The story goes something like this:\n Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update your deployment Realize that the bug is still present Rinse and repeat steps 3 to 5 until you recognize this doesn\u0026rsquo;t work  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, hence it doesn\u0026rsquo;t attempt to do a docker pull. When the new Pods come up, they still use the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push the build result to the registry.\n#!/usr/bin/env bash  # Set the docker image name and the corresponding repository # Ensure that you change them in the deployment.yml as well. # You must be logged in with docker login… # # CHANGE THIS TO YOUR Docker.io SETTINGS # PROJECT=awesomeapp REPOSITORY=cp-enablement # exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x # build my nodeJS app # npm run build # get latest version IDs from the Docker.io registry and increment them # VERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e \u0026#39;s/[][]//g\u0026#39; -e \u0026#39;s/\u0026#34;//g\u0026#39; -e \u0026#39;s/ //g\u0026#39; | tr \u0026#39;}\u0026#39; \u0026#39;\\n\u0026#39; | awk -F: \u0026#39;{print $3}\u0026#39; | grep v| tail -n 1) VERSION=${VERSION:1} ((VERSION++)) VERSION=\u0026#34;v$VERSION\u0026#34; # build a new docker image # echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Building new image\u0026#39; # Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875) docker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log RESULT=$(cat /tmp/docker_build_result.log | tail -n 1) if [[ \u0026#34;$RESULT\u0026#34; != *Successfully* ]]; then exit -1 fi echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Push new image\u0026#39; docker push $REPOSITORY/$PROJECT:$VERSION "},{"uri":"https://gardener.cloud/documentation/050-tutorials/content/","title":"Content","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/contribute/10_code/10-contribution_guide/","title":"Contribution Guide","tags":[],"description":"","content":"Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  Individual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project\u0026rsquo;s license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. “Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.”).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"},{"uri":"https://gardener.cloud/api-reference/core/","title":"Core","tags":[],"description":"","content":"Packages:\n  core.gardener.cloud/v1beta1   core.gardener.cloud/v1beta1  Package v1beta1 is a version of the API.\nResource Types:  BackupBucket  BackupEntry  CloudProfile  ControllerInstallation  ControllerRegistration  Plant  Project  Quota  SecretBinding  Seed  Shoot  BackupBucket   BackupBucket holds details about backup bucket\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec     Specification of the Backup Bucket.\n     provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n       status  BackupBucketStatus     Most recently observed status of the Backup Bucket.\n    BackupEntry   BackupEntry holds details about shoot backup.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec     (Optional) Spec contains the specification of the Backup Entry.\n     bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n       status  BackupEntryStatus     (Optional) Status contains the most recently observed status of the Backup Entry.\n    CloudProfile   CloudProfile represents certain properties about a provider environment.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  CloudProfile    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  CloudProfileSpec     (Optional) Spec defines the provider environment properties.\n     caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n       ControllerInstallation   ControllerInstallation represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerInstallation    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerInstallationSpec     Spec contains the specification of this installation.\n     registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n       status  ControllerInstallationStatus     Status contains the status of this installation.\n    ControllerRegistration   ControllerRegistration represents a registration of an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerRegistration    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerRegistrationSpec     Spec contains the specification of this registration.\n     resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n       Plant      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Plant    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  PlantSpec     Spec contains the specification of this Plant.\n     secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n       status  PlantStatus     Status contains the status of this Plant.\n    Project   Project holds certain properties about a Gardener project.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Project    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ProjectSpec     (Optional) Spec defines the project properties.\n     createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ProjectStatus     (Optional) Most recently observed status of the Project.\n    Quota      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Quota    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  QuotaSpec     (Optional) Spec defines the Quota constraints.\n     clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n       SecretBinding      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  SecretBinding    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret object in the same or another namespace.\n    quotas  []Kubernetes core/v1.ObjectReference     (Optional) Quotas is a list of references to Quota objects in the same or another namespace.\n    Seed   Seed represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Seed    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     Spec contains the specification of this installation.\n     backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n       status  SeedStatus     Status contains the status of this installation.\n    Shoot      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Shoot    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the Shoot cluster.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ShootStatus     (Optional) Most recently observed status of the Shoot cluster.\n    Addon   (Appears on: KubernetesDashboard, NginxIngress)  Addon allows enabling or disabling a specific addon and is used to derive from.\n   Field Description      enabled  bool    Enabled indicates whether the addon is enabled or not.\n    Addons   (Appears on: ShootSpec)  Addons is a collection of configuration for specific addons which are managed by the Gardener.\n   Field Description      kubernetesDashboard  KubernetesDashboard     (Optional) KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n    nginxIngress  NginxIngress     (Optional) NginxIngress holds configuration settings for the nginx-ingress addon.\n    AdmissionPlugin   (Appears on: KubeAPIServerConfig)  AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n   Field Description      name  string    Name is the name of the plugin.\n    config  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) Config is the configuration of the plugin.\n    Alerting   (Appears on: Monitoring)  Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n   Field Description      emailReceivers  []string    (Optional) MonitoringEmailReceivers is a list of recipients for alerts\n    AuditConfig   (Appears on: KubeAPIServerConfig)  AuditConfig contains settings for audit of the api server\n   Field Description      auditPolicy  AuditPolicy     (Optional) AuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n    AuditPolicy   (Appears on: AuditConfig)  AuditPolicy contains audit policy for kube-apiserver\n   Field Description      configMapRef  Kubernetes core/v1.ObjectReference     (Optional) ConfigMapRef is a reference to a ConfigMap object in the same namespace, which contains the audit policy for the kube-apiserver.\n    AvailabilityZone   (Appears on: Region)  AvailabilityZone is an availability zone.\n   Field Description      name  string    Name is an an availability zone name.\n    unavailableMachineTypes  []string    (Optional) UnavailableMachineTypes is a list of machine type names that are not availability in this zone.\n    unavailableVolumeTypes  []string    (Optional) UnavailableVolumeTypes is a list of volume type names that are not availability in this zone.\n    BackupBucketProvider   (Appears on: BackupBucketSpec)  BackupBucketProvider holds the details of cloud provider of the object store.\n   Field Description      type  string    Type is the type of provider.\n    region  string    Region is the region of the bucket.\n    BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the specification of a Backup Bucket.\n   Field Description      provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus holds the most recently observed status of the Backup Bucket.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus is the configuration passed to BackupBucket resource.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupBucket.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the BackupBucket\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the specification of a Backup Entry.\n   Field Description      bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus holds the most recently observed status of the Backup Entry.\n   Field Description      lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupEntry.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the BackupEntry\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    CRI   (Appears on: Worker)  CRI contains information about the Container Runtimes.\n   Field Description      name  CRIName     The name of the CRI library\n    containerRuntimes  []ContainerRuntime     (Optional) ContainerRuntimes is the list of the required container runtimes supported for a worker pool.\n    CRIName (string alias)\n  (Appears on: CRI)  CRIName is a type alias for the CRI name string.\nCloudInfo   (Appears on: ClusterInfo)  CloudInfo contains information about the cloud\n   Field Description      type  string    Type is the cloud type\n    region  string    Region is the cloud region\n    CloudProfileSpec   (Appears on: CloudProfile)  CloudProfileSpec is the specification of a CloudProfile. It must contain exactly one of its defined keys.\n   Field Description      caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    ClusterAutoscaler   (Appears on: Kubernetes)  ClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n   Field Description      scaleDownDelayAfterAdd  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1 hour).\n    scaleDownDelayAfterDelete  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (defaults to ScanInterval).\n    scaleDownDelayAfterFailure  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n    scaleDownUnneededTime  Kubernetes meta/v1.Duration     (Optional) ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30 mins).\n    scaleDownUtilizationThreshold  float64    (Optional) ScaleDownUtilizationThreshold defines the threshold in % under which a node is being removed\n    scanInterval  Kubernetes meta/v1.Duration     (Optional) ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n    ClusterInfo   (Appears on: PlantStatus)  ClusterInfo contains information about the Plant cluster\n   Field Description      cloud  CloudInfo     Cloud describes the cloud information\n    kubernetes  KubernetesInfo     Kubernetes describes kubernetes meta information (e.g., version)\n    Condition   (Appears on: ControllerInstallationStatus, PlantStatus, SeedStatus, ShootStatus)  Condition holds the information about the state of a resource.\n   Field Description      type  ConditionType     Type of the Shoot condition.\n    status  ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastTransitionTime  Kubernetes meta/v1.Time     Last time the condition transitioned from one status to another.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the condition was updated.\n    reason  string    The reason for the condition\u0026rsquo;s last transition.\n    message  string    A human readable message indicating details about the transition.\n    codes  []ErrorCode     (Optional) Well-defined error codes in case the condition reports a problem.\n    ConditionStatus (string alias)\n  (Appears on: Condition)  ConditionStatus is the status of a condition.\nConditionType (string alias)\n  (Appears on: Condition)  ConditionType is a string alias.\nContainerRuntime   (Appears on: CRI)  ContainerRuntime contains information about worker\u0026rsquo;s available container runtime\n   Field Description      type  string    Type is the type of the Container Runtime.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to container runtime resource.\n    ControllerDeployment   (Appears on: ControllerRegistrationSpec)  ControllerDeployment contains information for how this controller is deployed.\n   Field Description      type  string    Type is the deployment type.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains type-specific configuration.\n    policy  ControllerDeploymentPolicy     (Optional) Policy controls how the controller is deployed. It defaults to \u0026lsquo;OnDemand\u0026rsquo;.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional label selector for seeds. Only if the labels match then this controller will be considered for a deployment. An empty list means that all seeds are selected.\n    ControllerDeploymentPolicy (string alias)\n  (Appears on: ControllerDeployment)  ControllerDeploymentPolicy is a string alias.\nControllerInstallationSpec   (Appears on: ControllerInstallation)  ControllerInstallationSpec is the specification of a ControllerInstallation.\n   Field Description      registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n    ControllerInstallationStatus   (Appears on: ControllerInstallation)  ControllerInstallationStatus is the status of a ControllerInstallation.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a ControllerInstallations\u0026rsquo;s current state.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains type-specific status.\n    ControllerRegistrationSpec   (Appears on: ControllerRegistration)  ControllerRegistrationSpec is the specification of a ControllerRegistration.\n   Field Description      resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n    ControllerResource   (Appears on: ControllerRegistrationSpec)  ControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, \u0026hellip;) and the actual type for this kind (aws-route53, gcp, auditlog, \u0026hellip;).\n   Field Description      kind  string    Kind is the resource kind, for example \u0026ldquo;OperatingSystemConfig\u0026rdquo;.\n    type  string    Type is the resource type, for example \u0026ldquo;coreos\u0026rdquo; or \u0026ldquo;ubuntu\u0026rdquo;.\n    globallyEnabled  bool    (Optional) GloballyEnabled determines if this ControllerResource is required by all Shoot clusters.\n    reconcileTimeout  Kubernetes meta/v1.Duration     (Optional) ReconcileTimeout defines how long Gardener should wait for the resource reconciliation.\n    primary  bool    (Optional) Primary determines if the controller backed by this ControllerRegistration is responsible for the extension resource\u0026rsquo;s lifecycle. This field defaults to true. There must be exactly one primary controller for this kind/type combination.\n    DNS   (Appears on: ShootSpec)  DNS holds information about the provider, the hosted zone id and the domain.\n   Field Description      domain  string    (Optional) Domain is the external available domain of the Shoot cluster. This domain will be written into the kubeconfig that is handed out to end-users.\n    providers  []DNSProvider     (Optional) Providers is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if not a default domain is used.\n    DNSIncludeExclude   (Appears on: DNSProvider)     Field Description      include  []string    (Optional) Include is a list of resources that shall be included.\n    exclude  []string    (Optional) Exclude is a list of resources that shall be excluded.\n    DNSProvider   (Appears on: DNS)  DNSProvider contains information about a DNS provider.\n   Field Description      domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    primary  bool    (Optional) Primary indicates that this DNSProvider is used for shoot related domains.\n    secretName  string    (Optional) SecretName is a name of a secret containing credentials for the stated domain and the provider. When not specified, the Gardener will use the cloud provider credentials referenced by the Shoot and try to find respective credentials there (primary provider only). Specifying this field may override this behavior, i.e. forcing the Gardener to only look into the given secret.\n    type  string    (Optional) Type is the DNS provider type.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    DataVolume   (Appears on: Worker)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    Endpoint   (Appears on: PlantSpec)  Endpoint is an endpoint for monitoring, logging and other services around the plant.\n   Field Description      name  string    Name is the name of the endpoint\n    url  string    URL is the url of the endpoint\n    purpose  string    Purpose is the purpose of the endpoint\n    ErrorCode (string alias)\n  (Appears on: Condition, LastError)  ErrorCode is a string alias.\nExpirableVersion   (Appears on: KubernetesSettings, MachineImage)  ExpirableVersion contains a version and an expiration date.\n   Field Description      version  string    Version is the version identifier.\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which this version expires.\n    classification  VersionClassification     (Optional) Classification defines the state of a version (preview, supported, deprecated)\n    Extension   (Appears on: ShootSpec)  Extension contains type and provider information for Shoot extensions.\n   Field Description      type  string    Type is the type of the extension resource.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to extension resource.\n    disabled  bool    (Optional) Disabled allows to disable extensions that were marked as \u0026lsquo;globally enabled\u0026rsquo; by Gardener administrators.\n    Gardener   (Appears on: SeedStatus, ShootStatus)  Gardener holds the information about the Gardener version that operated a resource.\n   Field Description      id  string    ID is the Docker container id of the Gardener which last acted on a resource.\n    name  string    Name is the hostname (pod name) of the Gardener which last acted on a resource.\n    version  string    Version is the version of the Gardener which last acted on a resource.\n    Hibernation   (Appears on: ShootSpec)  Hibernation contains information whether the Shoot is suspended or not.\n   Field Description      enabled  bool    (Optional) Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot\u0026rsquo;s desired state is to be hibernated. If it is false or nil, the Shoot\u0026rsquo;s desired state is to be awaken.\n    schedules  []HibernationSchedule     (Optional) Schedules determine the hibernation schedules.\n    HibernationSchedule   (Appears on: Hibernation)  HibernationSchedule determines the hibernation schedule of a Shoot. A Shoot will be regularly hibernated at each start time and will be woken up at each end time. Start or End can be omitted, though at least one of each has to be specified.\n   Field Description      start  string    (Optional) Start is a Cron spec at which time a Shoot will be hibernated.\n    end  string    (Optional) End is a Cron spec at which time a Shoot will be woken up.\n    location  string    (Optional) Location is the time location in which both start and and shall be evaluated.\n    HorizontalPodAutoscalerConfig   (Appears on: KubeControllerManagerConfig)  HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      cpuInitializationPeriod  Kubernetes meta/v1.Duration     (Optional) The period after which a ready pod transition is considered to be the first.\n    downscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last downscale, before another downscale can be performed in horizontal pod autoscaler.\n    downscaleStabilization  Kubernetes meta/v1.Duration     (Optional) The configurable window at which the controller will choose the highest recommendation for autoscaling.\n    initialReadinessDelay  Kubernetes meta/v1.Duration     (Optional) The configurable period at which the horizontal pod autoscaler considers a Pod “not yet ready” given that it’s unready and it has transitioned to unready during that time.\n    syncPeriod  Kubernetes meta/v1.Duration     (Optional) The period for syncing the number of pods in horizontal pod autoscaler.\n    tolerance  float64    (Optional) The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n    upscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last upscale, before another upscale can be performed in horizontal pod autoscaler.\n    KubeAPIServerConfig   (Appears on: Kubernetes)  KubeAPIServerConfig contains configuration settings for the kube-apiserver.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     admissionPlugins  []AdmissionPlugin     (Optional) AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding configuration.\n    apiAudiences  []string    (Optional) APIAudiences are the identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. Defaults to [\u0026ldquo;kubernetes\u0026rdquo;].\n    auditConfig  AuditConfig     (Optional) AuditConfig contains configuration settings for the audit of the kube-apiserver.\n    enableBasicAuthentication  bool    (Optional) EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n    oidcConfig  OIDCConfig     (Optional) OIDCConfig contains configuration settings for the OIDC provider.\n    runtimeConfig  map[string]bool    (Optional) RuntimeConfig contains information about enabled or disabled APIs.\n    serviceAccountConfig  ServiceAccountConfig     (Optional) ServiceAccountConfig contains configuration settings for the service account handling of the kube-apiserver.\n    KubeControllerManagerConfig   (Appears on: Kubernetes)  KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     horizontalPodAutoscaler  HorizontalPodAutoscalerConfig     (Optional) HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n    nodeCIDRMaskSize  int32    (Optional) NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24)\n    KubeProxyConfig   (Appears on: Kubernetes)  KubeProxyConfig contains configuration settings for the kube-proxy.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     mode  ProxyMode     (Optional) Mode specifies which proxy mode to use. defaults to IPTables.\n    KubeSchedulerConfig   (Appears on: Kubernetes)  KubeSchedulerConfig contains configuration settings for the kube-scheduler.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     kubeMaxPDVols  string    (Optional) KubeMaxPDVols allows to configure the KUBE_MAX_PD_VOLS environment variable for the kube-scheduler. Please find more information here: https://kubernetes.io/docs/concepts/storage/storage-limits/#custom-limits Note that using this field is considered alpha-/experimental-level and is on your own risk. You should be aware of all the side-effects and consequences when changing it.\n    KubeletConfig   (Appears on: Kubernetes, WorkerKubernetes)  KubeletConfig contains configuration settings for the kubelet.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     cpuCFSQuota  bool    (Optional) CPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n    cpuManagerPolicy  string    (Optional) CPUManagerPolicy allows to set alternative CPU management policies (default: none).\n    evictionHard  KubeletConfigEviction     (Optional) EvictionHard describes a set of eviction thresholds (e.g. memory.available   evictionMaxPodGracePeriod  int32    (Optional) EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. Default: 90\n    evictionMinimumReclaim  KubeletConfigEvictionMinimumReclaim     (Optional) EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure. Default: 0 for each resource\n    evictionPressureTransitionPeriod  Kubernetes meta/v1.Duration     (Optional) EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. Default: 4m0s\n    evictionSoft  KubeletConfigEviction     (Optional) EvictionSoft describes a set of eviction thresholds (e.g. memory.available   evictionSoftGracePeriod  KubeletConfigEvictionSoftGracePeriod     (Optional) EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction. Default: memory.available: 1m30s nodefs.available: 1m30s nodefs.inodesFree: 1m30s imagefs.available: 1m30s imagefs.inodesFree: 1m30s\n    maxPods  int32    (Optional) MaxPods is the maximum number of Pods that are allowed by the Kubelet. Default: 110\n    podPidsLimit  int64    (Optional) PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n    imagePullProgressDeadline  Kubernetes meta/v1.Duration     (Optional) ImagePullProgressDeadline describes the time limit under which if no pulling progress is made, the image pulling will be cancelled. Default: 1m\n    failSwapOn  bool    (Optional) FailSwapOn makes the Kubelet fail to start if swap is enabled on the node. (default true).\n    KubeletConfigEviction   (Appears on: KubeletConfig)  KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n   Field Description      memoryAvailable  string    (Optional) MemoryAvailable is the threshold for the free memory on the host server.\n    imageFSAvailable  string    (Optional) ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  string    (Optional) ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n    nodeFSAvailable  string    (Optional) NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  string    (Optional) NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n    KubeletConfigEvictionMinimumReclaim   (Appears on: KubeletConfig)  KubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.\n   Field Description      memoryAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MemoryAvailable is the threshold for the memory reclaim on the host server.\n    imageFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n    nodeFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n    KubeletConfigEvictionSoftGracePeriod   (Appears on: KubeletConfig)  KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n   Field Description      memoryAvailable  Kubernetes meta/v1.Duration     (Optional) MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n    imageFSAvailable  Kubernetes meta/v1.Duration     (Optional) ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n    imageFSInodesFree  Kubernetes meta/v1.Duration     (Optional) ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n    nodeFSAvailable  Kubernetes meta/v1.Duration     (Optional) NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n    nodeFSInodesFree  Kubernetes meta/v1.Duration     (Optional) NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n    Kubernetes   (Appears on: ShootSpec)  Kubernetes contains the version and configuration variables for the Shoot control plane.\n   Field Description      allowPrivilegedContainers  bool    (Optional) AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot (default: true).\n    clusterAutoscaler  ClusterAutoscaler     (Optional) ClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n    kubeAPIServer  KubeAPIServerConfig     (Optional) KubeAPIServer contains configuration settings for the kube-apiserver.\n    kubeControllerManager  KubeControllerManagerConfig     (Optional) KubeControllerManager contains configuration settings for the kube-controller-manager.\n    kubeScheduler  KubeSchedulerConfig     (Optional) KubeScheduler contains configuration settings for the kube-scheduler.\n    kubeProxy  KubeProxyConfig     (Optional) KubeProxy contains configuration settings for the kube-proxy.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    version  string    Version is the semantic Kubernetes version to use for the Shoot cluster.\n    KubernetesConfig   (Appears on: KubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)  KubernetesConfig contains common configuration fields for the control plane components.\n   Field Description      featureGates  map[string]bool    (Optional) FeatureGates contains information about enabled feature gates.\n    KubernetesDashboard   (Appears on: Addons)  KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     authenticationMode  string    (Optional) AuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n    KubernetesInfo   (Appears on: ClusterInfo)  KubernetesInfo contains the version and configuration variables for the Plant cluster.\n   Field Description      version  string    Version is the semantic Kubernetes version to use for the Plant cluster.\n    KubernetesSettings   (Appears on: CloudProfileSpec)  KubernetesSettings contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n   Field Description      versions  []ExpirableVersion     (Optional) Versions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n    LastError   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastError indicates the last occurred error for an operation on a resource.\n   Field Description      description  string    A human readable message indicating details about the last error.\n    taskID  string    (Optional) ID of the task which caused this last error\n    codes  []ErrorCode     (Optional) Well-defined error codes of the last error(s).\n    lastUpdateTime  Kubernetes meta/v1.Time     (Optional) Last time the error was reported\n    LastOperation   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastOperation indicates the type and the state of the last operation, along with a description message and a progress indicator.\n   Field Description      description  string    A human readable message indicating details about the last operation.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the operation state transitioned from one to another.\n    progress  int32    The progress in percentage (0-100) of the last operation.\n    state  LastOperationState     Status of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.\n    type  LastOperationType     Type of the last operation, one of Create, Reconcile, Delete.\n    LastOperationState (string alias)\n  (Appears on: LastOperation)  LastOperationState is a string alias.\nLastOperationType (string alias)\n  (Appears on: LastOperation)  LastOperationType is a string alias.\nMachine   (Appears on: Worker)  Machine contains information about the machine type and image.\n   Field Description      type  string    Type is the machine type of the worker group.\n    image  ShootMachineImage     (Optional) Image holds information about the machine image to use for all nodes of this pool. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    MachineImage   (Appears on: CloudProfileSpec)  MachineImage defines the name and multiple versions of the machine image in any environment.\n   Field Description      name  string    Name is the name of the image.\n    versions  []ExpirableVersion     Versions contains versions and expiration dates of the machine image\n    MachineType   (Appears on: CloudProfileSpec)  MachineType contains certain properties of a machine type.\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     CPU is the number of CPUs for this machine type.\n    gpu  k8s.io/apimachinery/pkg/api/resource.Quantity     GPU is the number of GPUs for this machine type.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     Memory is the amount of memory for this machine type.\n    name  string    Name is the name of the machine type.\n    storage  MachineTypeStorage     (Optional) Storage is the amount of storage associated with the root volume of this machine type.\n    usable  bool    (Optional) Usable defines if the machine type can be used for shoot clusters.\n    MachineTypeStorage   (Appears on: MachineType)  MachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n   Field Description      class  string    Class is the class of the storage type.\n    size  k8s.io/apimachinery/pkg/api/resource.Quantity     StorageSize is the storage size.\n    type  string    Type is the type of the storage.\n    Maintenance   (Appears on: ShootSpec)  Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n   Field Description      autoUpdate  MaintenanceAutoUpdate     (Optional) AutoUpdate contains information about which constraints should be automatically updated.\n    timeWindow  MaintenanceTimeWindow     (Optional) TimeWindow contains information about the time window for maintenance operations.\n    confineSpecUpdateRollout  bool    (Optional) ConfineSpecUpdateRollout prevents that changes/updates to the shoot specification will be rolled out immediately. Instead, they are rolled out during the shoot\u0026rsquo;s maintenance time window. There is one exception that will trigger an immediate roll out which is changes to the Spec.Hibernation.Enabled field.\n    MaintenanceAutoUpdate   (Appears on: Maintenance)  MaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n   Field Description      kubernetesVersion  bool    KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).\n    machineImageVersion  bool    MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n    MaintenanceTimeWindow   (Appears on: Maintenance)  MaintenanceTimeWindow contains information about the time window for maintenance operations.\n   Field Description      begin  string    Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, a random value will be computed.\n    end  string    End is the end of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, the value will be computed based on the \u0026ldquo;Begin\u0026rdquo; value.\n    Monitoring   (Appears on: ShootSpec)  Monitoring contains information about the monitoring configuration for the shoot.\n   Field Description      alerting  Alerting     (Optional) Alerting contains information about the alerting configuration for the shoot cluster.\n    NamedResourceReference   (Appears on: ShootSpec)  NamedResourceReference is a named reference to a resource.\n   Field Description      name  string    Name of the resource reference.\n    resourceRef  Kubernetes autoscaling/v1.CrossVersionObjectReference     ResourceRef is a reference to a resource.\n    Networking   (Appears on: ShootSpec)  Networking defines networking parameters for the shoot cluster.\n   Field Description      type  string    Type identifies the type of the networking plugin.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to network resource.\n    pods  string    (Optional) Pods is the CIDR of the pod network.\n    nodes  string    (Optional) Nodes is the CIDR of the entire node network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    NginxIngress   (Appears on: Addons)  NginxIngress describes configuration values for the nginx-ingress addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     loadBalancerSourceRanges  []string    (Optional) LoadBalancerSourceRanges is list of whitelist IP sources for NginxIngress\n    config  map[string]string    (Optional) Config contains custom configuration for the nginx-ingress-controller configuration. See https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n    externalTrafficPolicy  Kubernetes core/v1.ServiceExternalTrafficPolicyType     (Optional) ExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service exposing the nginx-ingress. Defaults to Cluster.\n    OIDCConfig   (Appears on: KubeAPIServerConfig)  OIDCConfig contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientAuthentication  OpenIDConnectClientAuthentication     (Optional) ClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n    clientID  string    (Optional) The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    (Optional) The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n    requiredClaims  map[string]string    (Optional) ATTENTION: Only meaningful for Kubernetes \u0026gt;= 1.11 key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \u0026ldquo;sub\u0026rdquo;)\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OIDCConfig)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    secret  string    (Optional) The client Secret for the OpenID Connect client.\n    PlantSpec   (Appears on: Plant)  PlantSpec is the specification of a Plant.\n   Field Description      secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n    PlantStatus   (Appears on: Plant)  PlantStatus is the status of a Plant.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Plant\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Plant. It corresponds to the Plant\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterInfo  ClusterInfo     ClusterInfo is additional computed information about the newly added cluster (Plant)\n    ProjectMember   (Appears on: ProjectSpec)  ProjectMember is a member of a project.\n   Field Description      Subject  Kubernetes rbac/v1.Subject      (Members of Subject are embedded into this type.) Subject is representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    role  string    Role represents the role of this member. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the roles list. TODO: Remove this field in favor of the owner role in v1.\n    roles  []string    (Optional) Roles represents the list of roles of this member.\n    ProjectPhase (string alias)\n  (Appears on: ProjectStatus)  ProjectPhase is a label for the condition of a project at the current time.\nProjectSpec   (Appears on: Project)  ProjectSpec is the specification of a Project.\n   Field Description      createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ProjectStatus   (Appears on: Project)  ProjectStatus holds the most recently observed status of the project.\n   Field Description      observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this project.\n    phase  ProjectPhase     Phase is the current phase of the project.\n    ProjectTolerations   (Appears on: ProjectSpec)  ProjectTolerations contains the tolerations for taints on seed clusters.\n   Field Description      defaults  []Toleration     (Optional) Defaults contains a list of tolerations that are added to the shoots in this project by default.\n    whitelist  []Toleration     (Optional) Whitelist contains a list of tolerations that are allowed to be added to the shoots in this project. Please note that this list may only be added by users having the spec-tolerations-whitelist verb for project resources.\n    Provider   (Appears on: ShootSpec)  Provider contains provider-specific information that are handed-over to the provider-specific extension controller.\n   Field Description      type  string    Type is the type of the provider.\n    controlPlaneConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete definition in the documentation of your provider extension.\n    infrastructureConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete definition in the documentation of your provider extension.\n    workers  []Worker     Workers is a list of worker groups.\n    ProxyMode (string alias)\n  (Appears on: KubeProxyConfig)  ProxyMode available in Linux platform: \u0026lsquo;userspace\u0026rsquo; (older, going to be EOL), \u0026lsquo;iptables\u0026rsquo; (newer, faster), \u0026lsquo;ipvs\u0026rsquo; (newest, better in performance and scalability). As of now only \u0026lsquo;iptables\u0026rsquo; and \u0026lsquo;ipvs\u0026rsquo; is supported by Gardener. In Linux platform, if the iptables proxy is selected, regardless of how, but the system\u0026rsquo;s kernel or iptables versions are insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to \u0026lsquo;ipvs\u0026rsquo;, and the fall back path is firstly iptables and then userspace.\nQuotaSpec   (Appears on: Quota)  QuotaSpec is the specification of a Quota.\n   Field Description      clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n    Region   (Appears on: CloudProfileSpec)  Region contains certain properties of a region.\n   Field Description      name  string    Name is a region name.\n    zones  []AvailabilityZone     (Optional) Zones is a list of availability zones in this region.\n    labels  map[string]string    (Optional) Labels is an optional set of key-value pairs that contain certain administrator-controlled labels for this region. It can be used by Gardener administrators/operators to provide additional information about a region, e.g. wrt quality, reliability, access restrictions, etc.\n    SeedBackup   (Appears on: SeedSpec)  SeedBackup contains the object store configuration for backups for shoot (currently only etcd).\n   Field Description      provider  string    Provider is a provider name.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    region  string    (Optional) Region is a region name.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.\n    SeedDNS   (Appears on: SeedSpec)  SeedDNS contains DNS-relevant information about this seed cluster.\n   Field Description      ingressDomain  string    IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters.\n    SeedNetworks   (Appears on: SeedSpec)  SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network.\n    pods  string    Pods is the CIDR of the pod network.\n    services  string    Services is the CIDR of the service network.\n    shootDefaults  ShootNetworks     (Optional) ShootDefaults contains the default networks CIDRs for shoots.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    SeedProvider   (Appears on: SeedSpec)  SeedProvider defines the provider type and region for this Seed cluster.\n   Field Description      type  string    Type is the name of the provider.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to Seed resource.\n    region  string    Region is a name of a region.\n    SeedSettingExcessCapacityReservation   (Appears on: SeedSettings)  SeedSettingExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed. When enabled then this is done via PodPriority and requires the Seed cluster to have Kubernetes version 1.11 or the PodPriority feature gate as well as the scheduling.k8s.io/v1alpha1 API group enabled.\n   Field Description      enabled  bool    Enabled controls whether the excess capacity reservation should be enabled.\n    SeedSettingLoadBalancerServices   (Appears on: SeedSettings)  SeedSettingLoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of annotations that will be injected/merged into every load balancer service object.\n    SeedSettingScheduling   (Appears on: SeedSettings)  SeedSettingScheduling controls settings for scheduling decisions for the seed.\n   Field Description      visible  bool    Visible controls whether the gardener-scheduler shall consider this seed when scheduling shoots. Invisible seeds are not considered by the scheduler.\n    SeedSettingShootDNS   (Appears on: SeedSettings)  SeedSettingShootDNS controls the shoot DNS settings for the seed.\n   Field Description      enabled  bool    Enabled controls whether the DNS for shoot clusters should be enabled. When disabled then all shoots using the seed won\u0026rsquo;t get any DNS providers, DNS records, and no DNS extension controller is required to be installed here. This is useful for environments where DNS is not required.\n    SeedSettings   (Appears on: SeedSpec)  SeedSettings contains certain settings for this seed cluster.\n   Field Description      excessCapacityReservation  SeedSettingExcessCapacityReservation     (Optional) ExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.\n    scheduling  SeedSettingScheduling     (Optional) Scheduling controls settings for scheduling decisions for the seed.\n    shootDNS  SeedSettingShootDNS     (Optional) ShootDNS controls the shoot DNS settings for the seed.\n    loadBalancerServices  SeedSettingLoadBalancerServices     (Optional) LoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n    SeedSpec   (Appears on: Seed)  SeedSpec is the specification of a Seed.\n   Field Description      backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    SeedStatus   (Appears on: Seed)  SeedStatus is the status of a Seed.\n   Field Description      gardener  Gardener     (Optional) Gardener holds information about the Gardener which last acted on the Shoot.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the Kubernetes version of the seed cluster.\n    conditions  []Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the Seed\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    SeedTaint   (Appears on: SeedSpec)  SeedTaint describes a taint on a seed.\n   Field Description      key  string    Key is the taint key to be applied to a seed.\n    value  string    (Optional) Value is the taint value corresponding to the taint key.\n    SeedVolume   (Appears on: SeedSpec)  SeedVolume contains settings for persistentvolumes created in the seed cluster.\n   Field Description      minimumSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinimumSize defines the minimum size that should be used for PVCs in the seed.\n    providers  []SeedVolumeProvider     (Optional) Providers is a list of storage class provisioner types for the seed.\n    SeedVolumeProvider   (Appears on: SeedVolume)  SeedVolumeProvider is a storage class provisioner type.\n   Field Description      purpose  string    Purpose is the purpose of this provider.\n    name  string    Name is the name of the storage class provisioner type.\n    ServiceAccountConfig   (Appears on: KubeAPIServerConfig)  ServiceAccountConfig is the kube-apiserver configuration for service accounts.\n   Field Description      issuer  string    (Optional) Issuer is the identifier of the service account token issuer. The issuer will assert this identifier in \u0026ldquo;iss\u0026rdquo; claim of issued tokens. This value is a string or URI. Defaults to URI of the API server.\n    signingKeySecretName  Kubernetes core/v1.LocalObjectReference     (Optional) SigningKeySecret is a reference to a secret that contains an optional private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. Only useful if service account tokens are also issued by another external system.\n    ShootMachineImage   (Appears on: Machine)  ShootMachineImage defines the name and the version of the shoot\u0026rsquo;s machine image in any environment. Has to be defined in the respective CloudProfile.\n   Field Description      name  string    Name is the name of the image.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the shoot\u0026rsquo;s individual configuration passed to an extension resource.\n    version  string    (Optional) Version is the version of the shoot\u0026rsquo;s image. If version is not provided, it will be defaulted to the latest version from the CloudProfile.\n    ShootNetworks   (Appears on: SeedNetworks)  ShootNetworks contains the default networks CIDRs for shoots.\n   Field Description      pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    ShootPurpose (string alias)\n  (Appears on: ShootSpec)  ShootPurpose is a type alias for string.\nShootSpec   (Appears on: Shoot)  ShootSpec is the specification of a Shoot.\n   Field Description      addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ShootStatus   (Appears on: Shoot)  ShootStatus holds the most recently observed status of the Shoot cluster.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Shoots\u0026rsquo;s current state.\n    constraints  []Condition     (Optional) Constraints represents conditions of a Shoot\u0026rsquo;s current state that constraint some operations on it.\n    gardener  Gardener     Gardener holds information about the Gardener which last acted on the Shoot.\n    hibernated  bool    IsHibernated indicates whether the Shoot is currently hibernated.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the Shoot.\n    lastErrors  []LastError     (Optional) LastErrors holds information about the last occurred error(s) during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the Shoot\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    retryCycleStartTime  Kubernetes meta/v1.Time     (Optional) RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation must be retried until we give up).\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n    technicalID  string    TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and basically everything that is related to this particular Shoot.\n    uid  k8s.io/apimachinery/pkg/types.UID     UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters. It is used to compute unique hashes.\n    Toleration   (Appears on: ProjectTolerations, ShootSpec)  Toleration is a toleration for a seed taint.\n   Field Description      key  string    Key is the toleration key to be applied to a project or shoot.\n    value  string    (Optional) Value is the toleration value corresponding to the toleration key.\n    VersionClassification (string alias)\n  (Appears on: ExpirableVersion)  VersionClassification is the logical state of a version according to https://github.com/gardener/gardener/blob/master/docs/operations/versioning.md\nVolume   (Appears on: Worker)  Volume contains information about the volume type, size, and encryption.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    VolumeType   (Appears on: CloudProfileSpec)  VolumeType contains certain properties of a volume type.\n   Field Description      class  string    Class is the class of the volume type.\n    name  string    Name is the name of the volume type.\n    usable  bool    (Optional) Usable defines if the volume type can be used for shoot clusters.\n    Worker   (Appears on: Provider)  Worker is the base definition of a worker group.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n    cri  CRI     (Optional) CRI contains configurations of CRI support of every machine in the worker pool\n    kubernetes  WorkerKubernetes     (Optional) Kubernetes contains configuration for Kubernetes components related to this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    name  string    Name is the name of the worker group.\n    machine  Machine     Machine contains information about the machine type and image.\n    maximum  int32    Maximum is the maximum number of VMs to create.\n    minimum  int32    Minimum is the minimum number of VMs to create.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider-specific configuration for this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    volume  Volume     (Optional) Volume contains information about the volume type and size.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones is a list of availability zones that are used to evenly distribute this worker pool. Optional as not every provider may support availability zones.\n    WorkerKubernetes   (Appears on: Worker)  WorkerKubernetes contains configuration for Kubernetes components related to this worker pool.\n   Field Description      kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for all kubelets of this worker pool.\n     "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/create-delete-shoot/","title":"Create / Delete a Shoot cluster","tags":[],"description":"","content":"Create a Shoot Cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f your-shoot-aws.yaml You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called \u0026ldquo;project\u0026rdquo;) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nDelete a Shoot Cluster In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don\u0026rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/usage/delete shoot johndoe-1 johndoe ( hack bash script can be found here https://github.com/gardener/gardener/blob/master/hack/usage/delete)\nConfigure a Shoot cluster alert receiver The receiver of the Shoot alerts can be configured from the .spec.monitoring.alerting.emailReceivers section in the Shoot specification. The value of the field has to be a list of valid mail addresses.\nThe alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the Shoot resource specifies .spec.monitoring.alerting.emailReceivers and if a SMTP secret exists.\nIf the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/gardener_aws/","title":"Create a kubernetes cluster in AWS with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in an AWS Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCopy policy from the Gardener AWS Create new policy Create new policy\nCreate a new technical user Create a new technical user\nsave the keys of the user, you will need them later on Gardener Add AWS Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/gardener_gcp/","title":"Create a kubernetes cluster on GCP with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in the GCP Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCheck which roles are required by the Gardener GCP Create a new serviceaccount and assign roles Create a new serviceaccount\nCreate key for the serviceaccount Download the key of the serviceaccount as json save the keys of the user, you will need it later on\nEnable the Google compute API Enable the Google compute API Enable the Google IAM API Enable the Google IAM API Gardener Add GCP Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/create-shoot-into-existing-aws-vpc/","title":"Create a Shoot cluster into existing AWS VPC","tags":[],"description":"","content":"Create a Shoot cluster into existing AWS VPC Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC. The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.\nTL;DR If .spec.provider.infrastructureConfig.networks.vpc.cidr is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.\nIf .spec.provider.infrastructureConfig.networks.vpc.id is specified, Gardener will use the existing VPC and respectively won\u0026rsquo;t delete it on Shoot deletion.\n It\u0026rsquo;s not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.\nGardener won\u0026rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.\n 1. Configure AWS CLI The aws configure command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.\n$ aws configure AWS Access Key ID [None]: \u0026lt;ACCESS_KEY_ID\u0026gt; AWS Secret Access Key [None]: \u0026lt;SECRET_ACCESS_KEY\u0026gt; Default region name [None]: \u0026lt;DEFAULT_REGION\u0026gt; Default output format [None]: \u0026lt;DEFAULT_OUTPUT_FORMAT\u0026gt; 2. Create VPC $ aws ec2 create-vpc --cidr-block \u0026lt;cidr-block\u0026gt; { \u0026#34;Vpc\u0026#34;: { \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-ff7bbf86\u0026#34;, \u0026#34;InstanceTenancy\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Tags\u0026#34;: [], \u0026#34;CidrBlockAssociations\u0026#34;: [ { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-6e42b505\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } } ], \u0026#34;Ipv6CidrBlockAssociationSet\u0026#34;: [], \u0026#34;State\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;DhcpOptionsId\u0026#34;: \u0026#34;dopt-38f7a057\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;IsDefault\u0026#34;: false } } 3. Create Internet Gateway Gardener also requires that an internet gateway is attached to the VPC. You can create one using:\n$ aws ec2 create-internet-gateway { \u0026#34;InternetGateway\u0026#34;: { \u0026#34;Tags\u0026#34;: [], \u0026#34;InternetGatewayId\u0026#34;: \u0026#34;igw-c0a643a9\u0026#34;, \u0026#34;Attachments\u0026#34;: [] } } and attach it to the VPC using:\n$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86 4. Create the Shoot Prepare your Shoot manifest (you could check the example manifests). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the .spec.provider.infrastructureConfig.networks.vpc.id field:\nspec:region:\u0026lt;aws-region-of-vpc\u0026gt; provider:type:awsinfrastructureConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:id:vpc-ff7bbf86# ...Apply your Shoot manifest.\n$ kubectl apply -f your-shoot-aws.yaml Ensure that the Shoot cluster is properly created.\n$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE \u0026lt;SHOOT_NAME\u0026gt; aws 1.15.0 aws \u0026lt;SHOOT_DOMAIN\u0026gt; Succeeded 100 True True True True 20m "},{"uri":"https://gardener.cloud/curated-links/","title":"Curated Links","tags":[],"description":"","content":"Curated Links A curated list of Kubernetes resources and projects    A curated list of awesome kubernetes sources Inspired by @sindresorhus\u0026rsquo; awesome\nSetup  Install Docker for Mac Install Docker for Windows Run a Kubernetes Cluster on your local machine  A place that marks the beginning of a journey  Kubernetes Community Overview and Contributions Guide by Ihor Dvoretskyi An Intro to Google’s Kubernetes and How to Use It by Laura Frank Getting Started on Kubernetes by Rajdeep Dua Kubernetes: The Future of Cloud Hosting by Meteorhacks Kubernetes by Google by Gaston Pantana Key Concepts by Arun Gupta Application Containers: Kubernetes and Docker from Scratch by Keith Tenzer Learn the Kubernetes Key Concepts in 10 Minutes by Omer Dawelbeit Top Reasons Businesses Should Move to Kubernetes Now by Mike Johnston The Children\u0026rsquo;s Illustrated Guide to Kubernetes by Deis :-) The ‘kubectl run’ command by Michael Hausenblas Docker Kubernetes Lab Handbook by Peng Xiao  Interactive Learning Environments Learn Kubernetes using an interactive environment without requiring downloads or configuration\n Interactive Tutorial Katacoda Play with Kubernetes Kubernetes Bootcamp  Massive Open Online Courses / Tutorials List of available free online courses(MOOC) and tutorials\nCourses  Scalable Microservices with Kubernetes at Udacity Introduction to Kubernetes at edX  Tutorials  Kubernetes Tutorials by Kubernetes Team Kubernetes By Example by OpenShift Team Kubernetes Tutorial by Tutorialspoint  Package Managers  Helm KPM  RPC  gRPC Micro  Secret generation and management  Vault auth plugin backend: Kubernetes Vault controller kube-lego k8sec kubernetes-vault kubesec - Secure Secret management  Machine Learning  TensorFlow k8s mxnet-operator - Tools for ML/MXNet on Kubernetes. kubeflow - Machine Learning Toolkit for Kubernetes. seldon-core - Open source framework for deploying machine learning models on Kubernetes  Raspberry Pi Some of the awesome findings and experiments on using Kubernetes with Raspberry Pi.\n Kubecloud Setting up a Kubernetes on ARM cluster Setup Kubernetes on a Raspberry Pi Cluster easily the official way! by Mathias Renner and Lucas Käldström How to Build a Kubernetes Cluster with ARM Raspberry Pi then run .NET Core on OpenFaas by Scott Hanselman  Contributing Contributions are most welcome!\nThis list is just getting started, please contribute to make it super awesome.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/secure-seccomp/","title":"Custom Seccomp profile","tags":[],"description":"","content":"Custom Seccomp profile Context Seccomp (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.\nStarting from Kubernetes v1.3.0 the Seccomp feature is in Alpha. To configure it on a Pod, the following annotations can be used:\n seccomp.security.alpha.kubernetes.io/pod: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to all containers in a Pod. container.seccomp.security.alpha.kubernetes.io/\u0026lt;container-name\u0026gt;: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to \u0026lt;container-name\u0026gt; in a Pod.  More details can be found in the PodSecurityPolicy documentation.\nInstallation of custom profile By default, kubelet loads custom Seccomp profiles from /var/lib/kubelet/seccomp/. There are two ways in which Seccomp profiles can be added to a Node:\n to be baked in the machine image to be added at runtime.  This guide focuses on creating those profiles via a DaemonSet.\nCreate a file called seccomp-profile.yaml with the following content:\napiVersion:v1kind:ConfigMapmetadata:name:seccomp-profilenamespace:kube-systemdata:my-profile.json:| {\u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;,\u0026#34;syscalls\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;chmod\u0026#34;,\u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;}]} The policy above is a very simple one and not siutable for complex applications. The default docker profile can be used a reference. Feel free to modify it to your needs.\n Apply the ConfigMap in your cluster:\n$ kubectl apply -f seccomp-profile.yaml configmap/seccomp-profile created The next steps is to create the DaemonSet seccomp installer. It\u0026rsquo;s going to copy the policy from above in /var/lib/kubelet/seccomp/my-profile.json.\nCreate a file called seccomp-installer.yaml with the following content:\napiVersion:apps/v1kind:DaemonSetmetadata:name:seccompnamespace:kube-systemlabels:security:seccompspec:selector:matchLabels:security:seccomptemplate:metadata:labels:security:seccompspec:initContainers:- name:installerimage:alpine:3.10.0command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cp -r -L /seccomp/*.json /host/seccomp/\u0026#34;]volumeMounts:- name:profilesmountPath:/seccomp- name:hostseccompmountPath:/host/seccompreadOnly:falsecontainers:- name:pauseimage:k8s.gcr.io/pause:3.1terminationGracePeriodSeconds:5volumes:- name:hostseccomphostPath:path:/var/lib/kubelet/seccomp- name:profilesconfigMap:name:seccomp-profileCreate the installer and wait until it\u0026rsquo;s ready on all Nodes:\n$ kubectl apply -f seccomp-installer.yaml daemonset.apps/seccomp-installer created $ kubectl -n kube-system get pods -l security=seccomp NAME READY STATUS RESTARTS AGE seccomp-installer-wjbxq 1/1 Running 0 21s Create a Pod using custom Seccomp profile Finally we want to create a profile which uses our new Seccomp profile my-profile.json.\nCreate a file called my-seccomp-pod.yaml with the following content:\napiVersion:v1kind:Podmetadata:name:seccomp-appnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/my-profile.json\u0026#34;# you can specify seccomp profile per container. If you add another profile you can configure# it for a specific container - \u0026#39;pause\u0026#39; in this case.# container.seccomp.security.alpha.kubernetes.io/pause: \u0026#34;localhost/some-other-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1Create the Pod and see that\u0026rsquo;s running:\n$ kubectl apply -f my-seccomp-pod.yaml pod/seccomp-app created $ kubectl get pod seccomp-app NAME READY STATUS RESTARTS AGE seccomp-app 1/1 Running 0 42s Throubleshooting If an invalid or not existing profile is used then the Pod will be stuck in ContainerCreating phase:\nbroken-seccomp-pod.yaml:\napiVersion:v1kind:Podmetadata:name:broken-seccompnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/not-existing-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1$ kubectl apply -f broken-seccomp-pod.yaml pod/broken-seccomp created $ kubectl get pod broken-seccomp NAME READY STATUS RESTARTS AGE broken-seccomp 1/1 ContainerCreating 0 2m $ kubectl describe pod broken-seccomp Name: broken-seccomp Namespace: default .... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned kube-system/broken-seccomp to docker-desktop Warning FailedCreatePodSandBox 4s (x2 over 18s) kubelet, docker-desktop Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod \u0026#34;broken-seccomp\u0026#34;: failed to generate sandbox security options for sandbox \u0026#34;broken-seccomp\u0026#34;: failed to generate seccomp security options for container: cannot load seccomp profile \u0026#34;/var/lib/kubelet/seccomp/not-existing-profile.json\u0026#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory Further reading  https://en.wikipedia.org/wiki/Seccomp https://docs.docker.com/engine/security/seccomp https://lwn.net/Articles/656307/ http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf  "},{"uri":"https://gardener.cloud/components/dashboard/","title":"Dashboard","tags":[],"description":"","content":"Gardener Dashboard  \nDemo Development Setup Install Install all dependencies\nyarn Configuration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport:3030logLevel:debuglogFormat:textapiServerUrl:https://minkube# garden cluster kube-apiserver urlsessionSecret:c2VjcmV0# symetric key used for encryptionoidc:issuer:https://minikube:32001client_id:dashboardclient_secret:c2VjcmV0# oauth client secretredirect_uri:http://localhost:8080/auth/callbackscope:\u0026#39;openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl\u0026#39;clockTolerance:15frontend:dashboardUrl:pathname:/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/defaultHibernationSchedule:evaluation:- start:0017**1,2,3,4,5development:- start:0017**1,2,3,4,5end:0008**1,2,3,4,5production:~Run locally (during development) Concurrently run the backend server (port 3030) and the frontend server (port 8080) both with hot reload enabled.\nyarn serve All request to /api, /auth and /config.json will be proxied by default to the backend server.\nBuild Build docker image locally.\nmake build Push Push docker image to Google Container Registry.\nmake push This command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener is_active: true name: gardener properties: core: account: john.doe@example.org project: johndoe-1008 People The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2020 The Gardener Authors\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/20_dependencies/","title":"Dependencies","tags":[],"description":"","content":"Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\nmake test # runs tests make verify # runs static code checks and test There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\nmake test-cov open gardener.coverage.html make test-cov-clean Dependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u0026lt;PACKAGE\u0026gt;@\u0026lt;VERSION\u0026gt; or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy. go mod vendor resets the main module\u0026rsquo;s vendor directory to include all packages needed to build and test all the main module\u0026rsquo;s packages. It does not include test code for vendored packages. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module\u0026rsquo;s packages and dependencies, and it removes unused modules that don\u0026rsquo;t provide any relevant packages.\nmake revendor The dependencies are installed into the vendor folder which should be added to the VCS.\n:warning: Make sure that you test the code after you have updated the dependencies!\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/27_deploy_into_cluster/","title":"Deploy into a Cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component. Also note that all resources and deployments need to be created in the garden namespace (not overrideable).\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) The Gardenlet requires a bootstrap token as well as a bootstrap kubeconfig in order to properly register itself with the Gardener control plane.\nThe configuration values depict the various options to configure it. Please consult this document to get a detailed explanation of what can be configured.\nPrepare your values in a separate gardenlet-values.yaml file:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster (see this and this). Create a bootstrap kubeconfig containing this token:  apiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;Provide this bootstrap kubeconfig together with a desired name and namespace to the Gardenlet Helm chart values here:  gardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt;Define a name and namespace where the Gardenlet shall store the real kubeconfig it creates during the bootstrap process here:  gardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:gardenDefine either seedSelector or seedConfig (see this document  Now you are ready to deploy the Helm chart:\nhelm install charts/gardener/gardenlet \\  --namespace garden \\  --name gardenlet \\  -f gardenlet-values.yaml \\  --wait :warning: A current prerequisite of Kubernetes clusters that are used as seeds is to have a pre-deployed nginx-ingress-controller to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/30_deploy_seed_into_aks/","title":"Deploy into AKS","tags":[],"description":"","content":"Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig  Note: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template \u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026#34;*.$INGRESS_DOMAIN\u0026#34; \\ value=$LB_IP \\ type=A \\ ttl=300 Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; Retrying role assignment creation: 1/36 { \u0026#34;appId\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_id \u0026#34;displayName\u0026#34;: \u0026#34;azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_secret \u0026#34;tenant\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34; #az_tenant_id } Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls= Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m Deploy Gardener Helm Chart Check (current releases)[https://github.com/gardener/gardener/releases] and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1 gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} \\ --query \u0026#39;HostedZone.Name\u0026#39; \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026#34;garden-1.${HOSTED_ZONE_DOMAIN}\u0026#34; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026#34;Failed to list *v1beta1...\u0026#34; messages Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026#34;$KUBECONFIG_FOR_SEED_CLUSTER\u0026#34; | base64 -w 0)@\u0026#34; \\ example/40-secret-seed-azure.yaml After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml Before creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r \u0026#39;.[] | .subnets[] | .addressPrefix\u0026#39;) POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026#34;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026#34; \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026#34; \\ -e \u0026#34;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026#34; \\ -e \u0026#34;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026#34; \\ example/50-seed-azure.yaml Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-05-31T14:56:49Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] } Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ -e \u0026#34;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ example/70-secret-cloudprovider-azure.yaml After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e \u0026#39;s/# namespace: .*/ namespace: garden-dev/\u0026#39; \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) SHOOT_DOMAIN=\u0026#34;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026#34; \\ -e \u0026#34;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026#34; \\ -e \u0026#34;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026#34; \\ example/90-deprecated-shoot-azure.yaml And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026#34;2018-06-09T07:35:45Z\u0026#34; level=info msg=\u0026#34;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026#34; time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Starting flow Shoot cluster creation\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployNamespace\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployKubeAPIServerService\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).MoveBackupTerraformResources\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:56Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:01Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ... At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026#34;project:dev\u0026#34; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener. Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform execution ... lastOperation: description: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # \u0026#39;--\u0026#39; is required if you want to # pass any args starting with \u0026#39;-\u0026#39; # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath=\u0026#39;{.data.kubeconfig}\u0026#39; | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev "},{"uri":"https://gardener.cloud/documentation/concepts/deployment/in_kubernetes/","title":"Deploying the Gardener into a Kubernetes cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component. Also note that all resources and deployments need to be created in the garden namespace (not overrideable).\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) The Gardenlet requires a bootstrap token as well as a bootstrap kubeconfig in order to properly register itself with the Gardener control plane.\nThe configuration values depict the various options to configure it. Please consult this document to get a detailed explanation of what can be configured.\nPrepare your values in a separate gardenlet-values.yaml file:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster (see this and this). Create a bootstrap kubeconfig containing this token:  apiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;Provide this bootstrap kubeconfig together with a desired name and namespace to the Gardenlet Helm chart values here:  gardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt;Define a name and namespace where the Gardenlet shall store the real kubeconfig it creates during the bootstrap process here:  gardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:gardenDefine either seedSelector or seedConfig (see this document  Now you are ready to deploy the Helm chart:\nhelm install charts/gardener/gardenlet \\  --namespace garden \\  --name gardenlet \\  -f gardenlet-values.yaml \\  --wait :warning: A current prerequisite of Kubernetes clusters that are used as seeds is to have a pre-deployed nginx-ingress-controller to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\n"},{"uri":"https://gardener.cloud/components/dns-cm/","title":"DNS Controller Manager","tags":[],"description":"","content":"External DNS Management The main artefact of this project is the DNS controller manager for managing DNS records, also nicknamed as the Gardener \u0026ldquo;DNS Controller\u0026rdquo;.\nIt contains provisioning controllers for creating DNS records in one of the DNS cloud services\n Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, OpenStack Designate, Cloudflare DNS  and source controllers for services and ingresses to create DNS entries by annotations.\nThe configuration for the external DNS service is specified in a custom resource DNSProvider. Multiple DNSProvider can be used simultaneously and changed without restarting the DNS controller.\nDNS records are either created directly for a corresponding custom resource DNSEntry or by annotating a service or ingress.\nFor a detailed explanation of the model, see section The Model.\nFor extending or adapting this project with your own source or provisioning controllers, see section Extensions\nQuick start To install the DNS controller manager in your Kubernetes cluster, follow these steps.\n  Prerequisites\n  Check out or download the project to get a copy of the Helm charts. It is recommended to check out the tag of the last release, so that Helm values reference the newest released container image for the deployment.\n  Make sure, that you have installed Helm client (helm) locally and Helm server (tiller) on the Kubernetes cluster. See e.g. Helm installation for more details.\n    Install the DNS controller manager\nAs multiple Gardener DNS controllers can act on the same DNS Hosted Zone concurrently, each instance needs an owner identifier. Therefore choose an identifier sufficiently unique across these instances.\nThen install the DNS controller manager with\nhelm install charts/external-dns-management --name dns-controller --namespace=\u0026lt;my-namespace\u0026gt; --set configuration.identifier=\u0026lt;my-identifier\u0026gt; This will use the default configuration with all source and provisioning controllers enabled. The complete set of configuration variables can be found in charts/external-dns-management/values.yaml. Their meaning is explained by their corresponding command line options in section Using the DNS controller manager\nBy default, the DNS controller looks for custom resources in all namespaces. The choosen namespace is only relevant for the deployment itself.\n  Create a DNSProvider\nTo specify a DNS provider, you need to create a custom resource DNSProvider and a secret containing the credentials for your account at the provider. E.g. if you want to use AWS Route53, create a secret and provider with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-credentials namespace: default type: Opaque data: # replace \u0026#39;...\u0026#39; with values encoded as base64 # see https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html AWS_ACCESS_KEY_ID: ... AWS_SECRET_ACCESS_KEY: ... EOF and\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSProvider metadata: name: aws namespace: default spec: type: aws-route53 secretRef: name: aws-credentials domains: include: # this must be replaced with a (sub)domain of the hosted zone - my.own.domain.com EOF Check the successful creation with\nkubectl get dnspr You should see something like\nNAME TYPE STATUS AGE aws aws-route53 Ready 12s   Create a DNSEntry\nCreate an DNS entry with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSEntry metadata: name: mydnsentry namespace: default spec: dnsName: \u0026#34;myentry.my-own-domain.com\u0026#34; ttl: 600 targets: - 1.2.3.4 EOF Check the status of the DNS entry with\nkubectl get dnsentry You should see something like\nNAME DNS TYPE PROVIDER STATUS AGE mydnsentry myentry.my-own-domain.com aws-route53 default/aws Ready 24s As soon as the status of the entry is Ready, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, it may take up to a few minutes before the domain name can be resolved.\n  Wait for/check DNS record\nTo check the DNS resolution, use nslookup or dig.\nnslookup myentry.my-own-domain.com or with dig\n# or with dig dig +short myentry.my-own-domain.com Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)\ndig @8.8.8.8 +short myentry.my-own-domain.com   For more examples about the custom resources and the annotations for services and ingresses see the examples directory.\nThe Model This project provides a flexible model allowing to add DNS source objects and DNS provisioning environments by adding new independent controllers.\nThere is no single DNS controller anymore. The decoupling between the handling of DNS source objects, like ingresses or services, and the provisioning of DNS entries in an external DNS provider like Route53 or CloudDNS is achieved by introducing a new custom resource DNSEntry.\nThese objects can either be explicitly created to request dedicated DNS entries, or they are managed based on other resources like ingresses or services. For the latter dedicated DNS Source Controllers are used. There might be any number of such source controllers. They do not need to know anything about the various DNS environments. Their task is to figure out which DNS entries are required in their realm and manage appropriate DNSEntry objects. From these objects they can also read the provisioning status and report it back to the original source.\nProvisioning of DNS entries in external DNS providers is done by DNS Provisioning Controllers. They don\u0026rsquo;t need to know anything about the various DNS source objects. They watch DNSEntry objects and check whether they are responsible for such an object. If a provisioning controller feels responsible for an entry it manages the corresponding settings in the external DNS environment and reports the provisioning status back to the corresponding DNSEntry object.\nTo do this a provisioning controller is responsible for a dedicated environment (for example Route53). For every such environment the controller uses a dedicated type key. This key is used to look for DNSProvider objects. There might be multiple such objects per environment, specifying the credentials needed to access different external accounts. These accounts are then scanned for DNS zones and domain names they support. This information is then used to dynamically assign DNSEntry objects to dedicated DNSProvider objects. If such an assignment can be done by a provisioning controller then it is responsible for this entry and manages the corresponding entries in the external environment. DNSProvider objects can specify explicit inclusion and exclusion sets of domain names and/or DNS zone identifiers to override the scanning results of the account.\nOwner Identifiers Every DNS Provisioning Controller is responsible for a set of Owner Identifiers. DNS records in an external DNS environment are attached to such an identifier. This is used to identify the records in the DNS environment managed by a dedicated controller (manager). Every controller manager hosting DNS Provisioning Controllers offers an option to specify a default identifier. Additionally there might be dedicated DNSOwner objects that enable or disable additional owner ids.\nEvery DNSEntry object may specify a dedicated owner that is used to tag the records in the DNS environment. A DNS provisioning controller only acts of DNS entries it is responsible for. Other resources in the external DNS environment are not touched at all.\nThis way it is possbible to\n identify records in the external DNS management environment that are managed by the actual controller instance distinguish different DNS source environments sharing the same hosted zones in the external management environment cleanup unused entries, even if the whole resource set is already gone move the responsibility for dedicated sets of DNS entries among different kubernetes clusters or DNS source environments running different DNS Provisioning Controller without loosing the entries during the migration process.  If multiple DNS controller instances have access to the same DNS zones, it is very important, that every instance uses a unique owner identifier! Otherwise the cleanup of stale DNS record will delete entries created by another instance if they use the same identifier.\nDNS Classes Multiple sets of controllers of the DNS ecosystem can run in parallel in a kubernetes cluster working on different object set. They are separated by using different DNS Classes. Adding a DNS class annotation to an object of the DNS ecosytems assigns this object to such a dedicated set of DNS controllers. This way it is possible to maintain clearly separated set of DNS objects in a single kubernetes cluster.\nUsing the DNS controller manager The controllers to run can be selected with the --controllers option. Here the following controller groups can be used:\n  dnssources: all DNS Source Controllers. It includes the conrollers\n ingress-dns: handle DNS annotations for the standard kubernetes ingress resource service-dns: handle DNS annotations for the standard kubernetes service resource    dnscontrollers: all DNS Provisioning Controllers. It includes the controllers\n alicloud-dns: aws-route53: azure-dns: google-clouddns: openstack-designate: cloudflare-dns    all: (default) all controllers\n  It is also possible to list dedicated controllers by their name.\nIf a DNS Provisioning Controller is enabled it is important to specify a unique controller identity using the --identifier option. This identifier is stored in the DNS system to identify the DNS entries managed by a dedicated controller. There should never be two DNS controllers with the same identifier running at the same time for the same DNS domains/accounts.\nHere is the complete list of options provided:\nUsage: dns-controller-manager [flags] Flags: --alicloud-dns.cache-dir string Directory to store zone caches (for reload after restart) --alicloud-dns.cache-ttl int Time-to-live for provider hosted zone cache --alicloud-dns.default.pool.size int Worker pool size for pool default of controller alicloud-dns (default: 2) --alicloud-dns.disable-zone-state-caching disable use of cached dns zone state on changes --alicloud-dns.dns-class string Identifier used to differentiate responsible controllers for entries --alicloud-dns.dns-delay duration delay between two dns reconciliations --alicloud-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller alicloud-dns (default: 15m0s) --alicloud-dns.dns.pool.size int Worker pool size for pool dns of controller alicloud-dns (default: 1) --alicloud-dns.dry-run just check, don\u0026#39;t modify --alicloud-dns.identifier string Identifier used to mark DNS entries --alicloud-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller alicloud-dns (default: 1) --alicloud-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller alicloud-dns (default: 10m0s) --alicloud-dns.providers.pool.size int Worker pool size for pool providers of controller alicloud-dns (default: 2) --alicloud-dns.reschedule-delay duration reschedule delay after losing provider --alicloud-dns.secrets.pool.size int Worker pool size for pool secrets of controller alicloud-dns (default: 2) --alicloud-dns.setup int number of processors for controller setup --alicloud-dns.ttl int Default time-to-live for DNS entries --aws-route53.cache-dir string Directory to store zone caches (for reload after restart) --aws-route53.cache-ttl int Time-to-live for provider hosted zone cache --aws-route53.default.pool.size int Worker pool size for pool default of controller aws-route53 (default: 2) --aws-route53.disable-zone-state-caching disable use of cached dns zone state on changes --aws-route53.dns-class string Identifier used to differentiate responsible controllers for entries --aws-route53.dns-delay duration delay between two dns reconciliations --aws-route53.dns.pool.resync-period duration Period for resynchronization of pool dns of controller aws-route53 (default: 15m0s) --aws-route53.dns.pool.size int Worker pool size for pool dns of controller aws-route53 (default: 1) --aws-route53.dry-run just check, don\u0026#39;t modify --aws-route53.identifier string Identifier used to mark DNS entries --aws-route53.ownerids.pool.size int Worker pool size for pool ownerids of controller aws-route53 (default: 1) --aws-route53.providers.pool.resync-period duration Period for resynchronization of pool providers of controller aws-route53 (default: 10m0s) --aws-route53.providers.pool.size int Worker pool size for pool providers of controller aws-route53 (default: 2) --aws-route53.reschedule-delay duration reschedule delay after losing provider --aws-route53.secrets.pool.size int Worker pool size for pool secrets of controller aws-route53 (default: 2) --aws-route53.setup int number of processors for controller setup --aws-route53.ttl int Default time-to-live for DNS entries --azure-dns.cache-dir string Directory to store zone caches (for reload after restart) --azure-dns.cache-ttl int Time-to-live for provider hosted zone cache --azure-dns.default.pool.size int Worker pool size for pool default of controller azure-dns (default: 2) --azure-dns.disable-zone-state-caching disable use of cached dns zone state on changes --azure-dns.dns-class string Identifier used to differentiate responsible controllers for entries --azure-dns.dns-delay duration delay between two dns reconciliations --azure-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller azure-dns (default: 15m0s) --azure-dns.dns.pool.size int Worker pool size for pool dns of controller azure-dns (default: 1) --azure-dns.dry-run just check, don\u0026#39;t modify --azure-dns.identifier string Identifier used to mark DNS entries --azure-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller azure-dns (default: 1) --azure-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller azure-dns (default: 10m0s) --azure-dns.providers.pool.size int Worker pool size for pool providers of controller azure-dns (default: 2) --azure-dns.reschedule-delay duration reschedule delay after losing provider --azure-dns.secrets.pool.size int Worker pool size for pool secrets of controller azure-dns (default: 2) --azure-dns.setup int number of processors for controller setup --azure-dns.ttl int Default time-to-live for DNS entries --cloudflare-dns.cache-dir string Directory to store zone caches (for reload after restart) --cloudflare-dns.cache-ttl int Time-to-live for provider hosted zone cache --cloudflare-dns.default.pool.size int Worker pool size for pool default of controller cloudflare-dns (default: 2) --cloudflare-dns.disable-zone-state-caching disable use of cached dns zone state on changes --cloudflare-dns.dns-class string Identifier used to differentiate responsible controllers for entries --cloudflare-dns.dns-delay duration delay between two dns reconciliations --cloudflare-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller cloudflare-dns (default: 15m0s) --cloudflare-dns.dns.pool.size int Worker pool size for pool dns of controller cloudflare-dns (default: 1) --cloudflare-dns.dry-run just check, don\u0026#39;t modify --cloudflare-dns.identifier string Identifier used to mark DNS entries --cloudflare-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller cloudflare-dns (default: 1) --cloudflare-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller cloudflare-dns (default: 10m0s) --cloudflare-dns.providers.pool.size int Worker pool size for pool providers of controller cloudflare-dns (default: 2) --cloudflare-dns.reschedule-delay duration reschedule delay after losing provider --cloudflare-dns.secrets.pool.size int Worker pool size for pool secrets of controller cloudflare-dns (default: 2) --cloudflare-dns.setup int number of processors for controller setup --cloudflare-dns.ttl int Default time-to-live for DNS entries --cache-dir string default for all controller \u0026#34;cache-dir\u0026#34; options --cache-ttl int default for all controller \u0026#34;cache-ttl\u0026#34; options -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,source,target,all) (default \u0026#34;all\u0026#34;) --cpuprofile string set file for cpu profiling --disable-namespace-restriction disable access restriction for namespace local access only --disable-zone-state-caching default for all controller \u0026#34;disable-zone-state-caching\u0026#34; options --dns-class string default for all controller \u0026#34;dns-class\u0026#34; options --dns-delay duration default for all controller \u0026#34;dns-delay\u0026#34; options --dns-target-class string default for all controller \u0026#34;dns-target-class\u0026#34; options --dnsentry-source.default.pool.resync-period duration Period for resynchronization of pool default of controller dnsentry-source (default: 2m0s) --dnsentry-source.default.pool.size int Worker pool size for pool default of controller dnsentry-source (default: 2) --dnsentry-source.dns-class string identifier used to differentiate responsible controllers for entries --dnsentry-source.dns-target-class string identifier used to differentiate responsible dns controllers for target entries --dnsentry-source.exclude-domains stringArray excluded domains --dnsentry-source.key string selecting key for annotation --dnsentry-source.target-creator-label-name string label name to store the creator for generated DNS entries --dnsentry-source.target-creator-label-value string label value for creator label --dnsentry-source.target-name-prefix string name prefix in target namespace for cross cluster generation --dnsentry-source.target-namespace string target namespace for cross cluster generation --dnsentry-source.target-owner-id string owner id to use for generated DNS entries --dnsentry-source.target-realms string realm(s) to use for generated DNS entries --dnsentry-source.target-set-ignore-owners mark generated DNS entries to omit owner based access control --dnsentry-source.targets.pool.size int Worker pool size for pool targets of controller dnsentry-source (default: 2) --dry-run default for all controller \u0026#34;dry-run\u0026#34; options --exclude-domains stringArray default for all controller \u0026#34;exclude-domains\u0026#34; options --google-clouddns.cache-dir string Directory to store zone caches (for reload after restart) --google-clouddns.cache-ttl int Time-to-live for provider hosted zone cache --google-clouddns.default.pool.size int Worker pool size for pool default of controller google-clouddns (default: 2) --google-clouddns.disable-zone-state-caching disable use of cached dns zone state on changes --google-clouddns.dns-class string Identifier used to differentiate responsible controllers for entries --google-clouddns.dns-delay duration delay between two dns reconciliations --google-clouddns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller google-clouddns (default: 15m0s) --google-clouddns.dns.pool.size int Worker pool size for pool dns of controller google-clouddns (default: 1) --google-clouddns.dry-run just check, don\u0026#39;t modify --google-clouddns.identifier string Identifier used to mark DNS entries --google-clouddns.ownerids.pool.size int Worker pool size for pool ownerids of controller google-clouddns (default: 1) --google-clouddns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller google-clouddns (default: 10m0s) --google-clouddns.providers.pool.size int Worker pool size for pool providers of controller google-clouddns (default: 2) --google-clouddns.reschedule-delay duration reschedule delay after losing provider --google-clouddns.secrets.pool.size int Worker pool size for pool secrets of controller google-clouddns (default: 2) --google-clouddns.setup int number of processors for controller setup --google-clouddns.ttl int Default time-to-live for DNS entries --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for dns-controller-manager --identifier string default for all controller \u0026#34;identifier\u0026#34; options --ingress-dns.default.pool.resync-period duration Period for resynchronization of pool default of controller ingress-dns (default: 2m0s) --ingress-dns.default.pool.size int Worker pool size for pool default of controller ingress-dns (default: 2) --ingress-dns.dns-class string identifier used to differentiate responsible controllers for entries --ingress-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries --ingress-dns.exclude-domains stringArray excluded domains --ingress-dns.key string selecting key for annotation --ingress-dns.target-creator-label-name string label name to store the creator for generated DNS entries --ingress-dns.target-creator-label-value string label value for creator label --ingress-dns.target-name-prefix string name prefix in target namespace for cross cluster generation --ingress-dns.target-namespace string target namespace for cross cluster generation --ingress-dns.target-owner-id string owner id to use for generated DNS entries --ingress-dns.target-realms string realm(s) to use for generated DNS entries --ingress-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control --ingress-dns.targets.pool.size int Worker pool size for pool targets of controller ingress-dns (default: 2) --key string default for all controller \u0026#34;key\u0026#34; options --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default -D, --log-level string logrus log level --name string name used for controller manager --namespace string namespace for lease -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --openstack-designate.cache-dir string Directory to store zone caches (for reload after restart) --openstack-designate.cache-ttl int Time-to-live for provider hosted zone cache --openstack-designate.default.pool.size int Worker pool size for pool default of controller openstack-designate (default: 2) --openstack-designate.disable-zone-state-caching disable use of cached dns zone state on changes --openstack-designate.dns-class string Identifier used to differentiate responsible controllers for entries --openstack-designate.dns-delay duration delay between two dns reconciliations --openstack-designate.dns.pool.resync-period duration Period for resynchronization of pool dns of controller openstack-designate (default: 15m0s) --openstack-designate.dns.pool.size int Worker pool size for pool dns of controller openstack-designate (default: 1) --openstack-designate.dry-run just check, don\u0026#39;t modify --openstack-designate.identifier string Identifier used to mark DNS entries --openstack-designate.ownerids.pool.size int Worker pool size for pool ownerids of controller openstack-designate (default: 1) --openstack-designate.providers.pool.resync-period duration Period for resynchronization of pool providers of controller openstack-designate (default: 10m0s) --openstack-designate.providers.pool.size int Worker pool size for pool providers of controller openstack-designate (default: 2) --openstack-designate.reschedule-delay duration reschedule delay after losing provider --openstack-designate.secrets.pool.size int Worker pool size for pool secrets of controller openstack-designate (default: 2) --openstack-designate.setup int number of processors for controller setup --openstack-designate.ttl int Default time-to-live for DNS entries --plugin-dir string directory containing go plugins --pool.resync-period duration default for all controller \u0026#34;pool.resync-period\u0026#34; options --pool.size int default for all controller \u0026#34;pool.size\u0026#34; options --providers string cluster to look for provider objects --providers.disable-deploy-crds disable deployment of required crds for cluster provider --providers.id string id for cluster provider --reschedule-delay duration default for all controller \u0026#34;reschedule-delay\u0026#34; options --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-dns.default.pool.resync-period duration Period for resynchronization of pool default of controller service-dns (default: 2m0s) --service-dns.default.pool.size int Worker pool size for pool default of controller service-dns (default: 2) --service-dns.dns-class string identifier used to differentiate responsible controllers for entries --service-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries --service-dns.exclude-domains stringArray excluded domains --service-dns.key string selecting key for annotation --service-dns.target-creator-label-name string label name to store the creator for generated DNS entries --service-dns.target-creator-label-value string label value for creator label --service-dns.target-name-prefix string name prefix in target namespace for cross cluster generation --service-dns.target-namespace string target namespace for cross cluster generation --service-dns.target-owner-id string owner id to use for generated DNS entries --service-dns.target-realms string realm(s) to use for generated DNS entries --service-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control --service-dns.targets.pool.size int Worker pool size for pool targets of controller service-dns (default: 2) --setup int default for all controller \u0026#34;setup\u0026#34; options --target string target cluster for dns requests --target-creator-label-name string default for all controller \u0026#34;target-creator-label-name\u0026#34; options --target-creator-label-value string default for all controller \u0026#34;target-creator-label-value\u0026#34; options --target-name-prefix string default for all controller \u0026#34;target-name-prefix\u0026#34; options --target-namespace string default for all controller \u0026#34;target-namespace\u0026#34; options --target-owner-id string default for all controller \u0026#34;target-owner-id\u0026#34; options --target-realms string default for all controller \u0026#34;target-realms\u0026#34; options --target-set-ignore-owners default for all controller \u0026#34;target-set-ignore-owners\u0026#34; options --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --ttl int default for all controller \u0026#34;ttl\u0026#34; options Extensions This project can also be used as library to implement own source and provisioning controllers.\nHow to implement Source Controllers Based on the provided source controller library a source controller must implement the source.DNSSource interface and provide an appropriate creator function.\nA source controller can be implemented following this example:\npackage service import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/resources\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; ) var _MAIN_RESOURCE = resources.NewGroupKind(\u0026#34;core\u0026#34;, \u0026#34;Service\u0026#34;) func init() { source.DNSSourceController(source.NewDNSSouceTypeForExtractor(\u0026#34;service-dns\u0026#34;, _MAIN_RESOURCE, GetTargets),nil). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(source.CONTROLLER_GROUP_DNS_SOURCES) } Complete examples can be found in the sub packages of pkg/controller/source.\nHow to implement Provisioning Controllers Provisioning controllers can be implemented based on the provisioning controller library in this repository and must implement the provider.DNSHandlerFactory interface. This factory returns implementations of the provider.DNSHandler interface that does the effective work for a dedicated set of hosted zones.\nThese factories can be embedded into a final controller manager (the runnable instance) in several ways:\n The factory can be used to create a dedicated controller. This controller can then be embedded into a controller manager, either in its own controller manger or together with other controllers. The factory can be added to a compound factory, able to handle multiple infrastructures. This one can then be used to create a dedicated controller, again.  Embedding a Factory into a Controller A provisioning controller can be implemented following this example:\npackage controller import ( \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const CONTROLLER_NAME = \u0026#34;route53-dns-controller\u0026#34; func init() { provider.DNSController(CONTROLLER_NAME, \u0026amp;Factory{}). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(provider.CONTROLLER_GROUP_DNS_CONTROLLERS) } This controller can be embedded into a controller manager just by using an anonymous import of the controller package in the main package of a dedicated controller manager.\nComplete examples are available in the sub packages of pkg/controller/provider. They also show a typical set of implementation structures that help to structure the implementation of such controllers.\nThe provider implemented in this project always follow the same structure:\n the provider package contains the provider code the factory source file registers the factory at a default compound factory it contains a sub package controller, which contains the embedding of the factory into a dedicated controller  Embedding a Factory into a Compound Factory A provisioning controller based on a Compound Factory can be extended by a new provider factory by registering this factory at the compound factory. This could be done, for example, by using the default compound factory provided in package pkg/controller/provider/compound as shown here, where NewHandler is a function creating a dedicated handler for a dedicated provider type:\npackage aws import ( \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const TYPE_CODE = \u0026#34;aws-route53\u0026#34; var Factory = provider.NewDNSHandlerFactory(TYPE_CODE, NewHandler) func init() { compound.MustRegister(Factory) } The compound factory is then again embedded into a provisioning controller as shown in the previous section (see the controllersub package).\nSetting Up a Controller Manager One or multiple controller packages can be bundled into a controller manager, by implementing a main package like this:\npackage main import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/\u0026lt;your controller package\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;my-dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;some description\u0026#34;) } Using the standard Compound Provisioning Controller If the standard Compound Provisioning Controller should be used it is required to additionally add the anonymous imports for the providers intended to be embedded into the compound factory like this:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/\u0026lt;your provider\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Multiple Cluster Support The controller implementations provided in this project are prepared to work with multiple clusters by using the features of the used controller manager library.\nThe DNS Source Controllers support two clusters:\n the default cluster is used to scan for source objects the logical cluster target is used to maintain the DNSEnry objects.  The DNS Provisioning Controllers also support two clusters:\n the default cluster is used to scan for DNSEntry objects. It is mapped to the logical cluster target the logical cluster provider is used to look to the DNSProvider objects and their related secrets.  If those controller types should be combined in a single controller manager, it can be configured to support three potential clusters with the source objects, the one for the entry objects and the one with provider objects using cluster mappings.\nThis is shown in a complete example using the dns source controllers, the compound provisioning controller configured to support all the included DNS provider type factories:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/cluster\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller/mappings\u0026#34; dnsprovider \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; dnssource \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/alicloud\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/aws\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/azure\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/google\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/openstack\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/cloudflare\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/ingress\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/service\u0026#34; ) func init() { // target cluster already defined in dns source controller package  cluster.Configure( dnsprovider.PROVIDER_CLUSTER, \u0026#34;providers\u0026#34;, \u0026#34;cluster to look for provider objects\u0026#34;, ).Fallback(dnssource.TARGET_CLUSTER) mappings.ForControllerGroup(dnsprovider.CONTROLLER_GROUP_DNS_CONTROLLERS). Map(controller.CLUSTER_MAIN, dnssource.TARGET_CLUSTER).MustRegister() } func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Those clusters can the be separated by registering their names together with command line option names. These can be used to specify different kubeconfig files for those clusters.\nBy default all logical clusters are mapped to the default physical cluster specified via --kubeconfig or default cluster access.\nIf multiple physical clusters are defined they can be specified by a corresponding cluster option defining the kubeconfig file used to access this cluster. If no such option is specified the default is used.\nTherefore, even if the configuration is prepared for multiple clusters, such a controller manager can easily work on a single cluster if no special options are given on the command line.\nWhy not using the community external-dns solution? Some of the reasons are context-specific, i.e. relate to Gardener\u0026rsquo;s highly dynamic requirements.\n Custom resource for DNS entries  DNS entries are explicitly specified as custom resources. As an important side effect, each DNS entry provides an own status. Simply by querying the Kubernetes API, a client can check if a requested DNS entry has been successfully added to the DNS backend, or if an update has already been deployed, or if not to reason about the cause. It also opens for easy extensibility, as DNS entries can be created directly via the Kubernetes API. And it simplifies Day 2 operations, e.g. automatic cleanup of unused entries if a DNS provider is deleted.\nManagement of multiple DNS providers  The Gardener DNS controller uses a custom resource DNSProvider to dynamically manage the backend DNS services. While with external-dns you have to specify the single provider during startup, in the Gardener DNS controller you can add/update/delete providers during runtime with different credentials and/or backends. This is important for a multi-tenant environment as in Gardener, where users can bring their own accounts.\nA DNS provider can also restrict its actions on subset of the DNS domains (includes and excludes) for which the credentials are capable to edit.\nEach provider can define a separate “owner” identifier, to differentiate DNS entries in the same DNS zone from different providers.\nMulti cluster support  The Gardener DNS controller distinguish three different logical Kubernetes clusters: Source cluster, target cluster and runtime cluster. The source cluster is monitored by the DNS source controllers for annotations on ingress and service resources. These controllers then create DNS entries in the target cluster. DNS entries in the target cluster are then reconciliated/synchronized with the corresponding DNS backend service by the provider controller. The runtime cluster is the cluster the DNS controller runs on. For example, this enables needed flexibility in the Gardener deployment. The DNS controller runs on the seed cluster. This is also the target cluster. DNS providers and entries resources are created in the corresponding namespace of the shoot control plane, while the source cluster is the shoot cluster itself.\nOptimizations for handling hundreds of DNS entries  Some DNS backend services are restricted on the API calls per second (e.g. the AWS Route 53 API). To manage hundreds of DNS entries it is important to minimize the number of API calls. The Gardener DNS controller heavily makes usage of caches and batch processing for this reason.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/dockerfile_pitfall/","title":"Dockerfile pitfalls","tags":[],"description":"Common Dockerfile pitfalls","content":"Using latest tag for an image Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile FROMalpineWhile simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn\u0026rsquo;t actually make any changes.\nGood Dockerfile A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.\nFROMalpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430Running apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with its own problems.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won\u0026rsquo;t actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nAvoid big container images Building small container image will reduce the time needed to start or restart pods. An image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB). For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE postgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB postgres 9.6 d92dad241eff 13 days ago 235.4 MB postgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker\u0026rsquo;s support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here.\nGoogle\u0026rsquo;s distroless image is also a good base image.\n"},{"uri":"https://gardener.cloud/contribute/docs/","title":"Documentation","tags":[],"description":"","content":" Contributing Documentation How to Contribute to the Open Source Project Gardener    You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.   "},{"uri":"https://gardener.cloud/documentation/tutorials/dynamic-pvc/","title":"Dynamic Volume Provisioning","tags":[],"description":"Running a Postgres database on Kubernetes and dynamically provision and mount the storage volumes needed by the database","content":"Introduction The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database\nRun postgres database Define the following Kubernetes resources in a yaml file\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion:v1kind:PersistentVolumeClaimmetadata:name:postgresdb-pvcspec:accessModes:- ReadWriteOnceresources:requests:storage:9GistorageClassName:\u0026#39;default\u0026#39;This defines a PVC using storage class default. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {\u0026ldquo;storageclass.kubernetes.io/is-default-class\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}.\n$ kubectl describe sc default Name: default IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026#34;apiVersion\u0026#34;:\u0026#34;storage.k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;StorageClass\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;},\u0026#34;labels\u0026#34;:{\u0026#34;addonmanager.kubernetes.io/mode\u0026#34;:\u0026#34;Exists\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;parameters\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;gp2\u0026#34;},\u0026#34;provisioner\u0026#34;:\u0026#34;kubernetes.io/aws-ebs\u0026#34;} ,storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt; A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as \u0026ldquo;postgresdb-pvc\u0026rdquo;, and a corresponding PV \u0026ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026rdquo; is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s Notice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one is Retain. (A third policy Recycle has been deprecated). In case of Delete, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.\nOn the other hand, PV with Retain policy will not be deleted when the PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nYou can use the kubectl patch command to change the reclaim policy as described here here or use kubectl edit pv \u0026lt;pv-name\u0026gt; to edit online as below:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m # change the relcaim policy from \u0026#34;Delete\u0026#34; to \u0026#34;Retain\u0026#34; $ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb persistentvolume \u0026#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026#34; edited # check the reclaim policy afterwards $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m Deployment Once a PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two paths in the container are mounted to subfolders in the volume.\napiVersion:apps/v1kind:Deploymentmetadata:name:postgresnamespace:defaultlabels:app:postgresannotations:deployment.kubernetes.io/revision:\u0026#34;1\u0026#34;spec:replicas:1strategy:type:RollingUpdaterollingUpdate:maxUnavailable:1maxSurge:1selector:matchLabels:app:postgrestemplate:metadata:name:postgreslabels:app:postgresspec:containers:- name:postgresimage:\u0026#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\u0026#34;env:- name:POSTGRES_USERvalue:postgres- name:POSTGRES_PASSWORDvalue:p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ- name:POSTGRES_INITDB_XLOGDIRvalue:\u0026#34;/var/log/postgresql/logs\u0026#34;ports:- containerPort:5432volumeMounts:- mountPath:/var/lib/postgresql/dataname:postgre-dbsubPath:data# https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)- mountPath:/var/log/postgresql/logsname:postgre-dbsubPath:logsvolumes:- name:postgre-dbpersistentVolumeClaim:claimName:postgresdb-pvcreadOnly:falseimagePullSecrets:- name:cpettechregistryTo check the mount points in the container:\n$ kubectl get po NAME READY STATUS RESTARTS AGE postgres-7f485fd768-c5jf9 1/1 Running 0 32m $ kubectl exec -it postgres-7f485fd768-c5jf9 bash root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/ base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/ 000000010000000000000001 archive_status Deleting a PersistentVolumeClaim In case of \u0026ldquo;Delete\u0026rdquo; policy, deleting a PVC will also delete its associated PV. If \u0026ldquo;Retain\u0026rdquo; is the reclaim policy, the PV will change status from Bound to Released when PVC is deleted.\n# Check pvc and pv before deletion $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m # delete pvc $ kubectl delete pvc postgresdb-pvc persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; deleted # pv changed to status \u0026#34;Released\u0026#34; $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m "},{"uri":"https://gardener.cloud/documentation/contribute/10_code/13_env/","title":"Enviroment","tags":[],"description":"","content":"Preparing the Setup Conceptually, all Gardener components are designated to run inside as a Pod inside a Kubernetes cluster. The API server extends the Kubernetes API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details could be found in\n Principles of Kubernetes, and its components Kubernetes Development Guide Architecture of Gardener  This setup is based on minikube, a Kubernetes cluster running on a single node. Docker for Desktop and kind are also supported.\nInstalling Golang environment Install latest version of Golang. For MacOS you could use Homebrew:\nbrew install golang For other OS, please check Go installation documentation.\nInstalling kubectl and helm As already mentioned in the introduction, the communication with the Gardener happens via the Kubernetes (Garden) cluster it is targeting. To interact with that cluster, you need to install kubectl. Please make sure that the version of kubectl is at least v1.11.x.\nOn MacOS run\nbrew install kubernetes-cli Please check the kubectl installation documentation for other OS.\nYou may also need to develop Helm charts or interact with Tiller using the Helm CLI:\nOn MacOS run\nbrew install kubernetes-helm On other OS please check the Helm installation documentation.\nInstalling git We use git as VCS which you need to install.\nOn MacOS run\nbrew install git On other OS, please check the Git installation documentation.\nInstalling openvpn We use OpenVPN to establish network connectivity from the control plane running in the Seed cluster to the Shoot\u0026rsquo;s worker nodes running in private networks. To harden the security we need to generate another secret to encrypt the network traffic (details). Please install the openvpn binary. On MacOS run\nbrew install openvpn export PATH=$(brew --prefix openvpn)/sbin:$PATH On other OS, please check the OpenVPN downloads page.\nInstalling Minikube You\u0026rsquo;ll need to have minikube installed and running.\n Note: Gardener is working only with self-contained kubeconfig files because of security issue. You can configure your minikube to create self-contained kubeconfig files via:\nminikube config set embed-certs true  Alternatively, you can also install Docker for Desktop and kind.\nIn case you want to use the \u0026ldquo;Docker for Mac Kubernetes\u0026rdquo; or if you want to build Docker images for the Gardener you have to install Docker itself. On MacOS, please use Docker for MacOS which can be downloaded here.\nOn other OS, please check the Docker installation documentation.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration.\nOn MacOS run\nbrew install iproute2mac Installing yaml2json and jq go get -u github.com/bronze1man/yaml2json brew install jq [MacOS only] Install GNU core utilities When running on MacOS you have to install the GNU core utilities:\nbrew install coreutils gnu-sed This will create symbolic links for the GNU utilities with g prefix in /usr/local/bin, e.g., gsed or gbase64. To allow using them without the g prefix please put /usr/local/opt/coreutils/libexec/gnubin at the beginning of your PATH environment variable, e.g., export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH.\n[Optional] Installing gcloud SDK In case you have to create a new release or a new hotfix of the Gardener you have to push the resulting Docker image into a Docker registry. Currently, we are using the Google Container Registry (this could change in the future). Please follow the official installation instructions from Google.\nLocal Gardener setup This setup is only meant to be used for developing purposes, which means that only the control plane of the Gardener cluster is running on your machine.\nGet the sources Clone the repository from GitHub.\ngit clone git@github.com:gardener/gardener.git cd gardener Start the Gardener :warning: Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of Gardener, and what the various clusters are used for.  Start a local kubernetes cluster For the development of Gardener you need some kind of Kubernetes cluster, which can be used as a \u0026ldquo;garden\u0026rdquo; cluster. I.e. you need a Kubernetes API server on which you can register a APIService Gardener\u0026rsquo;s own Extension API Server.\nFor this you can use a standard tool from the community to setup a local cluster like minikube, kind or the Kubernetes Cluster feature in Docker for Desktop.\nHowever, if you develop and run Gardener\u0026rsquo;s components locally, you don\u0026rsquo;t actually a fully fledged Kubernetes Cluster, i.e. you don\u0026rsquo;t actually need to run Pods on it. If you want to use a more lightweight approach for development purposes, you can use the \u0026ldquo;nodeless Garden cluster setup\u0026rdquo; residing in hack/local-garden. This is the easiest way to get your Gardener development setup up and running.\nUsing the nodeless cluster setup\nSetting up a local nodeless Garden cluster is quite simple. The only prerequisite is a running docker daemon. Just use the provided Makefile rules to start your local Garden:\nmake local-garden-up [...] Starting gardener-dev kube-etcd cluster..! Starting gardener-dev kube-apiserver..! Starting gardener-dev kube-controller-manager..! Starting gardener-dev gardener-etcd cluster..! namespace/garden created clusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created clusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created [...] This will start all minimally required components of a Kubernetes cluster (etcd, kube-apiserver, kube-controller-manager) and an etcd Instance for the gardener-apiserver as Docker containers.\nTo tear down the local Garden cluster and remove the Docker containers, simply run:\nmake local-garden-down Using minikube\nAlternatively, spin up a cluster with minikube with this command:\nminikube start --embed-certs # `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files. 😄 minikube v1.8.2 on Darwin 10.15.3 🔥 Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... [...] 🏄 Done! Thank you for using minikube! Prepare the Gardener Now, that you have started your local cluster, we can go ahead and register the Gardener API Server. Just point your KUBECONFIG environment variable to the local cluster you created in the previous step and run:\nmake dev-setup Found Minikube ... namespace/garden created namespace/garden-dev created deployment.apps/etcd created service/etcd created service/gardener-apiserver created service/gardener-controller-manager created endpoints/gardener-apiserver created endpoints/gardener-controller-manager created apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created validatingwebhookconfiguration.admissionregistration.k8s.io/gardener-controller-manager created Optionally, you can switch off the Logging feature gate of Gardenlet to save resources:\nsed -i -e \u0026#39;s/Logging: true/Logging: false/g\u0026#39; dev/20-componentconfig-gardenlet.yaml The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\nkubectl apply -f example/10-secret-internal-domain-unmanaged.yaml secret/internal-domain-unmanaged created Run the Gardener Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the Makefile.\nmake start-apiserver Found Minikube ... I0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin \u0026#34;ResourceReferenceManager\u0026#34; I0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin \u0026#34;DeletionConfirmation\u0026#34; [...] I0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443 [...] (Optional) Now you are ready to launch the Gardener Controller Manager.\nmake start-controller-manager time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener controller manager...\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: \u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:2718\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTPS server on 0.0.0.0:2719\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Successfully bootstrapped the Garden cluster.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardener controller manager (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerRegistration controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;SecretBinding controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Project controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Quota controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;CloudProfile controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardener Scheduler.\nmake start-scheduler time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener scheduler ...\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:10251\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting scheduler.\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Gardener scheduler initialized (with Strategy: SameRegion)\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Scheduler controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardenlet.\nmake start-gardenlet time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardenlet...\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: HVPA=true, Logging=true\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardenlet (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerInstallation controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Shoot controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Seed controller initialized.\u0026#34; [...] :warning: The Gardenlet will handle all your seeds for this development scenario, although, for productive usage it is recommended to run it once per seed, see this document for more information.\nPlease checkout the Gardener Extensions Manager to install extension controllers - make sure that you install all of them required for your local development. Also, please refer to this document for further information about how extensions are registered in case you want to use other versions than the latest releases.\nThe Gardener should now be ready to operate on Shoot resources. You can use\nkubectl get shoots No resources found. to operate against your local running Gardener API Server.\n Note: It may take several seconds until the minikube cluster recognizes that the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Limitations of local development setup You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a \u0026ldquo;real\u0026rdquo; provider. Currently, it is not possible to run Gardener entirely isolated from any cloud provider. We are planning to support such a setup based on KubeVirt (see this for details), however, it does not yet exist. This means that - after you have setup Gardener - you need to register an external seed cluster (e.g., one created in AWS). Only after that step you can start creating shoot clusters with your locally running Gardener.\nSome time ago, we had a local setup based on VirtualBox/Vagrant. However, as we have progressed with the Extensibility epic we noticed that this implementation/setup does no longer fit into how we envision external providers to be. Moreover, it hid too many things and came with a bunch of limitations, making the development scenario too \u0026ldquo;artificial\u0026rdquo;:\n No integration with machine-controller-manager. The Shoot API Server is exposed via a NodePort. In a cloud setup a LoadBalancer would be used. It was not possible to create Shoot clusters consisting of more than one worker node. Cluster auto-scaling therefore is not supported. It was not possible to create two or more Shoot clusters in parallel. The communication between the Seed and the Shoot Clusters uses VPN tunnel. In this setup tunnels are not needed since all components run on localhost.  Additional information To make sure that a specific Seed cluster will be chosen, specify the .spec.seedName field (see here for an example Shoot manifest).\nPlease take a look at the example manifests folder to see which resource objects you need to install into your Garden cluster.\n"},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/25_markup/expand/","title":"Expand","tags":[],"description":"Displays an expandable/collapsible section of text on your page","content":"The Expand shortcode displays an expandable/collapsible section of text on your page. Here is an example\n   Expand me... \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;expand-content\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt;  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\u0026lt;/div\u0026gt;   Usage this shortcode takes exactly one optional parameter to define the text that appears next to the expand/collapse icon. (default is \u0026ldquo;Expand me\u0026hellip;\u0026quot;)\n{{%expand \u0026quot;Is this learn theme rocks ?\u0026quot; %}}Yes !.{{% /expand%}}     Is this learn theme rocks ? \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;expand-content\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt; Yes ! \u0026lt;/div\u0026gt;   Demo {{%expand%}} Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. {{% /expand%}}     Expand me... \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;expand-content\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt; Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod  tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. \n"},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/extending/","title":"Extending the Monitoring Stack","tags":[],"description":"","content":"Extending the Monitoring Stack This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.\nPlease ensure that you have understood the basic principles of Prometheus and its ecosystem before you continue.\n:bangbang: The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.\nOverview Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:\n Seed  Prometheus Grafana blackbox-exporter kube-state-metrics (Seed metrics) kube-state-metrics (Shoot metrics) Alertmanager (Optional)   Shoot  node-exporter(s) kube-state-metrics blackbox-exporter    In each Seed cluster there is a Prometheus in the garden namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.\nThe alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the garden namespace of the Seed. The purpose of this central alertmanager is to forward all important alerts to the operators of the Gardener setup.\nThe Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described here.\nAdding New Monitoring Targets After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.\nPrometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in charts/seed-monitoring/charts/prometheus/templates/config.yaml. New scrape jobs can be added in the section scrape_configs. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the Prometheus documentation.\nThe job_name of a scrape job should be the name of the component e.g. kube-apiserver or vpn. The collection interval should be the default of 30s. You do not need to specify this in the configuration.\nPlease do not ingest all metrics which are provided by a component. Rather collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following metric_relabel_configs statement to your scrape jobs (replace exampleComponent with component name).\n- job_name:example-component...metric_relabel_configs:{{include\u0026#34;prometheus.keep-metrics.metric-relabel-config\u0026#34;.Values.allowedMetrics.exampleComponent|indent6}}The whitelist for the metrics of your job can be maintained in charts/seed-monitoring/charts/prometheus/values.yaml in section allowedMetrics.exampleComponent (replace exampleComponent with component name). Check the following example:\nallowedMetrics:...exampleComponent:*metrics_name_1*metrics_name_2...Adding Alerts The alert definitons are located in charts/seed-monitoring/charts/prometheus/rules. There are two approaches for adding new alerts.\n Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component. Adding alerts for a new component. In this case a new rule file with name scheme example-component.rules.yaml needs to be added. Add the new alert to alertInhibitionGraph.dot, add any required inhibition flows and render the new graph. To render the graph run:  dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png  Create a test for the new alert. See Alert Tests.  Example alert:\ngroups:* name:example.rulesrules:* alert:ExampleAlertexpr:absent(up{job=\u0026#34;exampleJob\u0026#34;}==1)for:20mlabels:service:exampleseverity:critical# How severe is the alert? (blocker|critical|info|warning)type:shoot# For which topology is the alert relevant? (seed|shoot)visibility:all# Who should receive the alerts? (all|operator|owner)annotations:description:Alongerdescriptionoftheexamplealertthatshouldalsoexplaintheimpactofthealert.summary:Shortsummaryofanexamplealert.If the deployment of component is optional then the alert definitions needs to be added to charts/seed-monitoring/charts/prometheus/optional-rules instead. Furthermore the alerts for component need to be activatable in charts/seed-monitoring/charts/prometheus/values.yaml via rules.optional.example-component.enabled. The default should be true.\nBasic instruction how to define alert rules can be found in the Prometheus documentation.\nRouting tree The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency etc. You can find more information about Alertmanager routing in the Prometheus/Alertmanager documentation. The routing trees for the Alertmanagers deployed by Gardener are depicted below.\nCentral Seed Alertmanager\n∟ main route (all alerts for all shoots on the seed will enter) ∟ group by project and shoot name ∟ group by visibility \u0026#34;all\u0026#34; and \u0026#34;operator\u0026#34; ∟ group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34; → route to Garden operators ∟ group by severity \u0026#34;warning\u0026#34; (dropped) ∟ group by visibility \u0026#34;owner\u0026#34; (dropped) Shoot Alertmanager\n∟ main route (only alerts for one Shoot will enter) ∟ group by visibility \u0026#34;all\u0026#34; and \u0026#34;owner\u0026#34; ∟ group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34; → route to cluster alert receiver ∟ group by severity \u0026#34;warning\u0026#34; (dropped, will change soon → route to cluster alert receiver) ∟ group by visibility \u0026#34;operator\u0026#34; (dropped) Alert Inhibition All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can\u0026rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert make sure to add it to the diagram.\nAlert Attributes Each alert rule definition has to contain the following annotations:\n summary: A short description of the issue. description: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.  In addtion each alert must contain the following labels:\n type  shoot: Components running on the Shoot worker nodes in the kube-system namespace. seed: Components running on the Seed in the Shoot namespace as part of/next to the control plane.   service  Name of the component (in lowercase) e.g. kube-apiserver, alertmanager or vpn.   severity  blocker: All issues which make the cluster entirely unusable e.g. KubeAPIServerDown or KubeSchedulerDown critical: All issues which affect single functionalities/components but not affect the cluster in its core functionality e.g. VPNDown or KubeletDown. info: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity) warning: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. HighLatencyApiServerToWorkers or ApiServerResponseSlow.    Alert Tests Execute the tests in $GARDENERHOME/.ci/test or if you want to only test the Prometheus alerts:\n# Install promtool go get -u github.com/prometheus/prometheus/cmd/promtool # Move to seed-monitoring/prometheus chart cd $GARDENERHOME/charts/seed-monitoring/charts/prometheus/ # Execute tests promtool test rules rules-tests/*test.yaml If you want to add alert tests:\n  Create a new file in rules-tests in the form \u0026lt;alert-group-name\u0026gt;.rules.test.yaml or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.\n  Make sure that newly added tests succeed. See above.\n  Adding Grafana Dashboards The dashboard definition files are located in charts/seed-monitoring/charts/grafana/dashboards. Every dashboard needs its own file.\nIf you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.\nDashboard Structure The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.\n Kubernetes control plane components (Tag: control-plane)  All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager ETCD + Backup/Restore Kubernetes Addon Manager   Node/Machine components (Tag: node/machine)  All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets Machine-Controller-Manager + Cluster Autoscaler   Networking components (Tag: network)  CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress   Addon components (Tag: addon)  Cert Broker   Monitoring components (Tag: monitoring) Logging components (Tag: logging)  Mandatory Charts for Component Dashboards For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.\n Pod up/down status up{job=\u0026quot;example-component\u0026quot;} Pod/containers cpu utilization Pod/containers memorty consumption Pod/containers network i/o  These information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.\nChart Requirements Each chart needs to contain:\n a meaningful name a detailed description (for non trivial charts) appropriate x/y axis descriptions appropriate scaling levels for the x/y axis proper units for the x/y axis  Dashboard Parameters The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.\nDashboards have to \u0026hellip;\n contain a title which refers to the component name(s) contain a timezone statement which should be the browser time contain tags which express where the component is running (seed or shoot) and to which category the component belong (see dashboard structure) contain a version statement with a value of 1 be immutable  Example dashboard configuration\n{ \u0026#34;title\u0026#34;: \u0026#34;example-component\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;browser\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;seed\u0026#34;, \u0026#34;control-plane\u0026#34; ], \u0026#34;version\u0026#34;: 1, \u0026#34;editable\u0026#34;: \u0026#34;false\u0026#34;, } Furthermore all dashboards should contain the following time options:\n{ \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;10d\u0026#34; ] }, } "},{"uri":"https://gardener.cloud/api-reference/extensions/","title":"Extensions","tags":[],"description":"","content":"Packages:\n  extensions.gardener.cloud/v1alpha1   extensions.gardener.cloud/v1alpha1  Package v1alpha1 is the v1alpha1 version of the API.\nResource Types:  BackupBucket  BackupEntry  Cluster  ContainerRuntime  ControlPlane  Extension  Infrastructure  Network  OperatingSystemConfig  Worker  BackupBucket   BackupBucket is a specification for backup bucket.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupBucketStatus         BackupEntry   BackupEntry is a specification for backup Entry.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupEntryStatus         Cluster   Cluster is a specification for a Cluster resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Cluster    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterSpec          cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n       ContainerRuntime   ContainerRuntime is a specification for a container runtime resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ContainerRuntime    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ContainerRuntimeSpec          binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ContainerRuntimeStatus         ControlPlane   ControlPlane is a specification for a ControlPlane resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ControlPlane    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControlPlaneSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n       status  ControlPlaneStatus         Extension   Extension is a specification for a Extension resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Extension    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ExtensionSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ExtensionStatus         Infrastructure   Infrastructure is a specification for cloud provider infrastructure.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Infrastructure    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  InfrastructureSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n       status  InfrastructureStatus         Network   Network is the specification for cluster networking.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Network    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  NetworkSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n       status  NetworkStatus         OperatingSystemConfig   OperatingSystemConfig is a specification for a OperatingSystemConfig resource\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  OperatingSystemConfig    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  OperatingSystemConfigSpec          criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n       status  OperatingSystemConfigStatus         Worker   Worker is a specification for a Worker resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Worker    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  WorkerSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n       status  WorkerStatus         BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the spec for an BackupBucket resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus is the status for an BackupBucket resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the spec for an BackupEntry resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus is the status for an BackupEntry resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    CRIConfig   (Appears on: OperatingSystemConfigSpec)  CRI config is a structure contains configurations of the CRI library\n   Field Description      name  CRIName     Name is a mandatory string containing the name of the CRI library.\n    CRIName (string alias)\n  (Appears on: CRIConfig)  CRIName is a type alias for the CRI name string.\nCloudConfig   (Appears on: OperatingSystemConfigStatus)  CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n   Field Description      secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    ClusterSpec   (Appears on: Cluster)  ClusterSpec is the spec for a Cluster resource.\n   Field Description      cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n    ContainerRuntimeSpec   (Appears on: ContainerRuntime)  ContainerRuntimeSpec is the spec for a ContainerRuntime resource.\n   Field Description      binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ContainerRuntimeStatus   (Appears on: ContainerRuntime)  ContainerRuntimeStatus is the status for a ContainerRuntime resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    ContainerRuntimeWorkerPool   (Appears on: ContainerRuntimeSpec)     Field Description      name  string    Name specifies the name of the worker pool the container runtime should be available for.\n    selector  Kubernetes meta/v1.LabelSelector     Selector is the label selector used by the extension to match the nodes belonging to the worker pool.\n    ControlPlaneSpec   (Appears on: ControlPlane)  ControlPlaneSpec is the spec of a ControlPlane resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    ControlPlaneStatus   (Appears on: ControlPlane)  ControlPlaneStatus is the status of a ControlPlane resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    DataVolume   (Appears on: WorkerPool)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    DefaultSpec   (Appears on: BackupBucketSpec, BackupEntrySpec, ContainerRuntimeSpec, ControlPlaneSpec, ExtensionSpec, InfrastructureSpec, NetworkSpec, OperatingSystemConfigSpec, WorkerSpec)  DefaultSpec contains common status fields for every extension resource.\n   Field Description      type  string    Type contains the instance of the resource\u0026rsquo;s kind.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension         DefaultStatus   (Appears on: BackupBucketStatus, BackupEntryStatus, ContainerRuntimeStatus, ControlPlaneStatus, ExtensionStatus, InfrastructureStatus, NetworkStatus, OperatingSystemConfigStatus, WorkerStatus)  DefaultStatus contains common status fields for every extension resource.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific status.\n    conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    lastError  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    lastOperation  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation     (Optional) LastOperation holds information about the last operation on the resource.\n    observedGeneration  int64    ObservedGeneration is the most recent generation observed for this resource.\n    state  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) State can be filled by the operating controller with what ever data it needs.\n    resources  []github.com/gardener/gardener/pkg/apis/core/v1beta1.NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in the state by their names.\n    DropIn   (Appears on: Unit)  DropIn is a drop-in configuration for a systemd unit.\n   Field Description      name  string    Name is the name of the drop-in.\n    content  string    Content is the content of the drop-in.\n    ExtensionSpec   (Appears on: Extension)  ExtensionSpec is the spec for a Extension resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ExtensionStatus   (Appears on: Extension)  ExtensionStatus is the status for a Extension resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    File   (Appears on: OperatingSystemConfigSpec)  File is a file that should get written to the host\u0026rsquo;s file system. The content can either be inlined or referenced from a secret in the same namespace.\n   Field Description      path  string    Path is the path of the file system where the file should get written to.\n    permissions  int32    (Optional) Permissions describes with which permissions the file should get written to the file system. Should be defaulted to octal 0644.\n    content  FileContent     Content describe the file\u0026rsquo;s content.\n    FileContent   (Appears on: File)  FileContent can either reference a secret or contain inline configuration.\n   Field Description      secretRef  FileContentSecretRef     (Optional) SecretRef is a struct that contains information about the referenced secret.\n    inline  FileContentInline     (Optional) Inline is a struct that contains information about the inlined data.\n    FileContentInline   (Appears on: FileContent)  FileContentInline contains keys for inlining a file content\u0026rsquo;s data and encoding.\n   Field Description      encoding  string    Encoding is the file\u0026rsquo;s encoding (e.g. base64).\n    data  string    Data is the file\u0026rsquo;s data.\n    FileContentSecretRef   (Appears on: FileContent)  FileContentSecretRef contains keys for referencing a file content\u0026rsquo;s data from a secret in the same namespace.\n   Field Description      name  string    Name is the name of the secret.\n    dataKey  string    DataKey is the key in the secret\u0026rsquo;s .data field that should be read.\n    InfrastructureSpec   (Appears on: Infrastructure)  InfrastructureSpec is the spec for an Infrastructure resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n    InfrastructureStatus   (Appears on: Infrastructure)  InfrastructureStatus is the status for an Infrastructure resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    nodesCIDR  string    (Optional) NodesCIDR is the CIDR of the node network that was optionally created by the acting extension controller. This might be needed in environments in which the CIDR for the network for the shoot worker node cannot be statically defined in the Shoot resource but must be computed dynamically.\n    MachineDeployment   (Appears on: WorkerStatus)  MachineDeployment is a created machine deployment.\n   Field Description      name  string    Name is the name of the MachineDeployment resource.\n    minimum  int32    Minimum is the minimum number for this machine deployment.\n    maximum  int32    Maximum is the maximum number for this machine deployment.\n    MachineImage   (Appears on: WorkerPool)  MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n   Field Description      name  string    Name is the logical name of the machine image.\n    version  string    Version is the version of the machine image.\n    NetworkSpec   (Appears on: Network)  NetworkSpec is the spec for an Network resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n    NetworkStatus   (Appears on: Network)  NetworkStatus is the status for an Network resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    Object   Object is an extension object resource.\nOperatingSystemConfigPurpose (string alias)\n  (Appears on: OperatingSystemConfigSpec)  OperatingSystemConfigPurpose is a string alias.\nOperatingSystemConfigSpec   (Appears on: OperatingSystemConfig)  OperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.\n   Field Description      criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n    OperatingSystemConfigStatus   (Appears on: OperatingSystemConfig)  OperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    cloudConfig  CloudConfig     (Optional) CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n    command  string    (Optional) Command is the command whose execution renews/reloads the cloud config on an existing VM, e.g. \u0026ldquo;/usr/bin/reload-cloud-config -from-file=\u0026rdquo;. The  is optionally provided by Gardener in the .spec.reloadConfigFilePath field.\n    units  []string    (Optional) Units is a list of systemd unit names that are part of the generated Cloud Config and shall be restarted when a new version has been downloaded.\n    Purpose (string alias)\n  (Appears on: ControlPlaneSpec)  Purpose is a string alias.\nSpec   Spec is the spec section of an Object.\nStatus   Status is the status of an Object.\nUnit   (Appears on: OperatingSystemConfigSpec)  Unit is a unit for the operating system configuration (usually, a systemd unit).\n   Field Description      name  string    Name is the name of a unit.\n    command  string    (Optional) Command is the unit\u0026rsquo;s command.\n    enable  bool    (Optional) Enable describes whether the unit is enabled or not.\n    content  string    (Optional) Content is the unit\u0026rsquo;s content.\n    dropIns  []DropIn     (Optional) DropIns is a list of drop-ins for this unit.\n    Volume   (Appears on: WorkerPool)  Volume contains information about the root disks that should be used for worker pools.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    WorkerPool   (Appears on: WorkerSpec)  WorkerPool is the definition of a specific worker pool.\n   Field Description      machineType  string    MachineType contains information about the machine type that should be used for this worker pool.\n    maximum  int32    Maximum is the maximum size of the worker pool.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    machineImage  MachineImage     MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n    minimum  int32    Minimum is the minimum size of the worker pool.\n    name  string    Name is the name of this worker pool.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is a provider specific configuration for the worker pool.\n    userData  []byte    UserData is a base64-encoded string that contains the data that is sent to the provider\u0026rsquo;s APIs when a new machine/VM that is part of this worker pool shall be spawned.\n    volume  Volume     (Optional) Volume contains information about the root disks that should be used for this worker pool.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones contains information about availability zones for this worker pool.\n    WorkerSpec   (Appears on: Worker)  WorkerSpec is the spec for a Worker resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n    WorkerStatus   (Appears on: Worker)  WorkerStatus is the status for a Worker resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    machineDeployments  []MachineDeployment     MachineDeployments is a list of created machine deployments. It will be used to e.g. configure the cluster-autoscaler properly.\n     "},{"uri":"https://gardener.cloud/documentation/tutorials/featureflag/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"Implementing feature flags in Kubernetes applicaiton with labels and annotations","content":"Feature Flags in Kubernetes Applications Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nIn Kubernetes, labels are part of the identity of a resource and can be used through selectors. Annotations are similar, but do not participate in the identity of a resource and cannot be used to select resources. Nevertheless, they can still be used as feature flags to enable/disable application logic.\nPossible Use Cases  turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  How does this work We’ll use the Kubernetes downwardAPI ) to expose labels and annotations directly to our application. We’ll end up with two files (labels and annotations) in /etc/podinfo. First we add the downward api to spec.volumes. Note that it is possible to adding both labels and annotations into the same volume.\nHow to update/toggle the feature After the deployment of the demo application is done you can easily switch a feature in the application on or off. This is done very easily with kubectl by changing an annotation in the Pods.\nDeploy demo app/pod kubectl apply -f ./yaml/deployment.yaml Show the log kubectl logs featureflag-example -f Use business feature 2 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation2 Use business feature 1 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation1 Conclusion As you can see in the log of the Pod the application switches very fast between the implementations. Everything was controlled by annotations on the deployment or Pod. On the whole a very simple and maintainable solution to configure parts of the application without restarting the whole application.\nWrangling labels and annotations from the shell. # Add a label $ kubectl label pod my-pod-name a-label=foo # Show labels $ kubectl get pods --show-labels # If you only want to show specific labels, use -L=\u0026lt;label1\u0026gt;,\u0026lt;label2\u0026gt; # Update a label $ kubectl label pod my-pod-name a-label=bar --override # Delete a label .Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a label $ kubectl label pod my-pod-name a-label- # Add an annotation $ kubectl annotatate pod my-pod-name an-annotation=foo # Show annotations $ kubectl describe pod my-pod-name # Update an annotation $ kubectl annotation pod my-pod-name an-annotation=foo --override # Delete an annotation. Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a annotation $ kubectl annotation pod my-pod-name an-annotation- "},{"uri":"https://gardener.cloud/documentation/concepts/deployment/feature_gates/","title":"Feature Gates","tags":[],"description":"","content":"Feature Gates in Gardener This page contains an overview of the various feature gates an administrator can specify on different Gardener components.\nOverview Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the a component configuration file for a specific component.\nEach Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example this is the configuration of the gardenlet component.\nThe following tables are a summary of the feature gates that you can set on different Gardener components.\n The “Since” column contains the Gardener release when a feature is introduced or its release stage is changed. The “Until” column, if not empty, contains the last Gardener release in which you can still use a feature gate. If a feature is in the Alpha or Beta state, you can find the feature listed in the Alpha/Beta feature gate table. If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table. The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.  Feature gates for Alpha or Beta features    Feature Default Stage Since Until     Logging false Alpha 0.13    HVPA false Alpha 0.31    HVPAForShootedSeed false Alpha 0.32    ManagedIstio false Alpha 1.4     Using a feature A feature can be in Alpha, Beta or GA stage. An Alpha feature means:\n Disabled by default. Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.  A Beta feature means:\n Enabled by default. The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-critical uses because of potential for incompatible changes in subsequent releases.   Please do try Beta features and give feedback on them! After they exit beta, it may not be practical for us to make more changes.\n A General Availability (GA) feature is also referred to as a stable feature. It means:\n The feature is always enabled; you cannot disable it. The corresponding feature gate is no longer needed. Stable versions of features will appear in released software for many subsequent versions.  List of feature gates   Logging enables logging stack for Seed clusters.\n  HVPA enables simultaneous horizontal and vertical scaling in Seed Clusters.\n  HVPAForShootedSeed enables simultaneous horizontal and vertical scaling in shooted Seed clusters.\n  ManagedIstio enables a Gardener-tailored Istio in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the detailed documentation for more information.\n  "},{"uri":"https://gardener.cloud/components/gardenctl/","title":"gardenctl","tags":[],"description":"","content":"Gardenctl  \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nInstallation gardenctl is shipped for mac and linux in a binary format.\nOption 1: Install the latest release with Homebrew (macOS and Linux) as follows:\nbrew install gardener/tap/gardenctl Option 2: Manually download and install from gardenctl releases as follows:\n Download the latest release:  curl -LO https://github.com/gardener/gardenctl/releases/download/$(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST)/gardenctl-darwin-amd64 To download a specific version, replace the $(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST) portion of the command with the specific version.\nFor example, to download version 0.16.0 on macOS, type:\ncurl -LO https://github.com/gardener/gardenctl/releases/download/v0.16.0/gardenctl-darwin-amd64 Make the gardenctl binary executable.  chmod +x ./gardenctl-darwin-amd64 Move the binary in to your PATH.  sudo mv ./gardenctl-darwin-amd64 /usr/local/bin/gardenctl How to build it If no binary builds are available for your platform or architecture, you can build it from source, go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to clone the repository and build gardenctl.\ngit clone //documentation/concepts/architecture/15_gardenctl/ cd gardenctl make build After successfully building gardenctl the executables are in the directory ~/go/src/github.com/gardener/gardenctl/bin/. Next, move the executable for your architecture to /usr/local/bin. In this case for darwin-amd64.\nsudo mv bin/darwin-amd64/gardenctl-darwin-amd64 /usr/local/bin/gardenctl gardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\necho \u0026#34;source \u0026lt;(gardenctl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc Via Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\nemail:john.doe@example.comgithubURL:https://github.location.company.corpgardenClusters:- name:devkubeConfig:~/clusters/dev/kubeconfig.yaml- name:prodkubeConfig:~/clusters/prod/kubeconfig.yamlThe path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl supports multiple sessions. The session ID can be set via $GARDEN_SESSION_ID and the sessions are stored under $GARDENCTL_HOME/sessions.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs.\n aliyun aws az gcloud openstack  Moreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion bash print on the standard output a completion script which can be sourced via\nsource \u0026lt;(gardenctl completion bash) Please keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots uses the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster\ngardenctl ls seeds List all projects with shoot cluster\ngardenctl ls projects Target a seed cluster\ngardenctl target seed-gce-dev Target a project\ngardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster\ngardenctl show prometheus Execute an aws command on a targeted aws shoot cluster\ngardenctl aws ec2 describe-instances or\ngardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace\ngardenctl target myshoot\ngardenctl kubectl get pods -- -n kube-system -l k8s-app=kube-dns List all cluster with an issue\ngardenctl ls issues Drop an element from target stack\ngardenctl drop Open a shell to a cluster node\ngardenctl shell nodename Show logs from elasticsearch\ngardenctl logs etcd-main --elasticsearch Show last 100 logs from elasticsearch from the last 2 hours\ngardenctl logs etcd-main --elasticsearch --since=2h --tail=100 Show logs from seed nodes\ngardenctl target -g garden-name -s seed-name\ngardenctl logs tf infra shoot-name Show logs from shoot nodes\ngardenctl target -g garden-name -t shoot-name\ngardenctl logs api | scheduler | controller-manager | etcd-main -c etcd |etcd-main -c backup-restore | vpn-seed | vpn-shoot | machine-controller-manager | prometheus |grafana | alertmanager | cluster-autoscaler Show logs from garden nodes\ngardenctl target -g garden-name\ngardenctl logs gardener-apiserver | gardener-controller-manager  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues  gardenctl ls issues -o json | jq \u0026#39;.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }\u0026#39;  Print all issues of a single project e.g. garden-myproject  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.project==\u0026#34;garden-myproject\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state \u0026ldquo;Error\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Error\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state!=\u0026#34;Succeeded\u0026#34;) then . else empty end\u0026#39;  Print createdBy information (typically email addresses) of all shoots  gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026#34;.items[].metadata | {email: .annotations.\\\u0026#34;garden.sapcloud.io/createdBy\\\u0026#34;, name: .name, namespace: .namespace}\u0026#34; Here a few on cluster analysis:\n Which states are there and how many clusters are in this state?  gardenctl ls issues -o json | jq \u0026#39;.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}\u0026#39;  Get all clusters in state Failed  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Failed\u0026#34;) then . else empty end\u0026#39; "},{"uri":"https://gardener.cloud/components/gardener/","title":"Gardener","tags":[],"description":"","content":"Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.\nGardener\u0026rsquo;s main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nGardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:\n Kubernetes API Server = Gardener API Server Kubernetes Controller Manager = Gardener Controller Manager Kubernetes Scheduler = Gardener Scheduler Kubelet = Gardenlet Node = Seed cluster Pod = Shoot cluster  Please find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog posts on kubernetes.io: Gardener - the Kubernetes Botanist (17.5.2018) and Gardener Project Update (2.12.2019).\n K8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.18 v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     AWS            Azure            GCP            OpenStack            Alicloud       N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A N/A    Besides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener Project Update\u0026rdquo; blog on kubernetes.io. \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/gardener_certificate_management/","title":"Gardener Certificate Management","tags":[],"description":"Configure Certificate Management For Shoot Clusters","content":"Gardener Certificate Management Introduction Gardener comes with an extension that enables shoot owners to request X.509 compliant certificates for shoot domains.\nExtension Installation The Shoot-Cert-Service extension can be deployed and configured via Gardener\u0026rsquo;s native resource ControllerRegistration.\nPrerequisites To let the Shoot-Cert-Service operate properly, you need to have:\n a DNS service in your seed contact details and optionally a private key for a pre-existing Let\u0026rsquo;s Encrypt account  ControllerRegistration An example of a ControllerRegistration for the Shoot-Cert-Service can be found here: https://github.com/gardener/gardener-extensions/blob/master/controllers/extension-shoot-cert-service/example/controller-registration.yaml\nConfiguration The ControllerRegistration contains a Helm chart which eventually deploy the Shoot-Cert-Service to seed clusters. It offers some configuration options, mainly to set up a default issuer for shoot clusters. With a default issuer, pre-existing Let\u0026rsquo;s Encrypt accounts can be used and shared with shoot clusters (See \u0026ldquo;One Account or Many?\u0026rdquo; of the Integration Guide).\n Please keep the Let\u0026rsquo;s Encrypt Rate Limits in mind when using this shared account model. Depending on the amount of shoots and domains it is recommended to use an account with increased rate limits.\n apiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistration...values:certificateConfig:defaultIssuer:acme:email:foo@example.comprivateKey:|- -----BEGIN RSA PRIVATE KEY-----...-----ENDRSAPRIVATEKEY----- server: https://acme-v02.api.letsencrypt.org/directoryname:default-issuerIf the Shoot-Cert-Service should be enabled for every shoot cluster in your Gardener managed environment, you need to globally enable it in the ControllerRegistration:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistration...resources:- globallyEnabled:truekind:Extensiontype:shoot-cert-serviceAlternatively, you\u0026rsquo;re given the option to only enable the service for certain shoots:\nkind:ShootapiVersion:core.gardener.cloud/v1beta1...spec:extensions:- type:shoot-cert-service... #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  "},{"uri":"https://gardener.cloud/documentation/guides/gardener-cookies/","title":"Gardener Cookies","tags":[],"description":"","content":"Green Tea Matcha Cookies For a team event during the Christmas season we decided to completely reinterpret the topic cookies. :-)\n            .sh__item img { object-fit: cover !important; }  Matcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies are perfect with tea. And of course they fit perfectly to our logo\nIngredients  1 stick butter, softened ⅞ cup of granulated sugar 1 cup + 2 tablespoons all-purpose flour 2 eggs 1¼ tablespoons culinary grade matcha powder 1 teaspoon baking powder Pinch of salt  Instructions  Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy. Gently incorporate the eggs to the butter mixture one at a time. In a separate bowl, sift together all the dry ingredients. Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you\u0026rsquo;ve incorporated all the remaining flour mixture. The dough should be a beautiful green color. Chill the dough for at least an hour - up to overnight. The longer the better! Preheat your oven to 325 F. Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet. Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely. Remove and let cool on a rack and enjoy!  Note Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/gardener_dns_management/","title":"Gardener DNS Management for Shoots","tags":[],"description":"Configure DNS Management For Shoot Clusters","content":"Gardener DNS Management for Shoots Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. To support this the gardener must be installed with the shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\n #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  Configuration A general description for configuring the DNS management of the gardener can be found here.\nTo generally enable the DNS management for shoot objects the shoot-dns-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the registration must set the globallyEnabled flag to true.\nspec:resources:- kind:Extensiontype:shoot-dns-servicegloballyEnabled:trueProviding Base Domains usable for a Shoot So, far only the external DNS domain of a shoot already used for the kubernetes api server and ingress DNS names can be used for managed DNS names. This is either the shoot domain as subdomain of the default domain configured for the gardener installation or a dedicated domain with dedicated access credentials configured for a dedicated shoot via the shoot manifest.\nShoot Feature Gate If the shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster), it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must explicitly add the shoot-dns-service extension.\n...spec:extensions:- type:shoot-dns-service..."},{"uri":"https://gardener.cloud/documentation/tutorials/shoot_istio_dns_certs/","title":"Gardener yourself a Shoot with Istio, custom Domains, and Certificates","tags":[],"description":"","content":"As we ramp up more and more friends of Gardener, I thought it worthwile to explore and write a tutorial about how to simply\n create a Gardener managed Kubernetes Cluster (Shoot) via kubectl, install Istio as a preferred, production ready Ingress/Service Mesh (instead of the Nginx Ingress addon), attach your own custom domain to be managed by Gardener, combine everything with certificates from Let\u0026rsquo;s Encrypt.  Here are some pre-pointers that you will need to go deeper:\n CRUD Gardener Shoot DNS Management Certificate Mangement Tutorial Domain Names Tutorial Certifictates  If you try my instructions and fail, then read the alternative title of this tutorial as \"Shoot yourself in foot with Gardener, custom Domains, Istio and Certificates\".\n First Things First Login to your Gardener landscape, setup a project with adequate infrastructure credentials and then navigate to your account. Note down the name of your secret. I chose the GCP infrastructure from the vast possible options that my Gardener provides me with, so i had named the secret as shoot-operator-gcp.\nFrom the Access widget (leave the default settings) download your personalized kubeconfig into ~/.kube/kubeconfig-garden-myproject. Follow the instructions to setup kubelogin: For convinience, let us set an alias command with\nalias kgarden=\u0026#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\u0026#34; kgarden now gives you all botanical powers and connects you directly with your Gardener.\nYou should now be able to run kgarden get shoots, automatically get an oidc token, and list already running clusters/shoots.\nPrepare your Custom Domain I am going to use Cloud Flare as programmatic DNS of my custom domain mydomain.io. Please follow detailed instructions from Cloud Flare on how to delegate your domain (the free account does not support delegating subdomains). Alternatively, AWS Route53 (and most others) support delegating subdomains.\nI needed to follow these instructions and created the following secret:\napiVersion:v1kind:Secretmetadata:name:cloudflare-mydomain-iotype:Opaquedata:CLOUDFLARE_API_TOKEN:useYOURownDAMITzNDU2Nzg5MDEyMzQ1Njc4OQ==Apply this secret into your project with kgarden create -f cloudflare-mydomain-io.yaml.\nOur External DNS Manager also supports Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, or OpenStack Designate. Check it out.\nPrepare Gardener Extensions I now need to prepare the Gardener extensions shoot-dns-service and shoot-cert-service and set the parameters accordingly.\nPlease note, that the availability of Gardener Extensions depends on how your administrator has configured the Gardener landscape. Please contact your Gardener administrator in case you experience any issues during activation.\n The following snipplet allows Gardener to manage my entire custom domain, whereas with the include: attribute I restrict all dynamic entries under the subdomain gsicdc.mydomain.io:\ndns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-serviceThe next snipplet allows Gardener to manage certificates automatically from Let\u0026rsquo;s Encrypt on mydomain.io for me:\nextensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;Adjust the snipplets with your parameters (don't forget your email). And please use the mydomain-staging issuer while you are testing and learning. Otherwise, Let's Encrypt will rate limit your frequent requests and you can wait a week until you can continue.\n References for Let\u0026rsquo;s Encrypt:\n Rate limit Staging environment Challenge Types Wildcard Certificates  Create the Gardener Shoot Cluster Remember I chose to create the Shoot on GCP, so below is the simplest declarative shoot or cluster order document. Notice that I am referring to the infrastructure credentials with shoot-operator-gcp and I combined the above snipplets into the yaml file:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:gsicdcspec:dns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-service- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;cloudProfileName:gcpkubernetes:allowPrivilegedContainers:trueversion:1.18.2maintenance:autoUpdate:kubernetesVersion:truemachineImageVersion:truenetworking:nodes:10.250.0.0/16pods:100.96.0.0/11services:100.64.0.0/13type:calicoprovider:controlPlaneConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigzone:europe-west1-dinfrastructureConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:workers:10.250.0.0/16type:gcpworkers:- machine:image:name:gardenlinuxversion:11.29.2type:n1-standard-2maxSurge:1maxUnavailable:0maximum:2minimum:1name:my-workerpoolvolume:size:50Gitype:pd-standardzones:- europe-west1-dpurpose:testingregion:europe-west1secretBindingName:shoot-operator-gcpCreate your cluster and wait for it to be ready (about 5 to 7min).\n$ kgarden create -f gsicdc.yaml shoot.core.gardener.cloud/gsicdc created $ kgarden get shoot gsicdc --watch NAME CLOUDPROFILE VERSION SEED DOMAIN HIBERNATION OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Processing 38 Progressing Progressing Unknown Unknown 83s ... gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Succeeded 100 True True True False 6m7s Get access to your freshly baked cluster and set your KUBECONFIG:\n$ kgarden get secrets gsicdc.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u0026gt;kubeconfig-gsicdc.yaml $ export KUBECONFIG=$(pwd)/kubeconfig-gsicdc.yaml $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 89m Install Istio Please follow the Istio installation instructions and download istioctl. If you are on a Mac, I recommend\n$ brew install istioctl I want to install Istio with a default profile and SDS enabled. Furthermore I pass the following annotations to the service object istio-ingressgateway in the istio-system namespace.\nannotations:cert.gardener.cloud/issuer:mydomain-stagingcert.gardener.cloud/secretname:wildcard-tlsdns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:\u0026#39;*.gsicdc.mydomain.io\u0026#39;dns.gardener.cloud/ttl:\u0026#34;120\u0026#34;With these annotations three things now happen automagically:\n The External DNS Manager, provided to you as a service (dns.gardener.cloud/class: garden), picks up the request and creates the wildcard DNS entry *.gsicdc.mydomain.io with a time to live of 120sec at your DNS provider. My provider Cloud Flare is very very quick (as opposed to some other services). You should be able to verify the entry with dig lovemygardener.gsicdc.mydomain.io within seconds. The Certificate Mangemer picks up the request as well and initates a DNS01 protocol exchange with Let\u0026rsquo;s Encrypt; using the staging environment referred to with the issuer behind mydomain-staging. After aproximately 70sec (give and take) you will receive the wildcard certificate in the wildcard-tls secret in the namespace istio-system.  Notice, that the namespace for the certificate secret is often the cause of many troubeshooting sessions: the secret must reside in the same namespace of the gateway.\n Here is the istio-install script:\n$ export domainname=\u0026#34;*.gsicdc.mydomain.io\u0026#34; $ export issuer=\u0026#34;mydomain-staging\u0026#34; $ istioctl manifest apply --set profile=default \\  --set values.gateways.istio-ingressgateway.serviceAnnotations.\u0026#39;dns\\.gardener\\.cloud/dnsnames\u0026#39;=${domainname} \\  --set values.gateways.istio-ingressgateway.serviceAnnotations.\u0026#39;dns\\.gardener\\.cloud/ttl\u0026#39;=\u0026#39;120\u0026#39; \\  --set values.gateways.istio-ingressgateway.serviceAnnotations.\u0026#39;dns\\.gardener\\.cloud/class\u0026#39;=\u0026#39;garden\u0026#39; \\  --set values.gateways.istio-ingressgateway.serviceAnnotations.\u0026#39;cert\\.gardener\\.cloud/issuer\u0026#39;=${issuer} \\  --set values.gateways.istio-ingressgateway.serviceAnnotations.\u0026#39;cert\\.gardener\\.cloud/secretname\u0026#39;=\u0026#39;wildcard-tls\u0026#39; \\  --set values.gateways.istio-ingressgateway.sds.enabled=true Verify that setup is working and that DNS and certificates have been created/delivered:\n$ kubectl -n istio-system describe service istio-ingressgateway \u0026lt;snip\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 58s service-controller Ensuring load balancer Normal reconcile 58s cert-controller-manager created certificate object istio-system/istio-ingressgateway-service-pwqdm Normal cert-annotation 58s cert-controller-manager wildcard-tls: cert request is pending Normal cert-annotation 54s cert-controller-manager wildcard-tls: certificate pending: certificate requested, preparing/waiting for successful DNS01 challenge Normal cert-annotation 28s cert-controller-manager wildcard-tls: certificate ready Normal EnsuredLoadBalancer 26s service-controller Ensured load balancer Normal reconcile 26s dns-controller-manager created dns entry object shoot--core--gsicdc/istio-ingressgateway-service-p9qqb Normal dns-annotation 26s dns-controller-manager *.gsicdc.mydomain.io: dns entry is pending Normal dns-annotation 21s (x3 over 21s) dns-controller-manager *.gsicdc.mydomain.io: dns entry active $ dig lovemygardener.gsicdc.mydomain.io ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; lovemygardener.gsicdc.mydomain.io \u0026lt;snip\u0026gt; ;; ANSWER SECTION: lovemygardener.gsicdc.mydomain.io. 120 IN A\t35.195.120.62 \u0026lt;snip\u0026gt; There you have it, the wildcard-tls certificate is ready and the *.gsicdc.mydomain.io dns entry is active. Traffic will be going your way.\nHandy tools to install Another set of fine tools to use are k14s\u0026rsquo;s kapp, k9s and HTTPie. While we are at it, let\u0026rsquo;s install them all. If you are on a Mac, I recommend:\n$ brew tap k14s/tap $ brew install ytt kbld kapp kwt imgpkg vendir $ brew install derailed/k9s/k9s $ brew install httpie Ingress to your service Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. You should learn about Kubernetes networking, and first try to debug problems yourself. With a solid managed cluster from Gardener, it is always PEBCAK!\n Kubernetes Ingress is a subject that is evolving to much broader standard. Please watch Evolving the Kubernetes Ingress APIs to GA and Beyond for a good introduction. In this example, I did not want to use the Kubernetes Ingress compatibility option of Istio. Instead, I used VirtualService and Gateway from the Istio\u0026rsquo;s API group networking.istio.io/v1beta1 directly.\nI use httpbin as service that I want to expose to the internet, or where my ingress should be routed to (depends on your point of view, I guess).\napiVersion:v1kind:Namespacemetadata:name:productionlabels:istio-injection:enabled---apiVersion:v1kind:Servicemetadata:name:httpbinnamespace:productionlabels:app:httpbinspec:ports:- name:httpport:8000targetPort:80selector:app:httpbin---apiVersion:apps/v1kind:Deploymentmetadata:name:httpbinnamespace:productionspec:replicas:1selector:matchLabels:app:httpbintemplate:metadata:labels:app:httpbinspec:containers:- image:docker.io/kennethreitz/httpbinimagePullPolicy:IfNotPresentname:httpbinports:- containerPort:80---apiVersion:networking.istio.io/v1beta1kind:Gatewaymetadata:name:httpbin-gwnamespace:productionspec:selector:istio:ingressgateway#! use istio default ingress gatewayservers:- port:number:80name:httpprotocol:HTTPtls:httpsRedirect:truehosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;- port:number:443name:httpsprotocol:HTTPStls:mode:SIMPLEcredentialName:wildcard-tlshosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;---apiVersion:networking.istio.io/v1beta1kind:VirtualServicemetadata:name:httpbin-vsnamespace:productionspec:hosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;gateways:- httpbin-gwhttp:- match:- uri:regex:/.*route:- destination:port:number:8000host:httpbin---Let us now deploy the whole package of Kubernetes primitives using kapp:\n$ kapp deploy -a httpbin -f httpbin-kapp.yaml Target cluster \u0026#39;https://api.gsicdc.myproject.shoot.devgarden.cloud\u0026#39; (nodes: shoot--myproject--gsicdc-my-workerpool-z1-6586c8f6cb-x24kh) Changes Namespace Name Kind Conds. Age Op Wait to Rs Ri (cluster) production Namespace - - create reconcile - - production httpbin Deployment - - create reconcile - - ^ httpbin Service - - create reconcile - - ^ httpbin-gw Gateway - - create reconcile - - ^ httpbin-vs VirtualService - - create reconcile - - Op: 5 create, 0 delete, 0 update, 0 noop Wait to: 5 reconcile, 0 delete, 0 noop Continue? [yN]: y 5:36:31PM: ---- applying 1 changes [0/5 done] ---- \u0026lt;snip\u0026gt; 5:37:00PM: ok: reconcile deployment/httpbin (apps/v1) namespace: production 5:37:00PM: ---- applying complete [5/5 done] ---- 5:37:00PM: ---- waiting complete [5/5 done] ---- Succeeded Let\u0026rsquo;s finaly test the service (Of course you can use the browser as well):\n$ http httpbin.gsicdc.mydomain.io HTTP/1.1 301 Moved Permanently content-length: 0 date: Wed, 13 May 2020 21:29:13 GMT location: https://httpbin.gsicdc.mydomain.io/ server: istio-envoy $ curl -k https://httpbin.gsicdc.mydomain.io/ip { \u0026#34;origin\u0026#34;: \u0026#34;10.250.0.2\u0026#34; } Quod erat demonstrandum. The proof of exchanging the issuer is now left to the reader.\nRemember that the certificate is actually not valid because it is issued from the Let's encrypt staging environment. Thus, we needed \"curl -k\" or \"http --verify no\".\n Hint: use the interactive k9s tool. Cleanup Remove the cloud native application:\n$ kapp ls Apps in namespace \u0026#39;default\u0026#39; Name Namespaces Lcs Lca httpbin (cluster),production true 17m $ kapp delete -a httpbin ... Continue? [yN]: y ... 11:47:47PM: ---- waiting complete [8/8 done] ---- Succeeded Remove Istio:\n$ istioctl manifest generate --set profile=default | kubectl delete -f - clusterrole.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted ... Delete your Shoot:\nkgarden annotate shoot gsicdc confirmation.gardener.cloud/deletion=true --overwrite kgarden delete shoot gsicdc --wait=false "},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/shell-to-node/","title":"Get a Shell to a Gardener Shoot Worker Node","tags":[],"description":"Describes the methods for getting shell access to worker nodes.","content":"Get a Shell to a Kubernetes Node To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node to troubleshoot problems. This can be required if a node misbehaves or fails to join the cluster in the first place.\nWith access to the host, it is for instance possible to check the kubelet logs and interact with common tools such as systemctland journalctl.\nThe first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster. The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.\nThis guide only covers how to get access to the host, but does not cover troubleshooting methods.\n Get a Shell to an operational cluster node  Gardener Dashboard gardenctl shell Gardener Ops Toolbelt Custom root pod   SSH access to a node that failed to join the cluster  gardenctl-ssh SSH with manually created Bastion on AWS    Get a Shell to an operational cluster node The following describes four different approaches to get a shell to an operational Shoot worker node. As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod. All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.\nGardener Dashboard Prerequisite: the terminal feature is configured for the Gardener dashboard.\nNavigate to the cluster overview page and find the Terminal in the Access tile.\nSelect the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and access rights (only certain users have access to the Seed Control Plane).\nTo open the terminal configuration, click on the top right-hand corner of the screen.\nSet the Terminal Runtime to \u0026ldquo;Privileged. Also specify the target node from the drop-down menu.\nThe dashboard then schedules a pod and opens a shell session to the node.\nTo get access to common binaries installed on the host, prefix the command with chroot /hostroot. Note that the path depends on where the root path is mounted in the container. In the default image used by the Dashboard, it is under /hostroot.\ngardenctl shell Prerequisite: kubectl and gardenctl are available and configured.\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context and configures the kubeconfig file of the Shoot cluster. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; Get the nodes of the Shoot cluster.\n$ gardenctl kubectl get nodes Pick a node name from the list above and get a root shell access to it.\n$ gardenctl shell \u0026lt;target-node\u0026gt; Gardener Ops Toolbelt Prerequisite: kubectl is available.\nThe Gardener ops-toolbelt can be used as a convenient way to deploy a root pod to a node. The pod uses an image that is bundled with a bunch of useful troubleshooting tools. This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the previous section.\nThe easiest way to use the Gardener ops-toolbelt is to execute the ops-pod script in the hacks folder. To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:\n$ \u0026lt;path-to-ops-toolbelt-repo\u0026gt;/hacks/ops-pod \u0026lt;target-node\u0026gt; Custom root pod Alternatively, a pod can be assigned to a target node and a shell can be opened via standard Kubernetes means. To enable root access to the node, the pod specification requires proper securityContext and volume properties.\nFor instance you can use the following pod manifest, after changing  with the name of the node you want this pod attached to:\napiVersion:v1kind:Podmetadata:name:privileged-podnamespace:defaultspec:nodeSelector:kubernetes.io/hostname:\u0026lt;target-node-name\u0026gt; containers:- name:busyboximage:busyboxstdin:truesecurityContext:privileged:truevolumeMounts:- name:host-root-volumemountPath:/hostreadOnly:truevolumes:- name:host-root-volumehostPath:path:/hostNetwork:truehostPID:truerestartPolicy:NeverSSH access to a node that failed to join the cluster This section explores two options that can be used to get SSH access to a node that failed to join the cluster. As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.\nAdditionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.\nFor this scenario, cloud providers typically have extensive documentation (e.g AWS \u0026amp; GCP and in some cases tooling support). However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes the installation of a cloud provider specific agent one the node.\nAlternatively, gardenctl can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet. Currently gardenctl supports AWS, GCP, Openstack, Azure and Alibaba Cloud.\nIdentifying the problematic instance First, the problematic instance has to be identified. In Gardener, worker pools can be created in different cloud provider regions, zones and accounts.\nThe instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem. Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.\nGardener uses the Machine Controller Manager to create the Shoot worker nodes. For each worker node, the Machine Controller Manager creates a Machine CRD in the Shoot namespace in the respective Seed cluster. Usually the problematic instance can be identified as the respective Machine CRD has status pending.\nThe instance / node name can be obtained from the Machine .status field:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .status.node This is all the information needed to go ahead and use gardenctl ssh to get a shell to the node. In addition, the used cloud provider, the specific identifier of the instance and the instance region can be identified from the Machine CRD.\nGet the identifier of the instance via:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640 The identifier shows that the instance belongs to the cloud provider aws with the ec2 instance-id i-069733c435bdb4640 in region eu-north-1.\nTo get more information about the instance, check out the MachineClass (e.g AWSMachineClass) that is associated with each Machine CRD in the Shoot namespace of the Seed cluster. The AWSMachineClass contains the machine image (ami), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.\nOf course, the information can also be used to get the instance with the cloud provider CLI / API.\ngardenctl ssh Using the node name of the problematic instance, we can use the gardenctl ssh command to get SSH access to the cloud provider instance via an automatically set up bastion host. gardenctl takes care of spinning up the bastion instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance. After the SSH session has ended, gardenctl deletes the created cloud provider resources.\nUse the following commands:\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context, configures the kubeconfig file of the Shoot cluster and downloads the cloud provider credentials. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.\n$ gardenctl ssh \u0026lt;target-node\u0026gt; SSH with manually created Bastion on AWS In case you are not using gardenctl or want to control the bastion instance yourself, you can also manually set it up. The steps described here are generally the same as those used by gardenctl internally. Despite some cloud provider specifics they can be generalized to the following list:\n Open port 22 on the target instance. Create an instance / VM in a public subnet (bastion instance needs to have public ip address). Set-up security groups, roles and open port 22 for the bastion instance.  The following diagram shows an overview how the SSH access to the target instance works:\nThis guide demonstrates the setup of a bastion on AWS.\nPrerequisites:\n The AWS CLI is set up. Obtain target instance-id (see here). Obtain the VPC ID the Shoot resources are created in. This can be found in the Infrastructure CRD in the Shoot namespace in the Seed. Make sure that port 22 on the target instance is open (default for Gardener deployed instances).  Extract security group via  $ aws ec2 describe-instances --instance-ids \u0026lt;instance-id\u0026gt;  Check for rule that allows inbound connections on port 22:  $ aws ec2 describe-security-groups --group-ids=\u0026lt;security-group-id\u0026gt;  If not available, create the rule with the following comamnd:  $ aws ec2 authorize-security-group-ingress --group-id \u0026lt;security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create the Bastion Security Group   The common name of the security group is \u0026lt;shoot-name\u0026gt;-bsg. Create the security group:\n$ aws ec2 create-security-group --group-name \u0026lt;bastion-security-group-name\u0026gt; --description ssh-access --vpc-id \u0026lt;VPC-ID\u0026gt;   Optionally, create identifying tags for the security group:\n$ aws ec2 create-tags --resources \u0026lt;bastion-security-group-id\u0026gt; --tags Key=component,Value=\u0026lt;tag\u0026gt;   Create permission in the bastion security group that allows ssh access on port 22.\n$ aws ec2 authorize-security-group-ingress --group-id \u0026lt;bastion-security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create an IAM role for the bastion instance with the name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-role --role-name \u0026lt;shoot-name\u0026gt;-bastions The content should be:\n  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeRegions\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] }   Create the instance profile with name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-instance-profile --instance-profile-name \u0026lt;name\u0026gt;   Add the created role to the instance profile:\n$ aws iam add-role-to-instance-profile --instance-profile-name \u0026lt;instance-profile-name\u0026gt; --role-name \u0026lt;role-name\u0026gt;   Create the bastion instance Next, in order to be able to ssh into the bastion instance, the instance has to be set up with a user with a public ssh key. Create a user gardener that has the same Gardener-generated public ssh key as the target instance.\n  First, we need to get the public part of the Shoot ssh-key. The ssh-key is stored in a secret in the the project namespace in the Garden cluster. The name is: \u0026lt;shoot-name\u0026gt;-ssh-publickey. Get the key via:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34;   A script handed over as user-data to the bastion ec2 instance, can be used to create the gardener user and add the ssh-key. For your convenience, you can use the following script to generate the user-data.\n  #!/bin/bash -eu saveUserDataFile () { ssh_key=$1 cat \u0026gt; gardener-bastion-userdata.sh \u0026lt;\u0026lt;EOF #!/bin/bash -eu id gardener || useradd gardener -mU mkdir -p /home/gardener/.ssh echo \u0026#34;$ssh_key\u0026#34; \u0026gt; /home/gardener/.ssh/authorized_keys chown gardener:gardener /home/gardener/.ssh/authorized_keys echo \u0026#34;gardener ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;/etc/sudoers.d/99-gardener-user EOF } if [ -p /dev/stdin ]; then read -r input cat | saveUserDataFile \u0026#34;$input\u0026#34; else pbpaste | saveUserDataFile \u0026#34;$input\u0026#34; fi   Use the script by handing-over the public ssh-key of the Shoot cluster:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34; | ./generate-userdata.sh This generates a file called gardener-bastion-userdata.sh in the same directory containing the user-data.\n  The following information is needed to create the bastion instance:\nbastion-IAM-instance-profile-name\n Use the created instance profile with name \u0026lt;shoot-name\u0026gt;-bastions  image-id\n Possible use the same image-id as for the target instance (or any other image). Has cloud provider specific format (AWS: ami).  ssh-public-key-name\n This is the ssh key pair already created in the Shoot\u0026rsquo;s cloud provider account by Gardener during the Infrastructure CRD reconciliation. The name is usually: \u0026lt;shoot-name\u0026gt;-ssh-publickey  subnet-id\n Choose a subnet that is attached to an Internet Gateway and NAT Gateway (bastion instance must have a public IP). The Gardener created public subnet with the name \u0026lt;shoot-name\u0026gt;-public-utility-\u0026lt;xy\u0026gt; can be used. Please check the created subnets with the cloud provider.  bastion-security-group-id\n Use the id of the created bastion security group.  file-path-to-userdata\n  Use the filepath to user-data file generated in the previous step.\n  bastion-instance-name\n Optional to tag the instance. Usually \u0026lt;shoot-name\u0026gt;-bastions      Create the bastion instance via:\n  $ ec2 run-instances --iam-instance-profile Name=\u0026lt;bastion-IAM-instance-profile-name\u0026gt; --image-id \u0026lt;image-id\u0026gt; --count 1 --instance-type t3.nano --key-name \u0026lt;ssh-public-key-name\u0026gt; --security-group-ids \u0026lt;bastion-security-group-id\u0026gt; --subnet-id \u0026lt;subnet-id\u0026gt; --associate-public-ip-address --user-data \u0026lt;file-path-to-userdata\u0026gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=\u0026lt;bastion-instance-name\u0026gt;},{Key=component,Value=\u0026lt;mytag\u0026gt;}] ResourceType=volume,Tags=[{Key=component,Value=\u0026lt;mytag\u0026gt;}]\u0026#34; Capture the instance-id from the reponse and wait until the ec2 instance is running and has a public ip address.\nConnecting to the target instance Save the private key of the ssh-key-pair in a temporary local file for later use.\n$ umask 077 $ kubectl get secret \u0026lt;shoot-name\u0026gt;.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa\\\u0026#34; | base64 -d \u0026gt; id_rsa.key Use the private ssh key to ssh into the bastion instance.\n$ ssh -i \u0026lt;path-to-private-key\u0026gt; gardener@\u0026lt;public-bastion-instance-ip\u0026gt; If that works, connect from your local terminal to the target instance via the bastion.\n$ ssh -i \u0026lt;path-to-private-key\u0026gt; -o ProxyCommand=\u0026#34;ssh -W %h:%p -i \u0026lt;private-key\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@\u0026lt;public-ip-bastion\u0026gt;\u0026#34; gardener@\u0026lt;private-ip-target-instance\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no Cleanup Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.\n"},{"uri":"https://gardener.cloud/about/","title":"Getting Started","tags":[],"description":"","content":"Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.\nGardener\u0026rsquo;s main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nGardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:\n Kubernetes API Server = Gardener API Server Kubernetes Controller Manager = Gardener Controller Manager Kubernetes Scheduler = Gardener Scheduler Kubelet = Gardenlet Node = Seed cluster Pod = Shoot cluster  Please find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog posts on kubernetes.io: Gardener - the Kubernetes Botanist (17.5.2018) and Gardener Project Update (2.12.2019).\n K8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.18 v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     AWS            Azure            GCP            OpenStack            Alicloud       N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A N/A    Besides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener Project Update\u0026rdquo; blog on kubernetes.io. \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "},{"uri":"https://gardener.cloud/documentation/tutorials/gpu/","title":"GPU Enabled Cluster","tags":[],"description":"Setting up a GPU Enabled Cluster for Deep Learning","content":"Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, contributions are highly appreciated to update this guide.\nCreate a Cluster First thing first, let’s create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it\u0026rsquo;s the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. This costs around 1€/hour per GPU\nInstall NVidia Driver as Daemonset apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-driver-installernamespace:kube-systemlabels:k8s-app:nvidia-driver-installerspec:selector:matchLabels:name:nvidia-driver-installerk8s-app:nvidia-driver-installertemplate:metadata:labels:name:nvidia-driver-installerk8s-app:nvidia-driver-installerspec:hostPID:trueinitContainers:- image:squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972name:modulusargs:- compile- nvidia- \u0026#34;410.104\u0026#34;securityContext:privileged:trueenv:- name:MODULUS_CHROOTvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALLvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALL_DIRvalue:/opt/drivers- name:MODULUS_CACHE_DIRvalue:/opt/modulus/cache- name:MODULUS_LD_ROOTvalue:/root- name:IGNORE_MISSING_MODULE_SYMVERSvalue:\u0026#34;1\u0026#34;volumeMounts:- name:etc-coreosmountPath:/etc/coreosreadOnly:true- name:usr-share-coreosmountPath:/usr/share/coreosreadOnly:true- name:ld-rootmountPath:/root- name:module-cachemountPath:/opt/modulus/cache- name:module-install-dir-basemountPath:/opt/drivers- name:devmountPath:/devcontainers:- image:\u0026#34;gcr.io/google-containers/pause:3.1\u0026#34;name:pausetolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;volumes:- name:etc-coreoshostPath:path:/etc/coreos- name:usr-share-coreoshostPath:path:/usr/share/coreos- name:ld-roothostPath:path:/- name:module-cachehostPath:path:/opt/modulus/cache- name:devhostPath:path:/dev- name:module-install-dir-basehostPath:path:/opt/driversInstall Device Plugin apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-gpu-device-pluginnamespace:kube-systemlabels:k8s-app:nvidia-gpu-device-plugin#addonmanager.kubernetes.io/mode: Reconcilespec:selector:matchLabels:k8s-app:nvidia-gpu-device-plugintemplate:metadata:labels:k8s-app:nvidia-gpu-device-pluginannotations:scheduler.alpha.kubernetes.io/critical-pod:\u0026#39;\u0026#39;spec:priorityClassName:system-node-criticalvolumes:- name:device-pluginhostPath:path:/var/lib/kubelet/device-plugins- name:devhostPath:path:/devcontainers:- image:\u0026#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d\u0026#34;command:[\u0026#34;/usr/bin/nvidia-gpu-device-plugin\u0026#34;,\u0026#34;-logtostderr\u0026#34;,\u0026#34;-host-path=/opt/drivers/nvidia\u0026#34;]name:nvidia-gpu-device-pluginresources:requests:cpu:50mmemory:10Milimits:cpu:50mmemory:10MisecurityContext:privileged:truevolumeMounts:- name:device-pluginmountPath:/device-plugin- name:devmountPath:/devupdateStrategy:type:RollingUpdateTest To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026amp; Keras\napiVersion:apps/v1kind:Deploymentmetadata:name:deeplearning-workbenchnamespace:defaultspec:replicas:1selector:matchLabels:app:deeplearning-workbenchtemplate:metadata:labels:app:deeplearning-workbenchspec:containers:- name:deeplearning-workbenchimage:afritzler/deeplearning-workbenchresources:limits:nvidia.com/gpu:1tolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;Note: the tolerations section above is not required if you deploy the ExtendedResourceToleration admission controller to your cluster. You can do this in the kubernetes section of your Gardener cluster shoot.yaml as follows:\nkubernetes: kubeAPIServer: admissionPlugins: - name: ExtendedResourceToleration Now exec into the container and start an example Keras training\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash cd /keras/example python imdb_cnn.py Acknowledgments \u0026amp; References  Andreas Fritzler from the Gardener Core team for the R\u0026amp;D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  "},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/secure-setup/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"Hardening the Gardener Community Setup Context Gardener stakeholders in the Open Source community usually use the Gardener Setup Scripts, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:\n Seed cluster Shoot cluster  As Alban Crequy from Kinvolk has recommended in his recent Gardener blog Auditing Kubernetes for Secure Setup the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.\nRecommendations Mitigation for Gardener CVE-2018-2475 The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.\n Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account Create a Shoot cluster in a different IaaS account As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster Register this newly created Shoot cluster as a Seed cluster in the Gardener End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).  A tutorial on how to create a shooted seed cluster can be found here.\nThe rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.\nWhen you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener CVE-2018-2475 anymore.\nMitigation for Kubernetes CVE-2018-1002105 In addition when you follow the recommendations in the recent Gardener Security Announcement you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.\nAlternative Approach For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his blog directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/hibernate-cluster/","title":"Hibernate a Cluster","tags":[],"description":"","content":"Problem If you have built a customer scenario for demo purposes, you don\u0026rsquo;t want to run the cluster all the time. The costs would exceed here very fast. You can setup the cluster again for each demo, thanks to Helm this works relatively well, but takes a long time depending on the infrastructure. Furthermore not all 3rd party services are connected yet.\n\nSet a Gardener Cluster in Hibernate Mode Fortunately the Gardener offers the possibility to scale the Worker Nodes down to \u0026ldquo;Zero\u0026rdquo; without much effort. Follow the slide deck below to bring your Gardner Cluster in Hibernate Mode\n The mechanism to hibernate a cluster has changed. It is not necessary to scale down the workergroups to zero. You can now press the hibernate button or add the property hibernation to the shoot YAML.\n            "},{"uri":"https://gardener.cloud/documentation/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/debug-a-pod/","title":"How to debug a pod","tags":[],"description":"Your pod doesn&#39;t run as expected. Are there any log files? Where? How could I debug a pod?","content":"Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running  to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash:\n error during image pull caused by e.g. wrong/missing secrets or wrong/missing image the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets liveness probe failed too high resource consumption (memory and/or CPU) or too strict quota settings persistent volumes can\u0026rsquo;t be created/mounted the container image is not updated  Basically, the commands kubectl logs ... and kubectl describe ... with additional parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you\u0026rsquo;ll find some basic approaches to get some ideas what went wrong.\nRemarks:\n Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u0026lt;your-namespace\u0026gt; to select the target namespace. They require Kubernetes release ≥ 1.8.  Prerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren\u0026rsquo;t running.\nError caused by wrong image name You run kubectl describe pod \u0026lt;your-pod\u0026gt; \u0026lt;your-namespace\u0026gt; to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nkubectl delete pod termination-demo Next, create a resource based on the yaml content below\napiVersion:v1kind:Podmetadata:name:termination-demospec:containers:- name:termination-demo-containerimage:debianncommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]kubectl describe pod termination-demo lists the following content in the Event section\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal 2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debiann\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \u0026#34;debiann\u0026#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found 2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod 2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \u0026#34;debiann\u0026#34; The error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nApp runs in an error state caused by missing ConfigMaps or Secrets This example illustrates the behavior in case of the app expecting environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, deploy this manifest\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]Now, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal 19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container 19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container 19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod The command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file So you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]envFrom:- configMapRef:name:app-envNote that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and runs to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;SLEEP:\u0026#34;5\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]# args: [\u0026#34;-c\u0026#34;, \u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;while true; do sleep $SLEEP; echo sleeping; done;\u0026#34;]envFrom:- configMapRef:name:app-envToo high resource consumption or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. Find more details in Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u0026lt;= requests \u0026lt;= limit. For both settings you need to consider the total amount of resources the available nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption of your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, adapt the cpu in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]resources:requests:cpu:\u0026#34;600m\u0026#34;The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw Name: termination-demo-fdb7bb7d9-mzvfw Namespace: default ... Containers: termination-demo-container: Image: debian Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: /bin/sh Args: -c sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log Requests: cpu: 6 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro) Conditions: Type Status PodScheduled False Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu. More details in\n Managing Compute Resources for Containters Resource Quality of Service in Kubernetes  Remark:\n This example works similarly when specifying a too high request for memory In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn\u0026rsquo;t reach the maximum number of worker nodes If your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output  Why was the container image not updated? You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn\u0026rsquo;t change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag whenever you changed anything in your image (see Configuration Best Practices).\nFind more details in FAQ Container Image not updating\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  "},{"uri":"https://gardener.cloud/documentation/050-tutorials/content/app/","title":"HowTos","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/050-tutorials/content/howto/","title":"HowTos","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/applications/https/","title":"HTTPS with self Signed Certificate","tags":[],"description":"","content":"Configuring ingress with front-end TLS It is alyways recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server You\u0026rsquo;ll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion:v1kind:Servicemetadata:labels:app:node-servername:node-svcnamespace:defaultspec:type:NodePortports:- port:8080selector:app:node-server---apiVersion:extensions/v1beta1kind:Ingressmetadata:name:node-ingressspec:tls:- hosts:- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comsecretName:tls-secretrules:- host:ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:node-svcservicePort:8080"},{"uri":"https://gardener.cloud/documentation/concepts/core-components/api-server/admission-plugins/","title":"In-Tree Admission Plugins","tags":[],"description":"","content":"Admission Plugins Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins. If you want to get an overview of the what and why of admission plugins then this document might be a good start.\nThis document lists all existing admission plugins with a short explanation of what it is responsible for.\nClusterOpenIDConnectPreset, OpenIDConnectPreset (both enabled by default)\nThese admission controllers react on CREATE operations for Shoots. If the Shoot does not specify any OIDC configuration (.spec.kubernetes.kubeAPIServer.oidcConfig=nil) then it tries to find a matching ClusterOpenIDConnectPreset or OpenIDConnectPreset, respectively. If there are multiples that match then the one with the highest weight \u0026ldquo;wins\u0026rdquo;. In this case, the admission controller will default the OIDC configuration in the Shoot.\nControllerRegistrationResources (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for ControllerRegistrations. It validates that there exists only one ControllerRegistration in the system that is primarily responsible for a given kind/type resource combination. This prevents misconfiguration by the Gardener administrator/operator.\nCustomVerbAuthorizer (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Projects. It validates whether the user is bound to a RBAC role with the modify-spec-tolerations-whitelist verb in case the user tries to change the .spec.tolerations.whitelist field of the respective Project resource. Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on Project basis.\nDeletionConfirmation (enabled by default)\nThis admission controller reacts on DELETE operations for Projects and Shoots. It validates that the respective resource is annotated with a deletion confirmation annotation, namely confirmation.gardener.cloud/deletion=true. Only if this annotation is present it allows the DELETE operation to pass. This prevents users from accidental/undesired deletions.\nExtensionValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for BackupEntrys, BackupBuckets, Seeds, and Shoots. For all the various extension types in the specifications of these objects, it validates whether there exists a ControllerRegistration in the system that is primarily responsible for the stated extension type(s). This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don\u0026rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.\nPlantValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Plants. It sets the gardener.cloud/created-by annotation for newly created Plant resources. Also, it prevents creating new Plant resources in Projects that are already have a deletion timestamp.\nResourceReferenceManager (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for CloudProfiles, Projects, SecretBindings, Seeds, and Shoots. Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced Secret exists). However, it also has some special behaviours for certain resources:\n CloudProfiles: It rejects removing Kubernetes or machine image versions if there is at least one Shoot that refers to them. Projects: It sets the .spec.createdBy field for newly created Project resources, and defaults the .spec.owner field in case it is empty (to the same value of .spec.createdBy). Seeds: It rejects changing the .spec.settings.shootDNS.enabled value if there is at least one Shoot that refers to this seed. Shoots: It sets the gardener.cloud/created-by=\u0026lt;username\u0026gt; annotation for newly created Shoot resources.  ShootDNS (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It tries to assign a default domain to the Shoot if it gets scheduled to a seed that enables DNS for shoots (.spec.settings.shootDNS.enabled=true). It also validates that the DNS configuration (.spec.dns) is not set if the seed disables DNS for shoots.\nShootQuotaValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the resource consumption declared in the specification against applicable Quota resources. Only if the applicable Quota resources admit the configured resources in the Shoot then it allows the request. Applicable Quotas are referred in the SecretBinding that is used by the Shoot.\nShootStateDeletionValidator (enabled by default)\nThis admission controller reacts on DELETE operations for ShootStates. It prevents the deletion of the respective ShootState resource in case the corresponding Shoot resource does still exist in the system. This prevents losing the shoot\u0026rsquo;s data required to recover it / migrate its control plane to a new seed cluster.\nShootTolerationRestriction (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the .spec.tolerations used in Shoots against the whitelist of its Project, or against the whitelist configured in the admission controller\u0026rsquo;s configuration, respectively. Additionally, it defaults the .spec.tolerations in Shoots with those configured in its Project, and those configured in the admission controller\u0026rsquo;s configuration, respectively.\nShootValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates certain configurations in the specification against the referred CloudProfile (e.g., machine images, machine types, used Kubernetes version, \u0026hellip;). Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources). Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).\n"},{"uri":"https://gardener.cloud/documentation/tutorials/knative-install/","title":"Install Knative in Gardener clusters","tags":[],"description":"A walkthrough the steps for installing Knative in Gardener shoot clusters.","content":"This guide walks you through the installation of the latest version of Knative using pre-built images on a Gardener created cluster environment. To set up your own Gardener, see the documentation or have a look at the landscape-setup-template project. To learn more about this open source project, read the blog on kubernetes.io.\nYou can find guides for other platforms here.\nBefore you begin Knative requires a Kubernetes cluster v1.15 or newer.\nInstall and configure kubectl   If you already have kubectl CLI, run kubectl version --short to check the version. You need v1.10 or newer. If your kubectl is older, follow the next step to install a newer version.\n  Install the kubectl CLI.\n  Access Gardener   Create a project in the Gardener dashboard. This will essentially create a Kubernetes namespace with the name garden-\u0026lt;my-project\u0026gt;.\n  Configure access to your Gardener project using a kubeconfig. If you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard: go to the \u0026ldquo;Members\u0026rdquo; section and add a service account. You can then download the kubeconfig for your project. You can skip this step if you create your cluster using the user interface; it is only needed for programmatic access, make sure you set export KUBECONFIG=garden-my-project.yaml in your shell.   Creating a Kubernetes cluster You can create your cluster using kubectl cli by providing a cluster specification yaml file. You can find an example for GCP here. Make sure the namespace matches that of your project. Then just apply the prepared so-called \u0026ldquo;shoot\u0026rdquo; cluster crd with kubectl:\nkubectl apply --filename my-cluster.yaml The easier alternative is to create the cluster following the cluster creation wizard in the Gardener dashboard: Configure kubectl for your cluster You can now download the kubeconfig for your freshly created cluster in the Gardener dashboard or via cli as follows:\nkubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode \u0026gt; my-cluster.yaml This kubeconfig file has full administrators access to you cluster. For the rest of this guide be sure you have export KUBECONFIG=my-cluster.yaml set.\nInstalling Istio Knative depends on Istio. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need the ability to customize your installation.\nOtherwise, see the Installing Istio for Knative guide to install Istio.\nYou must install Istio on your Kubernetes cluster before continuing with these instructions to install Knative.\nInstalling cluster-local-gateway for serving cluster-internal traffic If you installed Istio, you can install a cluster-local-gateway within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, install and use the cluster-local-gateway.\nInstalling Knative The following commands install all available Knative components as well as the standard set of observability plugins. To customize your Knative installation, see Performing a Custom Knative Installation.\n  If you are upgrading from Knative 0.3.x: Update your domain and static IP address to be associated with the LoadBalancer istio-ingressgateway instead of knative-ingressgateway. Then run the following to clean up leftover resources:\nkubectl delete svc knative-ingressgateway -n istio-system kubectl delete deploy knative-ingressgateway -n istio-system If you have the Knative Eventing Sources component installed, you will also need to delete the following resource before upgrading:\nkubectl delete statefulset/controller-manager -n knative-sources While the deletion of this resource during the upgrade process will not prevent modifications to Eventing Source resources, those changes will not be completed until the upgrade process finishes.\n  To install Knative, first install the CRDs by running the kubectl apply command once with the -l knative.dev/crd-install=true flag. This prevents race conditions during the install, which cause intermittent errors:\nkubectl apply --selector knative.dev/crd-install=true \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   To complete the install of Knative and its dependencies, run the kubectl apply command again, this time without the --selector flag, to complete the install of Knative and its dependencies:\nkubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   Monitor the Knative components until all of the components show a STATUS of Running:\nkubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing kubectl get pods --namespace knative-monitoring   Set your custom domain  Fetch the external IP or CNAME of the knative-ingressgateway  kubectl --namespace istio-system get service knative-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE knative-ingressgateway LoadBalancer 100.70.219.81 35.233.41.212 80:32380/TCP,443:32390/TCP,32400:32400/TCP 4d Create a wildcard DNS entry in your custom domain to point to above IP or CNAME  *.knative.\u0026lt;my domain\u0026gt; == A 35.233.41.212 # or CNAME if you are on AWS *.knative.\u0026lt;my domain\u0026gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Adapt your knative config-domain (set your domain in the data field)  kubectl --namespace knative-serving get configmaps config-domain --output yaml apiVersion: v1 data: knative.\u0026lt;my domain\u0026gt;: \u0026#34;\u0026#34; kind: ConfigMap name: config-domain namespace: knative-serving What\u0026rsquo;s next Now that your cluster has Knative installed, you can see what Knative has to offer.\nTo deploy your first app with the Getting Started with Knative App Deployment guide.\nGet started with Knative Eventing by walking through one of the Eventing Samples.\nInstall Cert-Manager if you want to use the automatic TLS cert provisioning feature.\nCleaning up Use the Gardener dashboard to delete your cluster, or execute the following with kubectl pointing to your garden-my-project.yaml kubeconfig:\nkubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster "},{"uri":"https://gardener.cloud/documentation/guides/applications/content_trust/","title":"Integrity and Immutability","tags":[],"description":"Ensure that you get always the right image","content":"Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML\u0026rsquo;s or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9 or\napiVersion:apps/v1kind:Deploymentmetadata:name:rss-sitespec:replicas:1selector:matchLabels:app:webtemplate:metadata:labels:app:webspec:containers:- name:front-endimage:nginx:1.13.9ports:- containerPort:80But Tags are mutable and humans are prone to error. Not a good combination. Here we’ll dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de You can now make sure that the same image is always loaded at every deployment. It doesn\u0026rsquo;t matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. This solves the problem of trust.\nIn addition you should scan all images for known vulnerabilities, this can fill another book\n"},{"uri":"https://gardener.cloud/documentation/guides/client_tools/bash_kubeconfig/","title":"Kubeconfig context as bash prompt","tags":[],"description":"Expose the active kubeconfig into bash","content":"Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, the kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copying the used configuration always to the right place?\nExport the KUBECONFIG enviroment variable bash$ export KUBECONFIG=\u0026lt;PATH-TO-M\u0026gt;-CONFIG\u0026gt;/kubeconfig-dev.yaml How to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. bash$ Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it worth to be added here. Edit your ~/.bash_profile and add the following code snippet to show the current k8s context in the shell\u0026rsquo;s prompt.\nprompt_k8s(){ k8s_current_context=$(kubectl config current-context 2\u0026gt; /dev/null) if [[ $? -eq 0 ]] ; then echo -e \u0026#34;(${k8s_current_context}) \u0026#34;; fi } PS1+=\u0026#39;$(prompt_k8s)\u0026#39; After this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$ Note the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\Windows­PowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\nfunction prompt_k8s { $k8s_current_context = (kubectl config current-context) | Out-String if($?) { return $k8s_current_context }else { return \u0026#34;No K8S contenxt found\u0026#34; } } $host.ui.rawui.WindowTitle = prompt_k8s If you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/antipattern/","title":"Kubernetes Antipatterns","tags":[],"description":"Common Antipatterns for Kubernetes and Docker","content":"This HowTo covers common kubernetes antipatterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.\nWatch the very good presentation by Liz Rice at the KubeCon 2018\n Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and add a user to it. Use the USER command to switch to this user. Note that you may also consider to provide an explicit UID/GID if required.\nFor Example:\nARG GF_UID=\u0026#34;500\u0026#34; ARG GF_GID=\u0026#34;500\u0026#34; # add group \u0026amp; user RUN groupadd -r -g $GF_GID appgroup \u0026amp;\u0026amp; \\ useradd appuser -r -u $GF_UID -g appgroup USER appuser Store data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside of containers. Using an ELK stack is another good option for storing and processing logs.\nUsing pod IP addresses Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile. Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult. You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nCreating images in a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nSaving passwords in docker image 💀 Do not save passwords in a Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to provision passwords or inject them by mounting a persistent volume.\nUsing the \u0026lsquo;latest\u0026rsquo; tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case you don\u0026rsquo;t have complete control over your image - which is bad.\nDifferent images per environment Don\u0026rsquo;t create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nDepend on start order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.\nAdditional anti-patterns and patterns\u0026hellip; In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Refer to the following link for more information\n Kubernetes Production Patterns  "},{"uri":"https://gardener.cloud/components/kubify/","title":"kubify","tags":[],"description":"","content":"Kubify Kubify is a Terraform based provisioning project for setting up production ready Kubernetes clusters on public and private Cloud infrastructures. Kubify currently supports:\n OpenStack AWS Azure  Key features of Kubify are:\n Kubernetes v1.10.12 Etcd v3.3.10 multi master node setup Etcd backup and restore Supports rolling updates   To start using or developing Kubify locally See our documentation in the /docs repository or find the main documentation here.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Kubify itself as GitHub issues or join our Slack channel #gardener (Invite yourself to the Kubernetes Slack workspace here).\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/landscape-setup/","title":"Landscape Setup","tags":[],"description":"","content":"\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;yourclusterdomain\u0026gt;--oidc-client-id=kube-kubectl--oidc-username-claim=email--oidc-groups-claim=groupsFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt; Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt; Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39; deploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39; # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39; undeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39; Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting... while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...] Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found. If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"},{"uri":"https://gardener.cloud/documentation/concepts/mcm/","title":"Machine Controller Manager","tags":[],"description":"","content":"machine-controller-manager  \nMachine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs. The current implementation supports AWS, GCP, Azure, Alicloud, Packet and Openstack. It can easily be extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1 Key terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup. Node: Native kubernetes node objects. The objects you get to see when you do a \u0026ldquo;kubectl get nodes\u0026rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager See the design documentation in the /docs/design repository, please find the design doc here.\nTo start using or developing the Machine Controller Manager See the documentation in the /docs repository, please find the index here.\nCluster-api Implementation  cluster-api branch of machine-controller-manager implements the machine-api aspect of the cluster-api project. Link: https://github.com/gardener/machine-controller-manager/tree/cluster-api Once cluster-api project gets stable, we may make master branch of MCM as well cluster-api compliant, with well-defined migration notes.  "},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/add-node-to-cluster/","title":"Manually adding a node to an existing cluster","tags":[],"description":"How to add a node to an existing cluster without the support of Gardener","content":"Manually adding a node to an existing cluster Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\nDisclaimer  Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener. Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be responsible to replace it.\n How   Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.\nTo ssh into a machine which is already in the cluster, use the steps defined here.\nAttach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node\u0026rsquo;s name.\n  On the new machine, create file /var/lib/kubelet/kubeconfig-bootstrap with the following content:\n  apiVersion:v1kind:Configcurrent-context:kubelet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;CACertificate\u0026gt; server: \u0026lt;Server\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:kubelet-bootstrapname:kubelet-bootstrap@defaultusers:- name:kubelet-bootstrapuser:as-user-extra:{}token:\u0026lt;Token\u0026gt;ssh into an existing node, and run these commands to get the values of and  to be replaced in above file:   \u0026lt;Servr\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;server\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;CA Certificate\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;certificate-authority-data\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;Token\u0026gt;\nThe kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the kube-system namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its .data.expiration field. The name of this secret is of the format bootstrap-token-*. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets. To get an unexpired token, find the secrets with the name format bootstrap-token-* in the kube-system namespace in the cluster, and pick the one with minimum age. Eg. bootstrap-token-abcdef.\nRun these commands to get the token:\ntokenid=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-id\u0026#34;}}\u0026#39; | base64 --decode) tokensecret=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-secret\u0026#34;}}\u0026#39; | base64 --decode) echo $tokenid.$tokensecret The value of $TOKEN will be tokenid.tokensecret. Replace $TOKEN in above file with this value\n  Copy contents of the files - /var/lib/kubelet/config/kubelet, /var/lib/kubelet/ca.crt and /etc/systemd/system/kubelet.service - from an existing node to the new node\n  Run the following command in the new node to start the kubelet:\n  systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet The new node should be added to the existing cluster within a couple of minutes.\n"},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/25_markup/","title":"Markdown","tags":[],"description":"","content":"Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesn’t support well. You could use pure HTML to expand possibilities.\nBut this happens to be a bad idea. Everyone uses Markdown because it\u0026rsquo;s pure and simple to read even non-rendered. You should avoid HTML to keep it as simple as possible.\nTo avoid this limitations, Hugo created shortcodes. A shortcode is a simple snippet inside a page.\nGardener provides multiple shortcodes on top of existing ones.\n Attachments  The Attachments shortcode displays a list of files attached to a page.\n Button  Nice buttons on your page.\n Expand  Displays an expandable/collapsible section of text on your page\n Mermaid  Generation of diagram and flowchart from text in a similar manner as markdown\n Notice  Disclaimers to help you structure your page\n "},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/25_markup/mermaid/","title":"Mermaid","tags":[],"description":"Generation of diagram and flowchart from text in a similar manner as markdown","content":"Mermaid is a library helping you to generate diagram and flowcharts from text, in a similar manner as Markdown.\nJust insert your mermaid code in the mermaid shortcode and that\u0026rsquo;s it.\nFlowchart example {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] {{\u0026lt; /mermaid \u0026gt;}}  renders as\ngraph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two]  Sequence example {{\u0026lt;mermaid\u0026gt;}} sequenceDiagram participant Alice participant Bob Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! {{\u0026lt; /mermaid \u0026gt;}}  renders as\nsequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good!  GANTT Example {{\u0026lt;mermaid\u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d {{\u0026lt; /mermaid \u0026gt;}}  render as\ngantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d  "},{"uri":"https://gardener.cloud/documentation/guides/applications/network-isolation/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.\nThere are many reasons why you may chose to employ Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other  Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDRs or IP addresses used for matching source or destination IP’s. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and select subsets of objects.\nExample We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose kubectl create ns customer1 kubectl create ns customer2 # create a standard HTTP web server kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1 kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2 # expose the port 80 for external access kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1 kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2  Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \u0026#34;bash\u0026#34; pod in one namespace kubectl run -i --tty client --image=tutum/curl -n=customer1 try to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \u0026#34;customer1\u0026#34; =\u0026gt; success curl http://nginx.customer1 # get the index.html from the nginx of the namespace \u0026#34;customer2\u0026#34; =\u0026gt; success curl http://nginx.customer2 Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\n Test with NP Install the NetworkPolicy from your shell\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-from-other-namespacesspec:podSelector:matchLabels:ingress:- from:- podSelector:{} it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1 kubectl apply -f ./network-policy.yaml -n=customer2 after this curl http://nginx.customer2 shouldn\u0026rsquo;t work anymore if you are a service inside the namespace customer1 and vice versa\nNote: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type LoadBalancer in namespace customer1 that match the nginx pod. When you request the service by its \u0026lt;EXTERNAL_IP\u0026gt;:\u0026lt;PORT\u0026gt;, then the network policy will deny the ingress traffic from the service and the request will time out.\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  "},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/25_markup/notice/","title":"Notice","tags":[],"description":"Disclaimers to help you structure your page","content":"The notice shortcode shows 4 types of disclaimers to help you structure your page.\nNote {{% notice note %}} A notice disclaimer {{% /notice %}} renders as\nA notice disclaimer  Info {{% notice info %}} An information disclaimer {{% /notice %}} renders as\nAn information disclaimer  Tip {{% notice tip %}} A tip disclaimer {{% /notice %}} renders as\nA tip disclaimer  Warning {{% notice warning %}} An warning disclaimer {{% /notice %}} renders as\nA warning disclaimer  "},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/operator_alerts/","title":"Operator Alerts","tags":[],"description":"","content":"Operator Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiserverDown blocker seed All API server replicas are down/unreachable, or all API server could not be found.   KubeApiServerTooManyAuditlogFailures critical seed The API servers cumulative failure rate in logging audit events is {{ printf \u0026quot;%0.2f\u0026quot; $value }}%. This may be caused by an unavailable/unreachable audisink(s) and/or improper API server audit configuration.   KubeControllerManagerDown critical seed Deployments and replication controllers are not making progress.   KubeEtcdMainDown blocker seed Etcd3 cluster main is unavailable or cannot be scraped. As long as etcd3 main is down the cluster is unreachable.   KubeEtcdEventsDown critical seed Etcd3 cluster events is unavailable or cannot be scraped. Cluster events cannot be collected.   KubeEtcd3MainNoLeader critical seed Etcd3 main has no leader. No communication with etcd main possible. Apiserver is read only.   KubeEtcd3EventsNoLeader critical seed Etcd3 events has no leader. No communication with etcd events possible. New cluster events cannot be collected. Events can only be read.   KubeEtcd3HighNumberOfFailedProposals warning seed Etcd3 pod {{ $labels.pod }} has seen {{ $value }} proposal failures within the last hour.   KubeEtcd3DbSizeLimitApproaching warning seed Etcd3 {{ $labels.role }} DB size is approaching its current practical limit of 2GB.   KubeEtcd3DbSizeLimitCrossed critical seed Etcd3 {{ $labels.role }} DB size has crossed its current practical limit of 2GB. Etcd might now require more memory to continue serving traffic with low latency, and might face request throttling.   KubeEtcdDeltaBackupFailed critical seed No delta snapshot for the past at least 30 minutes.   KubeEtcdFullBackupFailed critical seed No full snapshot taken in the past day.   KubeEtcdRestorationFailed critical seed Etcd data restoration was triggered, but has failed.   KubeletTooManyOpenFileDescriptorsSeed critical seed Seed-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePersistentVolumeUsageCritical critical seed The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is only {{ printf \u0026quot;%0.2f\u0026quot; $value }}% free.   KubePersistentVolumeFullInFourDays warning seed Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ printf \u0026quot;%0.2f\u0026quot; $value }}% is available.   KubePodPendingControlPlane warning seed Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 30 minutes.   KubePodNotReadyControlPlane warning  Pod {{ $labels.pod }} is not ready for more than 30 minutes.   KubeSchedulerDown critical seed New pods are not being assigned to nodes.   KubeStateMetricsShootDown info seed There are no running kube-state-metric pods for the shoot cluster. No kubernetes resource metrics can be scraped.   KubeStateMetricsSeedDown critical seed There are no running kube-state-metric pods for the seed cluster. No kubernetes resource metrics can be scraped.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   PrometheusCantScrape warning seed Prometheus failed to scrape metrics. Instance {{ $labels.instance }}, job {{ $labels.job }}.   PrometheusConfigurationFailure warning seed Latest Prometheus configuration is broken and Prometheus is using the previous one.   VPNShootNoPods critical shoot vpn-shoot deployment in Shoot cluster has 0 available pods. VPN won't work.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/documentation/guides/applications/container-startup/","title":"Orchestration of container startup","tags":[],"description":"How to orchestrate startup sequence of multiple containers","content":"Disclaimer If an application depends on other services deployed separately do not rely on a certain start sequence of containers but ensure that the application can cope with unavailability of the services it depends on.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod\u0026rsquo;s initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=info msg=\u0026#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\u0026#34; time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=fatal msg=\u0026#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\u0026#34; $ kubectl get po -w NAME READY STATUS RESTARTS AGE webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 1s webapp-958cf5567-h247n 0/1 Error 0 2s webapp-958cf5567-h247n 0/1 Error 1 3s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s webapp-958cf5567-h247n 0/1 Error 2 18s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s webapp-958cf5567-h247n 0/1 Error 3 43s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s If the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers  can be defined which are executed prior to the application container. If one InitContainers fails, the application container won\u0026rsquo;t be triggered.\napiVersion:apps/v1kind:Deploymentmetadata:name:webappspec:selector:matchLabels:app:webapptemplate:metadata:labels:app:webappspec:initContainers:# check if DB is ready, and only continue when true- name:check-db-readyimage:postgres:9.6.5command:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;\u0026#39;]containers:- image:xcoulon/go-url-shortener:0.1.0name:go-url-shortenerenv:- name:POSTGRES_HOSTvalue:postgres- name:POSTGRES_PORTvalue:\u0026#34;5432\u0026#34;- name:POSTGRES_DATABASEvalue:url_shortener_db- name:POSTGRES_USERvalue:user- name:POSTGRES_PASSWORDvalue:mysecretpasswordports:- containerPort:8080In above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w NAME READY STATUS RESTARTS AGE nginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d privileged-pod 1/1 Running 0 4d webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s $ kubectl logs webapp-fdcb49cbc-4gs4n Error from server (BadRequest): container \u0026#34;go-url-shortener\u0026#34; in pod \u0026#34;webapp-fdcb49cbc-4gs4n\u0026#34; is waiting to start: PodInitializing "},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/10_organisation/","title":"Organisation","tags":[],"description":"","content":"Content Organisation This site uses Hugo. In Hugo, content organization is a core concept.\n**Hugo Tip:** Start Hugo with `hugo server --navigateToChanged` for content edit-sessions.  Page Lists Page Order The documentation side menu, the documentation page browser etc. are listed using Hugo\u0026rsquo;s default sort order, which sorts by weight (from 1), date (newest first) and finally by the link title.\nGiven that, if you want to move a page or a section up, set a weight in the page\u0026rsquo;s front matter:\ntitle:MyPageweight:10For page weights, it can be smart not to use 1, 2, 3 ..., but some other interval, say 10, 20, 30... This allows you to insert pages where you want later.  "},{"uri":"https://gardener.cloud/documentation/guides/client_tools/working-with-kubeconfig/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"Organizing Access Using kubeconfig Files The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\nProblem If you\u0026rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials in case anything was leaked. However, this is not possible with the initial or master kubeconfig from your cluster.\nPitfall Never distribute the kubeconfig, which you can download directly within the Gardener dashboard, for a productive cluster.\nCreate custom kubeconfig file for each user Create a separate kubeconfig for each user. One of the big advantages is, that you can revoke them and control the permissions better. A limitation to single namespaces is also possible here.\nThe script creates a new ServiceAccount with read privileges in the whole cluster (Secretes are excluded). To run the script jq, a lightweight and flexible command-line JSON processor, must be installed.\n#!/bin/bash  if [[ -z \u0026#34;$1\u0026#34; ]] ;then echo \u0026#34;usage: $0\u0026lt;username\u0026gt;\u0026#34; exit 1 fi user=$1 kubectl create sa ${user} secret=$(kubectl get sa ${user} -o json | jq -r .secrets[].name) kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;ca.crt\u0026#34;]\u0026#39; | base64 -D \u0026gt; ca.crt user_token=$(kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;token\u0026#34;]\u0026#39; | base64 -D) c=`kubectl config current-context` cluster_name=`kubectl config get-contexts $c | awk \u0026#39;{print $3}\u0026#39; | tail -n 1` endpoint=`kubectl config view -o jsonpath=\u0026#34;{.clusters[?(@.name == \\\u0026#34;${cluster_name}\\\u0026#34;)].cluster.server}\u0026#34;` # Set up the config KUBECONFIG=k8s-${user}-conf kubectl config set-cluster ${cluster_name} \\  --embed-certs=true \\  --server=${endpoint} \\  --certificate-authority=./ca.crt KUBECONFIG=k8s-${user}-conf kubectl config set-credentials ${user}-${cluster_name#cluster-} --token=${user_token} KUBECONFIG=k8s-${user}-conf kubectl config set-context ${user}-${cluster_name#cluster-} \\  --cluster=${cluster_name} \\  --user=${user}-${cluster_name#cluster-} KUBECONFIG=k8s-${user}-conf kubectl config use-context ${user}-${cluster_name#cluster-} cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: view-${user}-global subjects: - kind: ServiceAccount name: ${user} namespace: default roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io EOF echo \u0026#34;done! Test with: \u0026#34; echo \u0026#34;export KUBECONFIG=k8s-${user}-conf\u0026#34; echo \u0026#34;kubectl get pods\u0026#34; If edit or admin rights are to be assigned, the ClusterRoleBinding must be adapted in the roleRef section with the roles listed below.\nFurthermore, you can restrict this to a single namespace by not creating a ClusterRoleBinding but only a RoleBinding within the desired namespace.\n   Default ClusterRole Default ClusterRoleBinding Description     cluster-admin system:masters group Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding\u0026rsquo;s namespace, including the namespace itself.   admin None Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.   edit None Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.   view None Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.    "},{"uri":"https://gardener.cloud/documentation/guides/applications/service-cache-control/","title":"Out-Dated HTML and JS files delivered","tags":[],"description":"Why is my application always outdated?","content":"Problem After updating your HTML and JavaScript sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080where:\n \u0026lt;GARDENER-CLUSTER\u0026gt;: The cluster name in the Gardener \u0026lt;GARDENER-PROJECT\u0026gt;: You project name in the Gardener  What is the underlying problem? The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below:\n use a cache buster + HTTP-Cache-Control(prefered) use HTTP-Cache-Control with a lower retention period disable the caching in the ingress (just for dev purpose)  Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise for your web framework (e.g. Express/NodeJS, SpringBoot,\u0026hellip;)\nHere an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (during development).\n---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:annotations:ingress.kubernetes.io/cache-enable:\u0026#34;false\u0026#34;name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/37_process/","title":"Process","tags":[],"description":"","content":"Creating a new Feature If you want to contribute to the Gardener, please do that always on a dedicated branch on your own fork named after the purpose of the code changes, for example feature/helm-integration. Please do not forget to rebase your branch regularly.\nIf you have finished your work, please create a pull request based on master. It will be reviewed and merged if no further changes are requested from you.\n:warning: Please ensure that your modifications pass the lint checks, formatting checks, static code checks, and unit tests by executing\nmake verify Please do not file your pull request unless you receive a successful response from here!\nCreating a new Release Please refer to the Gardener contributor guide.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/read-write-many/","title":"ReadWriteMany with AWS","tags":[],"description":"Dynamically Provisioned PV’s Using Amazon EFS","content":""},{"uri":"https://gardener.cloud/documentation/contribute/10_code/40_repositories/","title":"Repositories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/dns_names/","title":"Request DNS Names","tags":[],"description":"Requesting DNS Names for Ingresses and Services in Shoot Clusters","content":"Request DNS Names in Shoot Clusters Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. Therefore the gardener must be installed with the shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\nShoot Feature Gate The shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster). Therefore it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must add the shoot-dns-service extension.\n...spec:extensions:- type:shoot-dns-service...Configuration In Shoot Cluster To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class garden and an annotation denoting the desired DNS names.\nFor a Service (it must have the type LoadBalancer) this looks like this:\napiVersion:v1kind:Servicemetadata:annotations:dns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:my.subdomain.for.shootsomain.cloudname:my-servicenamespace:defaultspec:ports:- port:80protocol:TCPtargetPort:80type:LoadBalancerThe dnsnames annotation accepts a comma-separated list of DNS names, if multiple names are required.\nFor an Ingress, the dns names are already declared in the specification. Nevertheless the dnsnames annotation must be present. Here a subset of the dns names of the ingress can be specified. If DNS names for all names are desired, the value all can be used.\nIf one of the accepted dns names is a direct subname of the shoot\u0026rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the dnsnames list in the annotation. If only this dns name is configured in the ingress, no explicit dns entry is required, and the dns annotations should be omitted at all.\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/x509_certificates/","title":"Request X.509 Certificates","tags":[],"description":"X.509 Certificates For TLS Communication","content":"Request X.509 Certificates Introduction Dealing with applications on Kubernetes which offer service endpoints (e.g. HTTP) may also require you to enable a secured communication via SSL/TLS. Gardener let\u0026rsquo;s you request a commonly trusted X.509 certificate for your application endpoint. Furthermore, Gardener takes care about the renewal process for your requested certificate.\nLet\u0026rsquo;s get the basics straight first. If this is too long for you, you can read below how to get certificates by\n Certificate Resources Ingress Service  Restrictions Domains Certificates may be received for any subdomain of your shoot\u0026rsquo;s domain (see .spec.dns.domain of your shoot resource) with the default issuer.\nCharacter Restrictions Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).\nFor example, the following request is invalid:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-invalidnamespace:defaultspec:commonName:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudBut it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudCertificate Resources Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener\u0026rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can either be requested by creating Certificate resources in the Kubernetes cluster or by configuring Ingress or Service (type LoadBalancer) resources. If the latter is used, a Certificate resource will automatically be created by Gardener\u0026rsquo;s certificate service.\nIf you\u0026rsquo;re interested in the current progress of your request, you\u0026rsquo;re advised to consult the Certificate's status subresource. You\u0026rsquo;ll also find error descriptions in the status in case the issuance failed.\nCertificate status example:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificate...status:commonName:short.ingress.shoot.project.default-domain.gardener.cloudexpirationDate:\u0026#34;2020-02-27T15:39:10Z\u0026#34;issuerRef:name:gardennamespace:shoot--foo--barlastPendingTimestamp:\u0026#34;2019-11-29T16:38:40Z\u0026#34;observedGeneration:11state:ReadyCustom Domains If you want to request certificates for domains other then any subdomain of shoot.spec.dns.domain, the following configuration is required:\nDNS provider In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for host.example.com your DNS provider must be capable of managing subdomains of host.example.com.\nDNS providers are specified in the shoot manifest:\nkind:Shoot...spec:dns:providers:- type:aws-route53# consult the DNS provisioning controllers group (dnscontrollers) in https://github.com/gardener/external-dns-management#using-the-dns-controller-manager for possible valuessecretName:provider-example-com# contains credentials for service account, see any 20-secret-\u0026lt;provider\u0026gt;-credentials.yaml in https://github.com/gardener/external-dns-management/tree/master/examplesThe secret referenced by secretName can also be conveniently created via the Gardener dashboard.\nIssuer Another prerequisite to request certificates for custom domains is a dedicated issuer.\nkind:Shoot...spec:extensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1kind:CertConfigissuers:- email:your-email@example.comname:custom-issuer# issuer name must be specified in every custom issuer request, must not be \u0026#34;garden\u0026#34;server:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;Examples Request a certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudsecretRef:name:cert-examplenamespace:default# issuerRef:# name: custom-issuer spec.commonName (required) specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.\n  spec.dnsName additional domains the certificate should be valid for. Entries in this list can be longer than 64 characters.\n  spec.secretRef specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.\n  spec.issuerRef (optional) specifies the issuer you want to use. Only necessary if you request certificates for custom domains.\n Request a wildcard certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-wildcardnamespace:defaultspec:commonName:\u0026#39;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#39;secretRef:name:cert-wildcardnamespace:default# issuerRef:# name: custom-issuer spec.commonName (required) specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.\n  Please note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n  spec.secretRef specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.\n  spec.issuerRef (optional) specifies the issuer you want to use. Only necessary if you request certificates for custom domains.\n Request a certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- short.ingress.shoot.project.default-domain.gardener.cloud- morethan64characters.ingress.shoot.project.default-domain.gardener.cloud# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080 metadata.annotations must contain cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.\n  spec.tls[].hosts specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.\n  spec.tls[].secretName specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.\n Request a wildcard certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- \u0026#34;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#34;# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080 metadata.annotations must contain cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.\n  spec.tls[].hosts please make sure the wildcard domain complies with the 64 character limit.\n  Please note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n Request a certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;service.shoot.project.default-domain.gardener.cloud, morethan64characters.svc.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer metadata.annotations[cert.gardener.cloud/secretname] specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.\n  metadata.annotations[cert.gardener.cloud/issuer] is optional and may be specified if the certificate is request for a custom domains.\n  metadata.annotations[dns.gardener.cloud/dnsnames] specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.\n Request a wildcard certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;*.service.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer metadata.annotations[cert.gardener.cloud/secretname] specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.\n  metadata.annotations[cert.gardener.cloud/issuer] is optional and may be specified if the certificate is request for a custom domains.\n  metadata.annotations[dns.gardener.cloud/dnsnames] please make sure the wildcard domain complies with the 64 character limit.\n  Please note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n  #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  "},{"uri":"https://gardener.cloud/documentation/contribute/10_code/12-security_guide/","title":"Security Release Process","tags":[],"description":"","content":"Gardener Security Release Process Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.\nGardener Security Team Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits. The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:\n Olaf Beier, (@olafbeier) Vasu Chandrasekhara, (@vasu1124) Alban Crequy, (@alban) Norbert Hamann, (@norberthamann) Claudia Hölters, (@hoeltcl) Oliver Kling, (@oliverkling) Vedran Lerenc, (@vlerenc) Dirk Marwinski, (@marwinski) Michael Schubert, (@schu) Matthias Sohn, (@msohn) Frederik Thormaehlen, (@ThormaehlenFred) Christian Cwienk (@ccwienk)  Disclosures Private Disclosure Processes The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you\u0026rsquo;ve found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to secure@sap.com. We\u0026rsquo;ll send a confirmation e-mail to acknowledge your report, and we\u0026rsquo;ll send an additional e-mail when we\u0026rsquo;ve identified the issue positively or negatively.\nPublic Disclosure Processes If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to secure@sap.com to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.\nIf possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a private disclosure process (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn\u0026rsquo;t necessary and is unlikely to make a public disclosure less damaging.\nPatch, Release, and Public Communication For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the \u0026ldquo;Fix Team\u0026rdquo; and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the \u0026ldquo;Fix Lead.\u0026rdquo; The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the \u0026ldquo;Fix team.\u0026rdquo; (I.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score \u0026gt;= 7; see below). If the fix relies on another upstream project\u0026rsquo;s disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.\nFix Team Organization The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team. The Fix Lead will give the Fix Team access to a private security repository to develop the fix.\nFix Development Process The Fix Lead and the Fix Team will create a CVSS using the CVSS Calculator. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect. The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers. If the CVSS score is under 7.0 (a medium severity score) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private Gardener Security mailing list.\nFix Disclosure Process With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the Gardener mailing list that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.\nFix Release Day The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date. The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue. The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will LGTM and merge. The Release Managers will merge these PRs as quickly as possible. Changes shouldn\u0026rsquo;t be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches. The Fix Lead will request a CVE from the SAP Product Security Response Team via email to cna@sap.com with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the Gardener mailing list and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.\nAs much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia. The Fix Lead will remove the Fix Team from the private security repository.\nRetrospective These steps should be completed after the Release Date. The retrospective process should be blameless.\nThe Fix Lead will send a retrospective of the process to the Gardener mailing list including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process. The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the Gardener mailing list. Honest critique is the only way we are going to get good at this as a community.\nCommunication Channel The private or public disclosure process should be triggered exclusively by writing an e-mail to secure@sap.com.\nGardener security announcements will be communicated by the Fix Lead sending an e-mail to the Gardener mailing list (reachable via gardener@googlegroups.com) as well as posting a link in the Gardener Slack channel. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (how to find and join a group)\nThe members of the Gardener Security Team are subscribed to the private Gardener Security mailing list (reachable via gardener-security@googlegroups.com).\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/setup-seed/","title":"Setting up a Seed Cluster","tags":[],"description":"How to configure a Kubernetes cluster as a Gardener seed","content":"The Seed Cluster The landscape-setup-template is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don\u0026rsquo;t have network policies, for example. See Hardening the Gardener Community Setup for more information.\nTo have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.\nSetting up the Shoot The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won\u0026rsquo;t work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but kubectl apply and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.\nSo, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest here. You could, for example, change the CIDRs to this:\n...networks:internal:- 10.254.112.0/22nodes:10.254.0.0/19pods:10.255.0.0/17public:- 10.254.96.0/22services:10.255.128.0/17vpc:cidr:10.254.0.0/16workers:- 10.254.0.0/19...Also make sure that your new seed cluster has enough resources for the expected number of shoots.\nRegistering the Shoot as Seed The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the seed-config component of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the state/seed-config/ directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.\n1. Seed Namespace First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called seed-test.\n2. Cloud Provider Secret The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).\napiVersion:v1kind:Secretmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awstype:Opaquedata:accessKeyID:\u0026lt;base64-encodedAWSaccesskey\u0026gt; secretAccessKey: \u0026lt;base64-encoded AWS secret key\u0026gt;kubeconfig:\u0026lt;base64-encodedkubeconfig\u0026gt;Deploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.\n3. Secretbinding for Cloud Provider Secret Create a secretbinding for your cloud provider secret:\napiVersion:core.gardener.cloud/v1beta1kind:SecretBindingmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awssecretRef:name:test-seed-secret# namespace: only required if in different namespace than referenced secretquotas:[]You can give it the same name as the referenced secret.\n4. Cloudprofile The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don\u0026rsquo;t want to change anything.\n5. Seed Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.\napiVersion:core.gardener.cloud/v1beta1kind:Seedmetadata:name:aws-securespec:provider:type:awsregion:eu-west-1secretRef:name:test-seed-secretnamespace:seed-testdns:ingressDomain:ingress.\u0026lt;yourclusterdomain\u0026gt; networks:nodes:10.254.0.0/19pods:10.255.0.0/17services:10.255.128.0/176. Hide Original Seed In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.\nTo solve this problem, edit the original seed and set its spec.visible field to false. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.\n"},{"uri":"https://gardener.cloud/api-reference/settings/","title":"Settings","tags":[],"description":"","content":"Packages:\n  settings.gardener.cloud/v1alpha1   settings.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  ClusterOpenIDConnectPreset  OpenIDConnectPreset  ClusterOpenIDConnectPreset   ClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot objects cluster-wide.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  ClusterOpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterOpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project mathching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n       OpenIDConnectPreset   OpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot in a namespace.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  OpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  OpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n       ClusterOpenIDConnectPresetSpec   (Appears on: ClusterOpenIDConnectPreset)  ClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and project selector matching Shoots in Projects.\n   Field Description      OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project mathching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    KubeAPIServerOpenIDConnect   (Appears on: OpenIDConnectPresetSpec)  KubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientID  string    The client ID for the OpenID Connect client. Required.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT). Required.\n    requiredClaims  map[string]string    (Optional) key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value. Only applied when the Kubernetes version of the Shoot is \u0026gt;= 1.11\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1 Defaults to [RS256]\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details. Defaults to \u0026ldquo;sub\u0026rdquo;.\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OpenIDConnectPresetSpec)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      secret  string    (Optional) The client Secret for the OpenID Connect client.\n    extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    OpenIDConnectPresetSpec   (Appears on: OpenIDConnectPreset, ClusterOpenIDConnectPresetSpec)  OpenIDConnectPresetSpec contains the Shoot selector for which a specific OpenID Connect configuration is applied.\n   Field Description      server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n     "},{"uri":"https://gardener.cloud/documentation/tutorials/s3/","title":"Shared storage with S3 backend","tags":[],"description":"Using S3 bucket as shared storage for pods","content":"Shared storage with S3 backend The storage is definitely the most complex and important part of an application setup, once this part is completed, 80% of the tasks are completed.\nMounting an S3 bucket into a pod using FUSE allows you to access the data as if it were on the local disk. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\nOverview Limitations Generally S3 cannot offer the same performance or semantics as a local file system. More specifically:\n random writes or appends to files require rewriting the entire file metadata operations such as listing directories have poor performance due to network latency eventual consistency can temporarily yield stale data(Amazon S3 Data Consistency Model) no atomic renames of files or directories no coordination between multiple clients mounting the same bucket no hard links  Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using the Gardener.\nEnsure that you have create the \u0026ldquo;imagePullSecret\u0026rdquo; in your cluster.\nkubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; Setup The first step is to clone this repository. Next is the Secret for the AWS API credentials of the user that has full access to our S3 bucket. Copy the configmap_secrets_template.yaml to configmap_secrets.yaml and place your secrets at the right place\napiVersion:v1kind:ConfigMapmetadata:name:s3-configdata:S3_BUCKET:\u0026lt;YOUR-S3-BUCKET-NAME\u0026gt; AWS_KEY: \u0026lt;YOUR-AWS-TECH-USER-ACCESS-KEY\u0026gt;AWS_SECRET_KEY:\u0026lt;YOUR-AWS-TECH-USER-SECRET\u0026gt;Build and deploy Change the settings in the build.sh file with your docker registry settings.\n#!/usr/bin/env bash  ######################################################################################################################## # PREREQUISTITS ######################################################################################################################## # # - ensure that you have a valid Artifactory or other Docker registry account # - Create your image pull secret in your namespace # kubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; # - change the settings below arcording your settings # # usage: # Call this script with the version to build and push to the registry. After build/push the # yaml/* files are deployed into your cluster # # ./build.sh 1.0 # VERSION=$1 PROJECT=kube-s3 REPOSITORY=cp-enablement.docker.repositories.sap.ondemand.com # causes the shell to exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x . . . . Create the S3Fuse Pod and check the status:\n# build and push the image to your docker registry ./build.sh 1.0 # check that the pods are up and running kubectl get pods Check success Create a demo Pod and check the status:\nkubectl apply -f ./yaml/example_pod.yaml # wait some second to get the pod up and running... kubectl get pods # go into the pd and check that the /var/s3 is mounted with your S3 bucket content inside kubectl exec -ti test-pd sh # inside the pod ls -la /var/s3 Why does this work? Docker engine 1.10 added a new feature which allows containers to share the host mount namespace. This feature makes it possible to mount a s3fs container file system to a host file system through a shared mount, providing a persistent network storage with S3 backend.\nThe key part is mountPath: /var/s3:shared which enables the volume to be mounted as shared inside the pod. When the container starts it will mount the S3 bucket onto /var/s3 and consequently the data will be available under /mnt/data-s3fs on the host and thus to any other container/pod running on it (and has /mnt/data-s3fs mounted too).\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/maintain-shoot/","title":"Shoot Cluster Maintenance","tags":[],"description":"Understanding and configuring Gardener&#39;s Day-2 operations for Shoot clusters.","content":"Shoot Cluster Maintenance Day two operations like updating the Kubernetes patch version and updating the Operating system version, happen in a daily maintenance time window of the Shoot cluster. The maintenance time window is part of the shoot spec (.spec.maintenance.timeWindow). If it is not specified during Shoot creation, Gardener will default to a randomized time window (to spread the load). The time interval cannot be less than 30 minutes and more than 6 hours.\nWhen referring to an operating system update in this document, this means updating the Shoot\u0026rsquo;s worker nodes with a machine image (e.g. AMI for AWS) that comes with a higher version of the operating system.\nDuring the maintenance, the Gardener Controller Manager updates the Shoot\u0026rsquo;s Kubernetes and operating system versions.\nA version is updated if either:\n there is a higher semantic version available in the CloudProfile and the Shoot opts-in for automatic version updates. See automatic Kubernetes version updates and automatic Operating System version updates. the currently used version is expired. See forceful version updates.  A version update during the maintenance time triggers a Shoot reconciliation.\nTo manually trigger the maintenance operation, the Shoot can be annotated with `gardener.cloud/operation: maintain`.\n Version Classifications Gardener classifies versions into preview,supported, deprecated and expired. Please see here for more information. Gardener takes version classifications into account during the maintenance operations as part of the efforts to ensure stable version updates without negatively impacting the workload deployed in the cluster. As such, preview versions are excluded from updates during the maintenance. This is because preview versions are typically recently releases version that have not yet undergone thorough testing and may contain bugs or security issues.\nAutomatic Kubernetes Version Updates If a Shoot is configured for automatic Kubenernetes version updates by setting .spec.maintenance.autoUpdate.kubernetesVersion: true, Gardener keeps the Kubernetes version of the Shoot up to date. During the daily maintenance time window, the Shoot\u0026rsquo;s Kubernetes version is updated to the latest non-preview classified patch version available in the associated CloudProfile (spec.cloudProfileName). The CloudProfile specifies the available Kubernetes versions in .spec.kubernetes.versions.\nIt will never auto update the Major releases since Kubernetes does not garuantee backward compatibility and updateability in those releases. While this is also true for minor versions, (forceful minor version updates)[#forceful-version-updates] can happen during the maintenance time window (even though specifying .spec.maintenance.autoUpdate.kubernetesVersion: false).\nConsider a Shoot cluster with the field values below (only related fields are shown):\nspec:kubernetes:version:1.10.0maintenance:timeWindow:begin:220000+0000end:230000+0000autoUpdate:kubernetesVersion:trueUpdating the CloudProfile used in the Shoot by adding 1.10.5 and 1.11.0 to the .spec.kubernetes.versions list, automatically updates the Shoot to 1.10.5 between 22:00-23:00 UTC. The Shoot won\u0026rsquo;t be automatically updated to 1.11.0 even though its the highest Kubernetes version in the CloudProfile. This is because that wouldn\u0026rsquo;t be a patch release update but a minor release update, and as such potentially have breaking changes that could impact the deployed workload/applications.\nIn this example if the operator wants to update the Kubernetes version to 1.11.0, he/she must update the Shoot\u0026rsquo;s .spec.kubernetes.version to 1.11.0 manually.\nBesides automatic versioning during the maintenance time, version updates can also be handled by updating the .spec.kubernetes.version field manually. In the above example, if the operator wants to update the Kubernetes version to 1.11.0, he/she can update the Shoot\u0026rsquo;s .spec.kubernetes.version to 1.11.0 manually.\nThese updates will either be executed immediately (default) or can be confined to the maintenance time window. Choosing the latter option, causes changes to the cluster (e.g. node pool rolling-updates) and the subsequent reconciliation, to only predictably happen during a defined time window. This is available since Gardener version 1.4. Before applying such update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.\nAutomatic Operating System Version Updates If a Shoot is configured for automatic Kubenernetes version updates by setting .spec.maintenance.autoUpdate.machineImageVersion: true, then during the maintenance time window, the Gardener makes sure that the Shoot is using the most recent patch version of the operating system. An update to the operating system of a worker pool in the Shoot cluster causes the Shoot to be reconciled.\nDuring the reconciliation, the corresponding \u0026lt;Provider\u0026gt;MachineClass resource in the Shoot namespace in the Seed will be updated and the machine controller manager will take care of the actual state to match the desired one.\nPlease note, that in contrast to the Kubernetes version update, the Operating System always updates to the latest version available in the `CloudProfile`.\n Forceful Version Updates While the automatic Kubernetes and operating system updates, are an opt-in feature for the Shoot cluster owner, Gardener administrators can force patch and minor version updates.\nForceful updates are triggered by an expiration date in the past for the Kubernetes or Operating System versions in the CloudProfile. Whilst an administrator should classify a version having an expiration date as deprecated, such a labeling is not evaluated for the force update.\nVersion expiration dates allow smoother transitions for Shoot owners giving them time for testing before the actual forceful version update happens.\nPlease note, that specifying an `expiration date` for the latest version in the `CloudProfile` is not allowed.\n Just like automatic version updates, forceful updates are only applied during the next maintenance time window.\nForce Version Updates for Kubernetes Versions The following scenarios exemplify the automatic patch update from an expired Kubernetes version.\n Forceful patch version update from expired Kubernetes version.  Let\u0026rsquo;s assume the following CloudProfile spec (only related fields are shown):\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.10.13- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;And the Shoot has the following spec:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe Shoot refers to a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-12 the Kubernetes version will stay the same as it is still not expired. But in the maintenance window on 2019-04-14 the Kubernetes version of the Shoot will be updated to 1.10.13 (no matter the value of .spec.maintenance.autoUpdate.kubernetesVersion). As long as there is a higher patch version available, the Cluster is always updated to the highest patch version.\n Forceful minor version update from expired Kubernetes version.  Starting with Gardener version 1.4, Shoot clusters can receive forceful minor updates when using an expired Kubernetes version. Minor version updates are only performed if the version is the latest patch version of the minor version, having an expiration date in the past.\nLet\u0026rsquo;s assume the following CloudProfile spec (only related fields are shown):\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.11.09- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;The Shoot has the following spec:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe Shoot refers to a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-14 the Kubernetes version of the Shoot will be updated to 1.11.10. This is the highest patch version of the consecutive minor version.\nKubernetes \u0026ldquo;minor version jumps\u0026rdquo; are not allowed - meaning to skip the update to the consecutive minor version but directly updating to any version after that. In the example above, the version 1.10.x can only update to a version 1.11.x, not to 1.12.x or any other version. This is because Kubernetes does not guarantee updateability in this case, leading to possibly broken Shoot clusters. The administrator has to set up the CloudProfile in such a way, that consecutive Kubernetes minor versions are available. Otherwise, Shoot clusters will fail to update during the maintenance time.\nPlease note, that multiple consecutive minor version updates are possible. This can occur if the Shoot is updated to a version that in turn is also `expired`. In this case, the version is again updated in the **next** maintenance time.\n Force Version Updates for Operating System Versions In the same fashion as automatic version updates, force Operating System updates are applied per Worker pool and updates to the Operating System are always performed to the latest version available in the CloudProfile.\n Forceful update from expired operating system version.  Let\u0026rsquo;s assume the following CloudProfile spec (only related fields are shown):\nspec:machineImages:- name:coreosversions:- version:2191.5.0- version:2191.4.1- version:2135.6.0expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;The Shoot has the following spec:\nspec:provider:type:awsworkers:- name:namemaximum:1minimum:1maxSurge:1maxUnavailable:0image:name:coreosversion:2135.6.0type:m5.largevolume:type:gp2size:20Gimaintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:machineImageVersion:falseThe Shoot refers to an operating system version that has an expirationDate. In the maintenance window on 2019-04-12 the machine image version will stay the same as it is still not expired. But in the maintenance window on 2019-04-14 the machine image version of the Shoot will be updated to 2191.5.0 (no matter the value of .spec.maintenance.autoUpdate.machineImageVersion) as version 2135.6.0 is already expired.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/commit_secret_fail/","title":"Storing secrets in git 💀","tags":[],"description":"Never ever commit a kubeconfig.yaml into github","content":"Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository\u0026rsquo;s history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository\u0026rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository\u0026rsquo;s history  Warning: If you run git filter-branch after stashing changes, you won\u0026rsquo;t be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you\u0026rsquo;ve made. To unstash the last set of changes you\u0026rsquo;ve stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we\u0026rsquo;ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository\u0026rsquo;s working directory.\ncd YOUR-REPOSITORY Run the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will:\n Force Git to process, but not check out, the entire history of every branch and tag Remove the specified file, as well as any empty commits generated as a result Overwrite your existing tags  git filter-branch --force --index-filter \\ \u0026#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\u0026#39; \\ --prune-empty --tag-name-filter cat -- --all Add your file with sensitive data to .gitignore to ensure that you don\u0026rsquo;t accidentally commit it again.\necho \u0026#34;YOUR-FILE-WITH-SENSITIVE-DATA\u0026#34; \u0026gt;\u0026gt; .gitignore Double-check that you\u0026rsquo;ve removed everything you wanted to from your repository\u0026rsquo;s history, and that all of your branches are checked out.\nOnce you\u0026rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you\u0026rsquo;ve pushed up:\ngit push origin --force --all In order to remove the sensitive file from your tagged releases, you\u0026rsquo;ll also need to force-push against your Git tags:\ngit push origin --force --tags  Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n References:\n https://help.github.com/articles/removing-sensitive-data-from-a-repository/   blockquote { border:1px solid red; padding:10px; margin-top:40px; margin-bottom:40px; } blockquote p { font-size: 1.5rem; color: black; }  "},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/20_style/","title":"Style Guide","tags":[],"description":"","content":"This page gives writing style guidelines for the Gardener documentation. These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\nLanguage Gardener documentation uses US English.\nDocumentation formatting standards Use camel case for API objects When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name. Typically, the names of API objects use camel case.\nDon\u0026rsquo;t split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying \u0026ldquo;object,\u0026rdquo; unless omitting \u0026ldquo;object\u0026rdquo; leads to an awkward construction.\n DoDon't The Pod has two containers.The pod has two containers. The Deployment is responsible for ...The Deployment object is responsible for ... A PodList is a list of Pods.A Pod List is a list of pods. The two ContainerPorts ...The two ContainerPort objects ... The two ContainerStateTerminated objects ...The two ContainerStateTerminateds ...  Use angle brackets for placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents.\n  Display information about a pod:\nkubectl describe pod \u0026lt;pod-name\u0026gt;  where \u0026lt;pod-name\u0026gt; is the name of one of your pods.\n  Use bold for user interface elements  DoDon't Click Fork.Click \"Fork\". Select Other.Select 'Other'.  Use italics to define or introduce new terms  DoDon't A cluster is a set of nodes ...A \"cluster\" is a set of nodes ... These components form the control plane.These components form the control plane.  Use code style for filenames, directories, and paths  DoDon't Open the envars.yaml file.Open the envars.yaml file. Go to the /docs/tutorials directory.Go to the /docs/tutorials directory. Open the /_data/concepts.yaml file.Open the /_data/concepts.yaml file.  Use the international standard for punctuation inside quotes  DoDon't events are recorded with an associated \"stage\".events are recorded with an associated \"stage.\" The copy is called a \"fork\".The copy is called a \"fork.\"  Inline code formatting Use code style for inline code and commands For inline code in an HTML document, use the \u0026lt;code\u0026gt; tag. In a Markdown document, use the backtick (`).\n DoDon't The kubectl run command creates a Deployment.The \"kubectl run\" command creates a Deployment. For declarative management, use kubectl apply.For declarative management, use \"kubectl apply\".  Use code style for object field names  DoDon't Set the value of the replicas field in the configuration file.Set the value of the \"replicas\" field in the configuration file. The value of the exec field is an ExecAction object.The value of the \"exec\" field is an ExecAction object.  Use normal style for string and integer field values For field values of type string or integer, use normal style without quotation marks.\n DoDon't Set the value of imagePullPolicy to Always.Set the value of imagePullPolicy to \"Always\". Set the value of image to nginx:1.8.Set the value of image to nginx:1.8. Set the value of the replicas field to 2.Set the value of the replicas field to 2.  Code snippet formatting Don\u0026rsquo;t include the command prompt  DoDon't kubectl get pods$ kubectl get pods  Separate commands from output Verify that the pod is running on your chosen node:\nkubectl get pods --output=wide  The output is similar to this:\nNAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0  Versioning Kubernetes examples Code examples and configuration examples that include version information should be consistent with the accompanying text. Identify the Kubernetes version in the Before you begin section.\nTo specify the Kubernetes version for a task or tutorial page, include min-kubernetes-server-version in the front matter of the page.\nIf the example YAML is in a standalone file, find and review the topics that include it as a reference. Verify that any topics using the standalone YAML have the appropriate version information defined. If a stand-alone YAML file is not referenced from any topics, consider deleting it instead of updating it.\nFor example, if you are writing a tutorial that is relevant to Kubernetes version 1.8, the front-matter of your markdown file should look something like:\n---title:\u0026lt;yourtutorialtitlehere\u0026gt;min-kubernetes-server-version:v1.8---In code and configuration examples, do not include comments about alternative versions. Be careful to not include incorrect statements in your examples as comments, such as:\napiVersion:v1# earlier versions use...kind:Pod..."},{"uri":"https://gardener.cloud/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/tail-logfile/","title":"tail -f /var/log/my-application.log","tags":[],"description":"Aggregate log files from different pods","content":"Problem One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don\u0026rsquo;t have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/trigger-shoot-operations/","title":"Trigger Shoot operations","tags":[],"description":"","content":"Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile Immediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain Retry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry Rotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials "},{"uri":"https://gardener.cloud/tutorials/","title":"Tutorials","tags":[],"description":"","content":"Initial Consideration There is a big difference between installing Kubernetes and using Kubernetes as a developer     Administrator The admin section is for anyone setting up or administering a Gardener Landscape. It assumes some familiarity with concepts of IaaS   Developer You don’t have to understand all the internals of Kubernetes; however, basic knowledge of the architecture is helpful for understanding how to deploy and debug your applications. In this section we offer best practices for service and application development on Kubernetes in the context of Gardener.      "},{"uri":"https://gardener.cloud/documentation/guides/client_tools/helm/","title":"Use a Helm chart to deploy some application or service","tags":[],"description":"","content":"Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default hence Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: helm namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm namespace: kube-system EOF Initialize Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \u0026quot;system:serviceaccount:kube-system:default\u0026quot; cannot list configmaps in the namespace \u0026quot;kube-system\u0026quot;. (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system kubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/ Now follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n"},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/user_alerts/","title":"User Alerts","tags":[],"description":"","content":"User Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiServerLatency warning seed Kube API server latency for verb {{ $labels.verb }} is high. This could be because the shoot workers and the control plane are in different regions. 99th percentile of request latency is greater than 3 second.   KubeApiServerTooManyOpenFileDescriptors warning seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerTooManyOpenFileDescriptors critical seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeControllerManagerDown critical seed Deployments and replication controllers are not making progress.   KubeEtcd3DbSizeLimitApproaching warning seed Etcd3 {{ $labels.role }} DB size is approaching its current practical limit of 2GB.   KubeEtcd3DbSizeLimitCrossed critical seed Etcd3 {{ $labels.role }} DB size has crossed its current practical limit of 2GB. Etcd might now require more memory to continue serving traffic with low latency, and might face request throttling.   KubeKubeletNodeDown warning shoot The kubelet {{ $labels.instance }} has been unavailable/unreachable for more than 1 hour. Workloads on the affected node may not be schedulable.   KubeKubeletTooManyPods warning  Kubelet {{ $labels.instance }} is running {{ $value }} pods, close to the limit of 110   KubeletTooManyOpenFileDescriptorsShoot warning shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubeletTooManyOpenFileDescriptorsShoot critical shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePodPendingShoot warning shoot Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 1 hour.   KubePodNotReadyShoot warning shoot Pod {{ $labels.pod }} is not ready for more than 1 hour.   KubeSchedulerDown critical seed New pods are not being assigned to nodes.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   NodeExporterDown warning shoot The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.   K8SNodeOutOfDisk critical shoot Node {{ $labels.node }} has run out of disk space.   K8SNodeMemoryPressure warning shoot Node {{ $labels.node }} is under memory pressure.   K8SNodeDiskPressure warning shoot Node {{ $labels.node }} is under disk pressure   VMRootfsFull critical shoot Root filesystem device on instance {{ $labels.instance }} is almost full.   VMConntrackTableFull critical shoot The nf_conntrack table is {{ $value }}% full.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/documentation/guides/applications/prometheus/","title":"Using Prometheus and Grafana to monitor K8s","tags":[],"description":"How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics","content":"Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments. Such advanced details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.\nPrometheus graduates within CNCF second hosted project.\nThe following characteristics make Prometheus a good match for monitoring Kubernetes clusters:\n  Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.\n  Labels Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.\nLabels are used to identify time series and sets of label matchers can be used in the query language ( PromQL ) to select the time series to be aggregated..\n  Exporters\nThere are many exporters available which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n  Powerful query language\nThe Prometheus query language PromQL lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the HTTP API.\n  Find query examples on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is a metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses data via Data Sources. The continuously growing list of supported backends includes Prometheus.\nDashboards are created by combining panels, e.g. Graph and Dashlist.\nIn this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.\nIf you miss elements on the Prometheus web page when accessing it via its service URL https://\u0026lt;your K8s FQN\u0026gt;/api/v1/namespaces/\u0026lt;your-prometheus-namespace\u0026gt;/services/prometheus-prometheus-server:80/proxy this is probably caused by Prometheus issue #1583 To workaround this issue setup a port forward kubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;prometheus-pod\u0026gt; 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana is based on Helm charts.\nMake sure to implement the Helm settings before deploying the Helm charts.\nThe Kubernetes clusters provided by Gardener use role based access control (RBAC). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster\u0026rsquo;s worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the garden.sapcloud.io:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\nContent of crbinding.yaml\napiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:name:\u0026lt;your-prometheus-name\u0026gt;-serverroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:garden.sapcloud.io:monitoring:prometheussubjects:- kind:ServiceAccountname:\u0026lt;your-prometheus-name\u0026gt;-servernamespace:\u0026lt;your-prometheus-namespace\u0026gt;Deployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nCopy the following configuration into a file called values.yaml and deploy Prometheus: helm install \u0026lt;your-prometheus-name\u0026gt; --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nContent of values.yaml for Prometheus:\nrbac:create:false# Already created in Preparation stepnodeExporter:enabled:false# The node-exporter is already deployed by defaultserver:global:scrape_interval:30sscrape_timeout:30sserverFiles:prometheus.yml:rule_files:- /etc/config/rules- /etc/config/alertsscrape_configs:- job_name:\u0026#39;kube-kubelet\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)- job_name:\u0026#39;kube-kubelet-cadvisor\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics/cadvisor- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)# Example scrape config for probing services via the Blackbox Exporter.## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/probe`: Only probe services that have a value of `true`- job_name:\u0026#39;kubernetes-services\u0026#39;metrics_path:/probeparams:module:[http_2xx]kubernetes_sd_configs:- role:servicerelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__address__]target_label:__param_target- target_label:__address__replacement:blackbox- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]target_label:kubernetes_name# Example scrape config for pods## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/scrape`: Only scrape pods that have a value of `true`# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.- job_name:\u0026#39;kubernetes-pods\u0026#39;kubernetes_sd_configs:- role:podrelabel_configs:- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_pod_annotation_prometheus_io_port]action:replaceregex:(.+):(?:\\d+);(\\d+)replacement:${1}:${2}target_label:__address__- action:labelmapregex:__meta_kubernetes_pod_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_pod_name]action:replacetarget_label:kubernetes_pod_name# Scrape config for service endpoints.## The relabeling allows the actual service scrape endpoint to be configured# via the following annotations:## * `prometheus.io/scrape`: Only scrape services that have a value of `true`# * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need# to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config.# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: If the metrics are exposed on a different port to the# service then set this appropriately.- job_name:\u0026#39;kubernetes-service-endpoints\u0026#39;kubernetes_sd_configs:- role:endpointsrelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scheme]action:replacetarget_label:__scheme__regex:(https?)- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_service_annotation_prometheus_io_port]action:replacetarget_label:__address__regex:(.+)(?::\\d+);(\\d+)replacement:$1:$2- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]action:replacetarget_label:kubernetes_name# Add your additional configuration here...Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. Deploy Grafana via helm install grafana --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nContent of values.yaml for Grafana:\nserver:ingress:enabled:falseservice:type:ClusterIPCheck the running state of the pods on the Kubernetes Dashboard or by running kubectl get pods -n \u0026lt;your-prometheus-namespace\u0026gt;. In case of errors check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u0026lt;your-prometheus-namespace\u0026gt; and could be decoded via kubectl get secret --namespace \u0026lt;my-grafana-namespace\u0026gt; grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo.\nBasic functional tests To access the web UI of both applications use port forwarding of port 9090.\nSetup port forwarding for port 9090:\nkubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;your-prometheus-server-pod\u0026gt; 9090:9090 Open http://localhost:9090 in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode=\u0026#39;idle\u0026#39;}[5m]))) This should show some data in a graph.\nTo show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser. Enter the credentials of the admin user.\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.\nRun\nhelm status \u0026lt;your-prometheus-name\u0026gt; to find this name. Below this server name is referenced by \u0026lt;your-prometheus-server-name\u0026gt;.\nFirst, you need to add your Prometheus server as data source.\n select Dashboards → Data Sources select Add data source enter Name: \u0026lt;your-prometheus-datasource-name\u0026gt;\nType: Prometheus\nURL: http://\u0026lt;your-prometheus-server-name\u0026gt;\n_Access: proxy select Save \u0026amp; Test  In case of failure check the Prometheus URL in the Kubernetes Dashboard.\nTo add a Graph follow these steps:\n in the left corner, select Dashboards → New to create a new dashboard select Graph to create a new graph next, select the Panel Title → Edit select your Prometheus Data Source in the drop down list enter the expression 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A select the floppy disk symbol (Save) on top  Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.\nAs a next step you can implement monitoring for your applications by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  "}]