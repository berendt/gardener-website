[{"uri":"https://gardener.cloud/blog/","title":"Blogs","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/","title":"Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/blog/2020-09/","title":"Gardener v1.9 and v1.10 Released","tags":[],"description":"","content":"Summer holidays aren\u0026rsquo;t over yet, still, the Gardener community was able to release two new minor versions in the past weeks. Despite being limited in capacity these days, we were able to reach some major milestones, like adding Kubernetes v1.19 support and the long-delayed automated gardenlet certificate rotation. Whilst we continue to work on topics related to scalability, robustness, and better observability, we agreed to adjust our focus a little more into the areas of development productivity, code quality and unit/integration testing for the upcoming releases.\nNotable Changes in v1.10 Gardener v1.10 was a comparatively small release (measured by the number of changes) but it comes with some major features!\nKubernetes 1.19 support (gardener/gardener#2799) The newest minor release of Kubernetes is now supported by Gardener (and all the maintained provider extensions)! Predominantly, we have enabled CSI migration for OpenStack now that it got promoted to beta, i.e. 1.19 shoots will no longer use the in-tree Cinder volume provisioner. The CSI migration enablement for Azure got postponed (to at least 1.20) due to some issues that the Kubernetes community is trying to fix in the 1.20 release cycle. As usual, the 1.19 release notes should be considered before upgrading your shoot clusters.\nAutomated certificate rotation for gardenlet (gardener/gardener#2542) Similar to the kubelet, the gardenlet supports TLS bootstrapping when deployed into a new seed cluster. It will request a client certificate for the garden cluster using the CertificateSigningRequest API of Kubernetes and store the generated results in a Secret object in the garden namespace of its seed. These certificates are usually valid for one year. We have now added support for automatic renewals if the expiration dates are approaching.\nImproved monitoring alerts (gardener/gardener#2776) We have worked on a larger refactoring to improve reliability and accuracy of our monitoring alerts for both shoot control planes in the seed as well as shoot system components running on worker nodes. The improvements are primarily for operators and should result in less false positive alerts. Also, the alerts should fire less frequently and are better grouped in order to reduce to overall amount of alerts.\nSeed deletion protection (gardener/gardener#2732) Our validation to improve robustness and countermeasures against accidental mistakes has been improved. Earlier, it was possible to remove the use-as-seed annotation for shooted seeds or directly set the deletionTimestamp on Seed objects, despite of the fact that they might still run shoot control planes. Seed deletion would not start in these cases, although, it would disrupt the system unnecessarily, and result in some unexpected behaviour. The Gardener API server is now forbidding such requests if the seeds are not completely empty yet.\nLogging improvements for Loki (multiple PRs) After we released our large logging stack refactoring (from EFK to Loki) with Gardener v1.8, we have continued to work on reliability, quality and user feedback in general. We aren\u0026rsquo;t done yet, though, Gardener v1.10 includes a bunch of improvements which will help to graduate the Logging feature gate to beta and GA, eventually.\nNotable Changes in v1.9 The v1.9 release contained tons of small improvements and adjustments in various areas of the code base and a little less new major features. However, we don\u0026rsquo;t want to miss the opportunity to highlight a few of them.\nCRI validation in CloudProfiles (gardener/gardener#2137) A couple of releases back we have introduced support for containerd and the ContainerRuntime extension API. The supported container runtimes are operating system specific, and until now it wasn\u0026rsquo;t possible for end-users to easily figure out whether they can enable containerd or other ContainerRuntime extensions for their shoots. With this change, Gardener administrators/operators can now provide that information in the .spec.machineImages section in the CloudProfile resource. This also allows for enhanced validation and prevents misconfigurations.\nNew shoot event controller (gardener/gardener#2649) The shoot controllers in both the gardener-controller-manager and gardenlet fire several Events for some important operations (e.g., automated hibernation/wake-up due to hibernation schedule, automated Kubernetes/machine image version update during maintenance, etc.). Earlier, the only way to prolong the lifetime of these events was to modify the --event-ttl command line parameter of the garden cluster\u0026rsquo;s kube-apiserver. This came with the disadvantage that all events were kept for a longer time (not only those related to Shoots that an operator is usually interested in and ideally wants to store for a couple of days). The new shoot event controller allows to achieve this by deleting non-shoot events. This helps operators and end-users to better understand which changes were applied to their shoots by Gardener.\nEarly deployment of the logging stack for new shoots (gardener/gardener#2750) Since the first introduction of the Logging feature gate two years back the logging stack was only deployed at the very end of the shoot creation. This had the disadvantage that control plane pod logs were not kept in case the shoot creation flow is interrupted before the logging stack could be deployed. In some situations, this was preventing fetching relevant information about why a certain control plane component crashed. We now deploy the logging stack very early in the shoot creation flow to always have access to such information.\n"},{"uri":"https://gardener.cloud/blog/2020-08/00/","title":"Gardener v1.8.0 Released","tags":[],"description":"","content":"Even if we are in the midst of the summer holidays, a new Gardener release came out yesterday: v1.8.0! It\u0026rsquo;s main themes are the large change of our logging stack to Loki (which was already explained in detail on a blog post on grafana.com), more configuration options to optimize the utilization of a shoot, node-local DNS, new project roles, and significant improvements for the Kubernetes client that Gardener uses to interact with the many different clusters.\nNotable Changes Logging 2.0: EFK stack replaced by Loki (gardener/gardener#2515) Since two years or so Gardener could optionally provision a dedicated logging stack per seed and per shoot which was based on fluent-bit, fluentd, ElasticSearch and Kibana. This feature was still hidden behind an alpha-level feature gate and never got promoted to beta so far. Due to various limitations of this solution we decided to replace the EFK stack with Loki. As we already have Prometheus and Grafana deployments for both users and operators by default for all clusters the choice was just natural. Please find out more on this topic at this dedicated blog post.\nCluster identities and DNSOwner objects (gardener/gardener#2471, gardener/gardener#2576) The shoot control plane migration topic is ongoing since a few months already, and we are very much progressing with it. A first alpha version will probably make it out soon. As part of these endeavors, we introduced cluster identities and the usage of DNSOwner objects in this release. Both are needed to gracefully migrate the DNSEntry extension objects from the old seed to the new seed as part of the control plane migration process. Please find out more on this topic at this blog post.\nNew uam role for Project members to limit user access management privileges (gardener/gardener#2611) In order to allow external user access management system to integrate with Gardener and to fulfil certain compliance aspects, we have introduced a new role called uam for Project members (next to admin and viewer). Only if a user has this role then he/she is allowed to add/remove other human users to the respective Project. By default, all newly created Projects assign this role only to the owner while, for backwards-compatibility reasons, it will be assigned for all members for existing projects. Project owners can steadily revoke this access as desired. Interestingly, the uam role is backed by a custom RBAC verb called manage-members, i.e., the Gardener API server is only admitting changes to the human Project members if the respective user is bound to this RBAC verb.\nNew node-local DNS feature for shoots (gardener/gardener#2528) By default, we are using CoreDNS as DNS plugin in shoot clusters which we auto-scale horizontally using HPA. However, in some situations we are discovering certain bottlenecks with it, e.g., unreliable UDP connections, unnecessary node hopping, inefficient load balancing, etc. To further optimize the DNS performance for shoot clusters, it is now possible to enable a new alpha-level feature gate in the gardenlet\u0026rsquo;s componentconfig: NodeLocalDNS. If enabled, all shoots will get a new DaemonSet to run a DNS server on each node.\nMore kubelet and API server configurability (gardener/gardener#2574, gardener/gardener#2668) One large benefit of Gardener is that it allows you to optimize the usage of your control plane as well as worker nodes by exposing relevant configuration parameters in the Shoot API. In this version, we are adding support to configure kubelet\u0026rsquo;s values for systemReserved and kubeReserved resources as well as the kube-apiserver\u0026rsquo;s watch cache sizes. This allows end-users to get to better node utilization and/or performance for their shoot clusters.\nConfigurable timeout settings for machine-controller-manager (gardener/gardener#2563) One very central component in Project Gardener is the machine-controller-manager for managing the worker nodes of shoot clusters. It has extensive qualities with respect to node lifecycle management and rolling updates. As such, it uses certain timeout values, e.g. when creating or draining nodes, or when checking their health. Earlier, those were not customizable by end-users, but we are adding this possibility now. You can fine-grain these settings per worker pool in the Shoot API such that you can optimize the lifecycle management of your worker nodes even more!\nImproved usage of cached client to reduce network I/O (gardener/gardener#2635, gardener/gardener#2637) In the last Gardener release v1.7 we have introduced a huge refactoring the clients that we use to interact with the many different Kubernetes clusters. This is to further optimize the network I/O performed by leveraging watches and caches as good as possible. It\u0026rsquo;s still an alpha-level feature that must be explicitly enabled in the Gardenlet\u0026rsquo;s component configuration, though, with this release we have improved certain things in order to pave the way for beta promotion. For example, we were initially also using a cached client when interacting with shoots. However, as the gardenlet runs in the seed as well (and thus can communicate cluster-internally with the kube-apiservers of the respective shoots) this cache is not necessary and just memory overhead. We have removed it again and saw the memory usage getting lower again. More to come!\nAWS EBS volume encryption by default (gardener/gardener-extension-provider-aws#147) The Shoot API already exposed the possibility to encrypt the root disks of worker nodes since quite a while, but it was disabled by default (for backwards-compatibility reasons). With this release we have change this default, so new shoot worker nodes will be provisioned with encrypted root disks out-of-the-box. However, the g4dn instance types of AWS don\u0026rsquo;t support this encryption, so when you use them you have to explicitly disable the encryption in the worker pool configuration.\nLiveness probe for Gardener API server deployment (gardener/gardener#2647) A small, but very valuable improvement is the introduction of a liveness probe for our Gardener API server. As it\u0026rsquo;s built with the same library like the Kubernetes API server, it exposes two endpoints at /livez and /readyz which were created exactly for the purpose of live- and readiness probes. With Gardener v1.8 the Helm chart contains a liveness probe configuration by default, and we are awaiting an upstream fix (kubernetes/kubernetes#93599) to also enable the readiness probe. This will help in a smoother rolling update of the Gardener API server pods, i.e., preventing clients from talking to a not yet initialized or already terminating API server instance.\nWebhook ports changed to enable OpenShift (gardener/gardener#2660) In order to make it possible to run Gardener on OpenShift clusters as well, we had to make a change in the port configuration for the webhooks we are using in both Gardener and the extension controllers. Earlier, all the webhook servers directly exposed port 443, i.e., a system port which is a security concern and disallowed in OpenShift. We have changed this port now across all places and also adapted our network policies accordingly. This is most likely not the last necessary change to enable this scenario, however, it\u0026rsquo;s a great improvement to push the project forward.\nIf you\u0026rsquo;re interested in more details and even more improvements you can find all release notes for Gardener v1.8.0 here: https://github.com/gardener/gardener/releases/tag/v1.8.0\n"},{"uri":"https://gardener.cloud/news/005/00/","title":"MCM, Tailscale and K3S","tags":[],"description":"","content":"MCM, Tailscale and K3S Using Gardener Machine-Controller-Manager and Tailscale to extend a local Raspberry-Pi K3s cluster with cloud instances. Read this blog by Christoph Voigt, where he takes on head-to-head the problem of dealing with machines in the cloud.\n Okay, so I can make any cloud instance part of my local cluster. But this still requires a bit of setup at the cloud vendor of your choice: at a minimum, you need to create a basic network infrastructure (VPC, subnets), create a VM (decide on an OS, setup Tailscale \u0026amp; K3s, assign Firewalls / Security Groups), oh, and please make sure it is running with a secure Runtime profile!\n He explores the options and then settles on Gardener\u0026rsquo;s Machine-Controller-Manager as the most Kubernetes-native aproach out there. In this experiment, only MCM and not the rest of Gardener is used. And that is quite possible because of the Kubernetes and Gardener\u0026rsquo;s idiomatic loosely coupled, modular designs.\nTo explore how MCM was integrated and more, head on to the original blog at the blog.nativecloud.dev.\n"},{"uri":"https://gardener.cloud/news/","title":"News","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/news/004/00/","title":"What&#39;s Gardener Project to SAP?","tags":[],"description":"","content":"What\u0026rsquo;s Gardener Project to SAP? An New Article on about SAP, Gardener and OSS. Recently, Thomas Hertz, CNCF board member and head of Development Experience at SAP, published an overview article at the TheNewStack where he discusses projct Gardener and its place in SAP and the open source community, and what\u0026rsquo;s the future ahead of it.\n\u0026ldquo;Gardener is an SAP-driven open source project that tackles real-world demands for hyperscale Kubernetes services, regardless of infrastructure.\u0026quot;, Thomas says disucssing the benefits from Gardener, yielding analogy with Borg, born to address real-world problems and eventually becomming Kubernetes. Further stressing upon the project\u0026rsquo;s Kubernetes DNA he outlines the unique, vendor-neutral approach of Gardener to offer a lock-in free solution and lists a number of SAP solutions that already benefit from that far and wide.\n Gardener provides a neutral toolbox for the technology stack of today, and we designed it to be sufficiently extensible so that — with relatively low effort — it can additionally adapt for the tools and infrastructures that come next. No one can say which direction the Kubernetes ecosystem will take, but Gardener is designed to keep things open and flexible.\n \u0026ldquo;We are determined to be transparent with Gardener, by developing everything in the public space and then adopting it with minor SAP specific integrations in-house. It’s always been imperative to keep it vendor-neutral and to stick to upstream Kubernetes practices, design and processes.\u0026quot; Thomas then elaborates on SAP\u0026rsquo;s comitment to transparently drive the project in vendor-neutral manner and in a trully transparent for the community community and collaborative manner.\nHe then discusses various use cases of community members using Gardener in differnet domains and concludes with the benefits of the open source project for SAP and how it catalizes a cultural change\n Within SAP, Gardener is influencing and catalyzing change. We already have great inner sourcing examples, with internal stakeholders contributing directly to the open source software project and doing almost everything in the public.\n \u0026ldquo;We developed Gardener to provide our customers with a single, consistent Kubernetes feature set that abstracts resources and underlying infrastructure, and can be used by SAP solutions anywhere. We are seeing an uptake by those developing applications and deploying them across multiple clouds, and look forward to working with the community to extend Gardener and deliver hyperscale Kubernetes services for the tools and infrastructures of the future.\u0026quot; Thomas says at the end of the article, inviting contributors and adopters to the growing Gardener community for a collaboraiton to meet the challenges today and in future.\nRead more about that in the original article at the .\n"},{"uri":"https://gardener.cloud/blog/2020-05/00/","title":"PingCAP’s Experience in Implementing their Managed TiDB Service with Gardener","tags":[],"description":"","content":"Gardener is showing successful collaboration with its growing community of contributors and adopters. With this come some success stories, including PingCAP using Gardener to implement its managed service.\nAbout PingCAP and its TiDB Cloud PingCAP started in 2015, when three seasoned infrastructure engineers working at leading Internet companies got sick and tired of the way databases were managed, scaled and maintained. Seeing no good solution on the market, they decided to build their own - the open-source way. With the help of a first-class team and hundreds of contributors from around the globe, PingCAP is building a distributed NewSQL, hybrid transactional and analytical processing (HTAP) database.\nIts flagship project, TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project.\nPingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers. As a result, the company turned to Gardener to build their managed TiDB cloud service offering.\nTiDB Cloud Beta Preview Limitations with other public managed Kubernetes services Previously, PingCAP encountered issues while using other public managed K8s cluster services, to develop the first version of its TiDB Cloud. Their worst pain point was that they felt helpless when encountering certain malfunctions. PingCAP wasn’t able to do much to resolve these issues, except waiting for the providers’ help. More specifically, they experienced problems due to cloud-provider specific Kubernetes system upgrades, delays in the support response (which could be avoided in exchange of a costly support fee), and no control over when things got fixed.\nThere was also a lot of cloud-specific integration work needed to follow a multi-cloud strategy, which proved to be expensive both to produce and maintain. With one of these managed K8s services, you would have to integrate the instance API, as opposed to a solution like Gardener, which provides a unified API for all clouds. Such a unified API eliminates the need to worry about cloud specific-integration work altogether.\nWhy PingCAP chose Gardener to build TiDB Cloud  “Gardener has similar concepts to Kubernetes. Each Kubernetes cluster is just like a Kubernetes pod, so the similar concepts apply, and the controller pattern makes Gardener easy to manage. It was also easy to extend, as the team was already very familiar with Kubernetes, so it wasn’t hard for us to extend Gardener. We also saw that Gardener has a very active community, which is always a plus!”- Aylei Wu, (Cloud Engineer) at PingCAP\n At first glance, PingCAP had initial reservations about using Gardener - mainly due to its adoption level (still at the beginning) and an apparent complexity of use. However, these were soon eliminated as they learned more about the solution. As Aylei Wu mentioned during the last Gardener community meeting, “a good product speaks for itself”, and once the company got familiar with Gardener, they quickly noticed that the concepts were very similar to Kubernetes, which they were already familiar with.\nThey recognized that Gardener would be their best option, as it is highly extensible and provides a unified abstraction API layer. In essence, the machines can be managed via a machine controller manager for different cloud providers - without having to worry about the individual cloud APIs.\nThey agreed that Gardener’s solution, although complex, was definitely worth it. Even though it is a relatively new solution, meaning they didn’t have access to other user testimonials, they decided to go with the service since it checked all the boxes (and as SAP was running it productively with a huge fleet). PingCAP also came to the conclusion that building a managed Kubernetes service themselves would not be easy. Even if they were to build a managed K8s service, they would have to heavily invest in development and would still end up with an even more complex platform than Gardener’s. For all these reasons combined, PingCAP decided to go with Gardener to build its TiDB Cloud.\nHere are certain features of Gardener that PingCAP found appealing:\n Cloud agnostic: Gardener’s abstractions for cloud-specific integrations dramatically reduce the investment in supporting more than one cloud infrastructure. Once the integration with Amazon Web Services was done, moving on to Google Cloud Platform proved to be relatively easy. (At the moment, TiDB Cloud has subscription plans available for both GCP and AWS, and they are planning to support Alibaba Cloud in the future.) Familiar concepts: Gardener is K8s native; its concepts are easily related to core Kubernetes concepts. As such, it was easy to onboard for a K8s experienced team like PingCAP’s SRE team. Easy to manage and extend: Gardener’s API and extensibility are easy to implement, which has a positive impact on the implementation, maintenance costs and time-to-market. Active community: Prompt and quality responses on Slack from the Gardener team tremendously helped to quickly onboard and produce an efficient solution.  How PingCAP built TiDB Cloud with Gardener On a technical level, PingCAP’s set-up overview includes the following:\n A Base Cluster globally, which is the top-level control plane of TiDB Cloud A Seed Cluster per cloud provider per region, which makes up the fundamental data plane of TiDB Cloud A Shoot Cluster is dynamically provisioned per tenant per cloud provider per region when requested A tenant may create one or more TiDB clusters in a Shoot Cluster  As a real world example, PingCAP sets up the Base Cluster and Seed Clusters in advance. When a tenant creates its first TiDB cluster under the us-west-2 region of AWS, a Shoot Cluster will be dynamically provisioned in this region, and will host all the TiDB clusters of this tenant under us-west-2. Nevertheless, if another tenant requests a TiDB cluster in the same region, a new Shoot Cluster will be provisioned. Since different Shoot Clusters are located in different VPCs and can even be hosted under different AWS accounts, TiDB Cloud is able to achieve hard isolation between tenants and meet the critical security requirements for our customers.\nTo automate these processes, PingCAP creates a service in the Base Cluster, known as the TiDB Cloud “Central” service. The Central is responsible for managing shoots and the TiDB clusters in the Shoot Clusters. As shown in the following diagram, user operations go to the Central, being authenticated, authorized, validated, stored and then applied asynchronously in a controller manner. The Central will talk to the Gardener API Server to create and scale Shoot clusters. The Central will also access the Shoot API Service to deploy and reconcile components in the Shoot cluster, including control components (TiDB Operator, API Proxy, Usage Reporter for billing, etc.) and the TiDB clusters.\nTiDB Cloud on Gardener Architecture Overview What’s next for PingCAP and Gardener With the initial success of using the project to build TiDB Cloud, PingCAP is now working heavily on the stability and day-to-day operations of TiDB Cloud on Gardener. This includes writing Infrastructure-as-Code scripts/controllers with it to achieve GitOps, building tools to help diagnose problems across regions and clusters, as well as running chaos tests to identify and eliminate potential risks. After benefiting greatly from the community, PingCAP will continue to contribute back to Gardener.\nIn the future, PingCAP also plans to support more cloud providers like AliCloud and Azure. Moreover, PingCAP may explore the opportunity of running TiDB Cloud in on-premise data centers with the constantly expanding support this project provides. Engineers at PingCAP enjoy the ease of learning from Gardener’s kubernetes-like concepts and being able to apply them everywhere. Gone are the days of heavy integrations with different clouds and worrying about vendor stability. With this project, PingCAP now sees broader opportunities to land TiDB Cloud on various infrastructures to meet the needs of their global user group.\nStay tuned, more blog posts to come on how Gardener is collaborating with its contributors and adopters to bring fully-managed clusters at scale everywhere! If you want to join in on the fun, connect with our community.\n"},{"uri":"https://gardener.cloud/news/002/00/","title":"Gardener Website 2.0","tags":[],"description":"","content":" .news-item .green-power { color: #0f674e; font-weight: 700; } .news-item .green-power img { display: inline-block; width: 32px; vertical-align: middle; margin: 0 5px 0; } @media (min-width: 750px) { .news-item .green-power img { width: 48px; } } .news-item .title-2 { font-weight: 700; background-color: unset!important; color: #222!important; }  New Website, Same Green Power! The Gardener Project just got a brand new website with multiple improvements and we are all very excited!\n Go ahead and explore around now and help us spread the word: https://gardener.cloud Tweet  Read more about changes and plans in the blog post.\n"},{"uri":"https://gardener.cloud/blog/2020_week_20/00/","title":"New Website, Same Green Flower","tags":[],"description":"","content":"The Gardener project website just received a serious facelift. Here are some of the highlights:\n A completely new landing page, emphasizing both on Gardener\u0026rsquo;s value proposition and the open community behind it. The Community page was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the communty. A completely new News section and content type available at /documentation/news. Use metadata such as publishdate and archivedate to schedule for news publish and archive automatically, regardless of when you contributed them. You can now track what\u0026rsquo;s happening from the landing page or in the dedicated News section on the website and share. Improved blogs layout. One-click sharing options are available starting with simple URL copy link and twitter button and others will closely follow up. While we are at it, give it a try. Spread the word.  Website builds also got to a new level with:\n Containerization. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the /documentation repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing make serve in your local /documentation clone. Numerous improvements to the buld scripts. More configuration options, authenticated requests, fault tollerance and performance. Good news for Windows WSL users who will now nejoy a significantly support. See the updated README for details on that. A number of improvements in layouts styles, site assets and hugo site-building techniques.  But hey, THAT\u0026rsquo;S NOT ALL!\nStay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and \u0026hellip; let\u0026rsquo;s cut the spoilers here.\nI hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on Twitter and Slack, or in case of issues - on GitHub.\nGo ahead and help us spread the word: https://gardener.cloud\n  "},{"uri":"https://gardener.cloud/blog/2020_week_test/","title":"New Website, Same Green Flower","tags":[],"description":"","content":"This is a test "},{"uri":"https://gardener.cloud/news/003/release/","title":"Gardener 1.4.0 Released","tags":[],"description":"","content":"Gardener Release 1.4 ​ The Gardener release 1.4 is mainly focused on stability improvments and optimizations. For example, we twitched some configurations (e.g CPU and memory limits), and improved the monitoring and healtchechks. And there are some new features too. The list below is an overview of the most notable changes. Explore the full release notes in GitHub Release 1.4. ​​\nNotable Changes Now you decide when the reconciliation happens if you change your Shoot specification ​ Previously whenever you change a configuration of your Shoot.yaml specification, a reconciliation process was triggered enabling your desired state. Now using .spec.maintenance.confineSpecUpdateRollout=true you confine those changes to be updated in the individual maintenance time window. This is helpful if you want to update your Shoot but don\u0026rsquo;t want the changes to be applied immediately. One example use-case would be a Kubernetes version upgrade that you want to roll out during the maintenance time window. It is important to note that if you change the maintenance window itself, then it will only be effective after the upcoming maintenance. Of course, there is an exception with the .spec.hibernation.enabled field, which changes are taken under consideration immediately. If you hibernate or wake-up your Shoots then Gardener gets active right away. ​\nNew Grafana dashboard to monitor CoreDNS\u0026rsquo;s stats ​ We exposed this Dashboard to give you insight over some CoreDNS metrics of your cluster, like: DNS Requests, DNS Lookups, Cache Hits/Misses. ​\nBetter way of describing Shoot reconciliation errors ​ The Shoot health check controller has been improved to produce error codes (if applicable) to the .status.conditions[].codes that help to categorize observed problems. Also, there are two new error codes: ERR_INFRA_RESOURCES_DEPLETED indicates that the underlying infrastructure does not have enough resources anymore, and ERR_CONFIGURATION_PROBLEM indicates that the user has misconfigured something and should double-check the specification.\nGardener will now block removal of Kubernetes and machine image versions from the CloudProfile which are still in use ​ The Gardener API Server now validates the changes of CloudProfile against Shoots that are using it. And will block removal of in use versions of Kubernetes and machine images from the CloudProfile. This is part of the \u0026ldquo;Gardener Versioning Policy\u0026rdquo; proposal that you can find at GEP-5 ​\nForceful Shoot clusters updates ​ You can specify for which Kubernetes and machine image versions you can forcefully upgrade to newer ones when expired. This is part of the \u0026ldquo;Gardener Versioning Policy\u0026rdquo; proposal that you can find at GEP-5\nGardener supports metadata.generateName as alternative to metadata.name Create unlimited count of worker groups per Shoot Previously it was not possible to create more that 20-25 worker groups, because the number of worker\u0026rsquo;s cloud configs were limited by the size of the Secret. We divided the cloud config to dedicated managed resources to eliminate this limitation.\nCommon library for simple validating webhooks ​ We introduced new common library in the extension package that will help you to develop simple validating or mutating webhooks for different K8s types with different handlers. Please have a look at PR#69 to see an implementation in action.\nImprovements Kubernetes Dashboard addon version v2.0.0 ​ We bumped the already presented addon for Kubernetes Dashboard to v2.0.0\nGardener validates Pod/Service — Service/Pod network intersection between the Shoot and the Seed ​ Previosuly an overlap of these networks resulted in a broken state, so we added such validation to our admission plugin. ​\nThe infrastructure reconciliation for hibernated shoots is now skipped Endpoint managed by 3rd party operators doesn\u0026rsquo;t block the Shoot\u0026rsquo;s hibernation ​ Now, gardener checks if the Endpoints object is reconciled by kube-controller-manager, otherwise it ignores and does not block the hibernation. ​\nFixed a bug in the healthcheck that prevents checks after a Shoot has been woken up from hibernation If you have implemented custom extension controller for your cluster, you can vendor Gardener v1.4.0 to benefit from the fix.\nDetect outdated health check reports ​ The Gardenlet detects outdated health check reports on extension CRDs with a default threshold of 5 minutes in case Gardener extensions stop performing health checks. For backwards-compatibility reasons, the gardenlet does not check for stale extension reports per default. To enable, the field controllers.shootCare.staleExtensionHealthCheckThreshold in the Gardenlet configuration file should be set. ​\nUpdates on Grafana Dashboard ​ Removed the cluster overview dashboard since metrics used in this dashboard were removed. Other dashboards are changed to no longer show data on a \u0026ldquo;Pod level\u0026rdquo; since pod level metrics have a high cardinality and have been mostly removed from the aggregate-prometheus. ​\nETCD Encryption data is persisted in the ShootState ​ We now replicate the ETCD encryption into the ShootState for future restoration purposes. ​\nImproved Shoot operations ​ We fixed a race condition that led to incomplete maintenance operations for shoot clusters and fixed a bug that prevented the Shoot reconciliation to wait for the deletion of Extension CRDs. ​\nExtension controllers now support Migrate and Restore operations ​ The Actuator interface for the Infrastructure, ControlPlane, Network, Worker, OperationSystemConfig, BackupEntry extension controllers were extended to support migrate and restore operations. ​\nAutoscaler has configurable delay for Pod age before considering scaling-up​ HVPA now properly scales ETCD containers Enhnaced etcd-backup-restore API for delta and full snapshots ​ Triggering full or delta snapshots now returns metadata for the snapshot taken in the response body. Also, new endpoint was introduces for fetching details of the latest full and delta snapshots.\n"},{"uri":"https://gardener.cloud/news/001/release/","title":"Gardener 1.3.0 Released","tags":[],"description":"","content":"Gardener 1.3.0 Released See what\u0026rsquo;s new and noteworthy presented by the team on the regular community meeting recording.\nBookmark the agenda for upcomming community meetings and stay up to date with what\u0026rsquo;s going on in the project.\nFor a detailed list of changes refer to the release notes.\nEnjoy!\n"},{"uri":"https://gardener.cloud/blog/2019_week_21/","title":"Cluster Overprovisioning","tags":[],"description":"","content":"This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work load that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n..read some more on Cluster Overprovisioning.\n"},{"uri":"https://gardener.cloud/blog/2019_week_21_2/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"","content":"Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nPossible Use Cases\n turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  ..read some more on Feature Flags for App.\n"},{"uri":"https://gardener.cloud/blog/2019_week_06/","title":"Manually adding a node to an existing cluster","tags":[],"description":"","content":"Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\n This tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\n..read some more on Adding Nodes to a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2019_week_02/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n  What happens if your kubeconfig file of your production cluster is leaked or published by accident?\n Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if it is has leaked - delete the cluster.\n..learn more on Work with kubeconfig files.\n"},{"uri":"https://gardener.cloud/blog/2018_week_40/","title":"Hibernate a Cluster to save money","tags":[],"description":"","content":"You want to experiment with Kubernetes or have set up a customer scenario, but you don\u0026rsquo;t want to run the cluster 24 / 7 for reasons of cost?\n The Gardener gives you the possibility to scale your cluster down to zero nodes.\n..read some more on Hibernate a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2018_week_22/","title":"Anti Patterns","tags":[],"description":"","content":" Running as root user Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user.\nStoring data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n"},{"uri":"https://gardener.cloud/blog/2018_week_46/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"","content":"In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\n Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\n..read some more on Auditing Kubernetes for Secure Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_07/","title":"Big things come in small packages","tags":[],"description":"","content":"Microservices tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - size of the technology stack.\nGeneral purpose technology stack There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight technology stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\nAdditionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize and deploy a microservice without impacting other subsystems.\n"},{"uri":"https://gardener.cloud/blog/2018_week_51/","title":"Cookies are dangerous...","tags":[],"description":"","content":"\u0026hellip;they mess up the figure.\nFor a team event during the Christmas season we decided to completely reinterpret the topic cookies\u0026hellip; since the vegetables have gone on a well-deserved vacation. :-)\nGet recipe on Gardener Cookies.\n"},{"uri":"https://gardener.cloud/blog/2018_week_17/","title":"Frontend HTTPS","tags":[],"description":"","content":" For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nCreate a secret in the namespace of the ingress containing the TLS private key and certificate. Then configure the secret name in the TLS configuration section of the ingress specification.\n..read on HTTPS - Self Signed Certificates how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_50/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"The Gardener project team has analyzed the impact of the Gardener CVE-2018-2475 and the Kubernetes CVE-2018-1002105 on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.\nRead more on Hardening the Gardener Community Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_06/","title":"Kubernetes is available in Docker for Mac 17.12 CE","tags":[],"description":"","content":"    Kubernetes is only available in Docker for Mac 17.12 CE and higher on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see general configuration.     Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes. The Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n\u0026hellip;see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n"},{"uri":"https://gardener.cloud/blog/2018_week_09/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may chose to configure Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another  ..read on Namespace Isolation how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_08_2/","title":"Namespace Scope","tags":[],"description":"","content":"Should I use:\n❌ one namespace per user/developer? ❌ one namespace per team? ❌ one per service type? ❌ one namespace per application type? 😄 one namespace per running instance of your application?  Apply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don\u0026rsquo;t provide:\n Network isolation Access Control Audit Logging on user level  "},{"uri":"https://gardener.cloud/blog/2018_week_27/","title":"ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS","tags":[],"description":"","content":"The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  When you have application running on multiple nodes which require shared access to a file system When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS supports encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about vendor lock-in and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (roughly twice the price of EBS storage) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don’t use EFS, use it for the files which can\u0026rsquo;t be stored in a CDN. Don’t use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.  ..read some more on ReadWriteMany.\n"},{"uri":"https://gardener.cloud/blog/2018_week_10/","title":"Shared storage with S3 backend","tags":[],"description":"","content":"The storage is definitely the most complex and important part of an application setup, once this part is completed, one of the most problematic parts could be solved.\nMounting a S3 bucket into a pod using FUSE allows to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_08/","title":"Watching logs of several pods","tags":[],"description":"","content":"One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment and you don\u0026rsquo;t have setup a log viewer service like Kibana.\nkubetail comes to the rescue, it is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n"},{"uri":"https://gardener.cloud/installer/","title":"","tags":[],"description":"","content":"gardener-installerWe're sorry but gardener-installer doesn't work properly without JavaScript enabled. Please enable it to continue."},{"uri":"https://gardener.cloud/readme/","title":"","tags":[],"description":"","content":"This folder contains the content of the public facing gardener landing page http://gardener.cloud\n"},{"uri":"https://gardener.cloud/adopter/","title":"Adopters","tags":[],"description":"","content":" See who is using Gardener Gardener adopters in production environments that have publicly shared details of their usage.       b’nerd uses Gardener as the core technology for its own managed Kubernetes as a Service solution and operates multiple Gardener installations for several cloud hosting service providers.    SAP uses Gardener to deploy and manage Kubernetes clusters at scale in a uniform way across infrastructures (AWS, Azure, GCP, Alicloud, OpenStack). Workloads include databases (SAP HANA), Big Data (SAP Data Hub), IoT, AI, and Machine Learning (SAP Leonardo), Serverless and diverse business workloads.    ScaleUp Technologiesruns Gardener within their public Openstack Clouds (Hamburg, Berlin, Düsseldorf). Their clients run all kinds of workloads on top of Gardener maintained Kubernetes clusters ranging from databases to Software-as-a-Service applications.    Finanz Informatik Technologie Services GmbHuses Gardener to offer k8s as a service for customers in the financial industry in Germany. It is built on top of a \"metal as a service\" infrastructure implemented from scratch for k8s workloads in mind. The result is k8s on top of bare metal in minutes.    PingCAP TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project. PingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers and they chose Gardener.    Beezlabs uses Gardener to deliver Intelligent Process Automation platform, on multiple cloud providers and reduce costs and lock-in risks.   If you’re using Gardener and you aren’t on this list, submit a pull request!    "},{"uri":"https://gardener.cloud/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/community/","title":"Community","tags":[],"description":"","content":"Gardener Community Follow - Engage - Contribute\n@GardenerProject  Follow the latest project updates on Twitter  Community Meetings  You are welcome on our community meetings where you can engage with other contributors in person. See calendar for schedule or watch past recordings to get the idea.  GitHub  Eveyone is welcome to contribute with what they can - an issue or a pull request. Check Gardener project there and our contributors guide to help you get started.   Gardener Project  Watch videos and community meetings recordings on our YouTube channel  #gardener  Discuss Gardener on our Slack channel in the Kubernetes workspace   COMMUNITY The Gardener development process is an open process. Here are the general communication channels we use to communicate. We work with the wider community to create a strong, vibrant codebase. 60+ Committer  1300+ Merged Pull Requests  1400+ Github Stars  500+ Closed Community Issues   We are cordially inviting interested parties to join our weekly meetings. Here you can address questions regarding the direction of the project, technical problems and support.   Our Slack Channel is the best way to contact the experts in all questions about Kubernetes and the Gardener and share your ideas with them or ask for support.   Find out more about the project and consider making a contribution..     "},{"uri":"https://gardener.cloud/tags/","title":"Tags","tags":[],"description":"","content":""}]