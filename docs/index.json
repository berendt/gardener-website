[
{
	"uri": "https://gardener.cloud/adopter/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "See who is using Gardener      This is a list of adopters of Gardener in production environments that have publicly shared details of their usage.   b’nerd uses Gardener as the core technology for its own managed Kubernetes as a Service solution and operates multiple Gardener installations for several cloud hosting service providers.       SAP uses Gardener to deploy and manage Kubernetes clusters at scale in a uniform way across infrastructures (AWS, Azure, GCP, Alicloud, OpenStack). Workloads include databases (SAP Hana), Big Data (SAP Data Hub), IoT, AI, and Machine Learning (SAP Leonardo), Serverless and diverse business workloads.    ScaleUp Technologies runs Gardener within their public Openstack Clouds (Hamburg, Berlin, Düsseldorf). Their clients run all kinds of workloads on top of Gardener maintained Kubernetes clusters ranging from databases to Software as a Service applications.    Finanz Informatik Technologie Services GmbH uses Gardener to offer k8s as a service for customers in the financial industry in Germany. It is built on top of a \"metal as a service\" infrastructure implemented from scratch for k8s workloads in mind. The result is k8s on top of bare metal in minutes.   If you’re using Gardener and aren’t on this list, feel free to submit a pull request!  "
},
{
	"uri": "https://gardener.cloud/blog/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/community/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "COMMUNITY The Gardener development process is an open process. Here are the general communication channels we use to communicate. We work with the wider community to create a strong, vibrant codebase. 60+ Committer  1300+ Merged Pull Requests  1400+ Github Stars  500+ Closed Community Issues   We are cordially inviting interested parties to join our weekly meetings. Here you can address questions regarding the direction of the project, technical problems and support.   Our Slack Channel is the best way to contact the experts in all questions about Kubernetes and the Gardener and share your ideas with them or ask for support.   Find out more about the project and consider making a contribution..     "
},
{
	"uri": "https://gardener.cloud/installer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "gardener-installerWe're sorry but gardener-installer doesn't work properly without JavaScript enabled. Please enable it to continue."
},
{
	"uri": "https://gardener.cloud/api-reference/",
	"title": "API Reference",
	"tags": [],
	"description": "",
	"content": " API Reference Documentation Bellow you can find reference documentation of the API for Gardener.\nGardener Gardener APIs are Kubernetes-style APIs for the life-cycle of a Kubernetes Cluster.\nThose APIs are divided into different groups:\n Core Extensions Settings Garden  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/access_pod_from_local/",
	"title": "Access a port of a pod locally",
	"tags": [],
	"description": "",
	"content": " Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to Access my service\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps:\n Run kubectl get pods Note down the name of the pod in question as \u0026lt;your-pod-name\u0026gt; Run kubectl port-forward \u0026lt;your-pod-name\u0026gt; \u0026lt;local-port\u0026gt;:\u0026lt;your-app-port\u0026gt; Run a web browser or curl locally and enter the URL http(s)://localhost:\u0026lt;local-port\u0026gt;  In addition, kubectl port-forward allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward. Find more details in the Kubernetes documentation.\nThe main drawback of this approach is that the pod\u0026rsquo;s name will change as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.\nSolution 2: Using apiserver proxy There are several different proxies used with Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. Different from the first solution, a service is required for this solution .\nUse the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read Discovering builtin services\nhttps://\u0026lt;cluster-master\u0026gt;/api/v1/namespace/\u0026lt;namespace\u0026gt;/services/\u0026lt;service\u0026gt;:\u0026lt;service-port\u0026gt;/proxy/\u0026lt;service-endpoint\u0026gt;\nExample:\n   cluster-master namespace service yservice-port service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / url   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 url    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the port-forward approach described above.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/service-access/",
	"title": "Access service from outside Kubernetes cluster",
	"tags": [],
	"description": "Is there an ingress deployed and how is it configured",
	"content": " TL;DR To expose your application / service for access from outside the cluster, following options exist:\n Kubernetes Service of type LoadBalancer Kubernetes Service of type \u0026lsquo;NodePort\u0026rsquo; + Ingress  This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there are many examples, here is one brief example.\nService Types A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.\nServices can be exposed in different ways by specifying a type in the service spec, and different types determine accessibility from inside and outside of cluster.\n ClusterIP NodePort LoadBalancer  Type ExternalName is a special case of service and not discussed here.\nType ClusterIP A service of type ClusterIP exposes a service on an internal IP in the cluster, which makes the service only reachable from within the cluster. This is the default value if no type is specified.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx-app replicas: 1 template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.13.12 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: labels: app: nginx-app name: nginx-svc namespace: default spec: type: ClusterIP # use ClusterIP as type here ports: - port: 80 selector: app: nginx-app  Execute following commands to create deployment and service\nkubectl create -f \u0026lt;Your yaml file name\u0026gt;  Checking the service status\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc ClusterIP 100.66.125.61 \u0026lt;none\u0026gt; 80/TCP 45m  As shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file.\nYou can test the service like this:\n# list all existing pods in cluster $ kubectl get pods NAME READY STATUS RESTARTS AGE docker-nodejs-app-76b77494-vwv4d 1/1 Running 0 11d nginx-deployment-74d949bf69-nvdzs 1/1 Running 0 1h privileged-pod 1/1 Running 0 11d # test service from within the cluster on the same pod $ kubectl exec -it nginx-deployment-74d949bf69-nvdzs curl 100.66.125.61:80 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 612 100 612 0 0 1006k 0 --:--:-- --:--:-- --:--:-- 597k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; ...    Tip - The service is also accessible from any other container (even from different pods) within the same cluster, e.g. kubectl -it exec \u0026lt;another POD_NAME\u0026gt; curl \u0026lt;YourServiceClusterIP:YourPort\u0026gt;. You need to make sure command curl is installed in the container. - You can also find out the dns name of the ClusterIP by command kubectl exec -it \u0026lt;POD_NAME\u0026gt; nslookup \u0026lt;ClusterIP\u0026gt;, replace the IP address with the resolved name in your test. The resolved name typically looks like nginx-svc.default.svc.cluster.local where nginx-svc is the name of your service defined in the configuration file.\n Type NodePort Follow the previous example, just replace the type with NodePort\n... spec: type: NodePort ports: - port: 80 ...  A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates a port in the range 30000–32767 and opens this port on every node (thus the name “NodePort”). Connections to this port are forwarded to the service’s cluster IP. If we create the service above and run kubectl get svc \u0026lt;your-service\u0026gt;, we can see the NodePort that has been allocated for it.\nNote that in the in following example, in addition to port 80, port 32521 has been opened as well on the node, in contrast to the output of \u0026ldquo;ClusterIP\u0026rdquo; case where only port 80 is opened.\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc NodePort 100.70.105.182 \u0026lt;none\u0026gt; 80:32521/TCP 16m  Therefore you can access the service from within the cluster in two ways:\n Access via ClusterIP:port  #via ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80 #via internal name of ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80   Access via NodeIP:NodePort  # First find out the Node IP address $ kubectl describe node Name: ip-10-250-20-203.eu-central-1.compute.internal Roles: node Addresses: InternalIP: 10.250.20.203 InternalDNS: ip-10-250-20-203.eu-central-1.compute.internal Hostname: ip-10-250-20-203.eu-central-1.compute.internal ... #via NodeIP:NodePort kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521 #via internal name of NodeIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521  Type LoadBalancer The LoadBalancer type is the simplest approach, which is created by specifying type as LoadBalancer.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx-app replicas: 1 template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.13.12 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: labels: app: nginx-app name: nginx-svc namespace: default spec: type: LoadBalancer # use LoadBalancer as type here ports: - port: 80 selector: app: nginx-app  Once the service is created, it has an external IP address as shown here:\n$ kubectl get services -l app=nginx-app -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-svc LoadBalancer 100.67.182.148 a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com 80:31196/TCP 9m app=nginx-app  A service of type LoadBalancer combines the capabilities of a NodePort with the ability to setup a complete ingress path.\nHence the service can be accessible from outside the cluster without the need for additional components like an Ingress.\nTo test the external IP run this curl command from your local machine:\n$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;... RawContent : HTTP/1.1 200 OK ...  Obviously the service can also is accessed from within the cluster. You can test this in the same way as described in section NodePort.\nLoadBalancer vs. Ingress As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster. However this approach has its own limitation. You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced for this purpose.\nWhy an Ingress LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a a separate resource that configures a LoadBalancer in a more flexible way. The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected pods which is more efficient.\nTypically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them you already pay 400$ per month just for load balancing.\nHow to use the ingress? In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:\n k8s-hana.ondemand.com  \u0026lt;gardener_cluster_name\u0026gt;.\u0026lt;gardener_project_name\u0026gt;.shoot.canary.k8s-hana.ondemand.com.\nBoth \u0026lt;gardener_cluster_name\u0026gt; and \u0026lt;gardener_project_name\u0026gt; are defined in Gardener which can be determined on Gardener dashboard.\nThis results in the following default DNS endpoints: * api.\u0026lt;cluster_domain\u0026gt; Kubernetes API * *.ingress.\u0026lt;cluster_domain\u0026gt; Internal nginx ingress\nExample: Configure an Ingress resource with Service type: NodePort With the configuration below you can reach your service nginx-svc with:\nhttp://test.ingress.\u0026amp;lt;GARDENER-CLUSTER-NAME\u0026amp;gt;.\u0026amp;lt;GARDENER-PROJECT-NAME\u0026amp;gt;.shoot.canary.k8s-hana.ondemand.com\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx-app replicas: 1 template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.13.12 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: labels: app: nginx-app name: nginx-svc namespace: default spec: type: NodePort ports: - port: 80 selector: app: nginx-app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: nginxsvc-ingress spec: rules: - host: nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: nginx-svc servicePort: 80  Show the newly created ingress and test it :\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginxsvc-ingress nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com 10.250.20.203 80 29s $ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; ...  Reference:  Concepts: Kubernetes Service Concepts: Connecting Applications with Services Tutorial: Using a Service to Expose Your App Tutorial: Using Source IP Kubernetes Networking Accessing Kubernetes Pods from Outside of the Cluster  "
},
{
	"uri": "https://gardener.cloud/using-gardener/administrator/",
	"title": "Administrator",
	"tags": [],
	"description": "",
	"content": "Learning Material Everything you need to know to operate gardener.    "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_22/",
	"title": "Anti Patterns",
	"tags": [],
	"description": "",
	"content": " Running as root user Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user.\nStoring data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n"
},
{
	"uri": "https://gardener.cloud/using-gardener/developer/topic/",
	"title": "App Developer",
	"tags": [],
	"description": "",
	"content": "Learning Material Everything you need to know about running your software.      by Topic     by Experience Level  \n\n"
},
{
	"uri": "https://gardener.cloud/030-architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": " Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it\u0026rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps:\n Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active  Overview Architecture Diagram Note: The kubelet as well as the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/25_markup/attachments/",
	"title": "Attachments",
	"tags": [],
	"description": "The Attachments shortcode displays a list of files attached to a page.",
	"content": " The Attachments shortcode displays a list of files attached to a page.\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    Usage The shortcurt lists files found in a specific folder. Currently, it support two implementations for pages\n If your page is a markdown file, attachements must be place in a folder named like your page and ending with .files.\n  content  _index.md page.files  attachment.pdf  page.md    If your page is a folder, attachements must be place in a nested \u0026lsquo;files\u0026rsquo; folder.\n  content  _index.md page  index.md files  attachment.pdf       Be aware that if you use a multilingual website, you will need to have as many folders as languages.\nThat\u0026rsquo;s all !\nParameters    Parameter Default Description     title \u0026ldquo;Attachments\u0026rdquo; List\u0026rsquo;s title   style \u0026rdquo;\u0026rdquo; Choose between \u0026ldquo;orange\u0026rdquo;, \u0026ldquo;grey\u0026rdquo;, \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;green\u0026rdquo; for nice style   pattern \u0026rdquo;.*\u0026rdquo; A regular expressions, used to filter the attachments by file name. The pattern parameter value must be regular expressions.    For example:\n To match a file suffix of \u0026lsquo;jpg\u0026rsquo;, use .*jpg (not *.jpg). To match file names ending in \u0026lsquo;jpg\u0026rsquo; or \u0026lsquo;png\u0026rsquo;, use .*(jpg|png)  Examples List of attachments ending in pdf or mp4 {{%attachments title=\u0026quot;Related files\u0026quot; pattern=\u0026quot;.*(pdf|mp4)\u0026quot;/%}}  renders as\n  Related files   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    Colored styled box {{%attachments style=\u0026quot;orange\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    {{%attachments style=\u0026quot;grey\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    {{%attachments style=\u0026quot;blue\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    {{%attachments style=\u0026quot;green\u0026quot; /%}}  renders as\n  Attachments   BachGavotteShort.mp3  (0 ko)   Carroll_AliceAuPaysDesMerveilles.pdf  (0 ko)   adivorciarsetoca00cape.pdf  (0 ko)   hugo.png  (17 ko)   movieselectricsheep-flock-244-32500-2.mp4  (0 ko)    "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/insecure-configuration/",
	"title": "Auditing Kubernetes for Secure Setup",
	"tags": [],
	"description": "A few insecure configurations in Kubernetes",
	"content": " Auditing Kubernetes for Secure Setup  Increasing the Security of all Gardener Stakeholders In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\nMajor Findings From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.\nAlban Crequy (Kinvolk) and Dirk Marwinski (SAP SE) gave a presentation entitled Hardening Multi-Cloud Kubernetes Clusters as a Service at KubeCon 2018 in Shanghai presenting some of the findings.\nHere is a summary of the findings:\n Privilege escalation due to insecure configuration of the Kubernetes API server\n Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server. Risk: Users can get access to the API server. Recommendation: Always use different CAs.  Exploration of the control plane network with malicious HTTP-redirects\n Root cause: See detailed description below. Risk: Provoked error message contains full HTTP payload from an existing endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials. Recommendation:\n Use the latest version of Gardener Ensure the seed cluster\u0026rsquo;s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn\u0026rsquo;t support network policies.   Reading private AWS metadata via Grafana\n Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control Risk: Users can get the \u0026ldquo;user-data\u0026rdquo; for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster Recommendation: Lockdown Grafana features to only what\u0026rsquo;s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints   Scenario 1: Privilege Escalation with Insecure API Server In most configurations, different components connect directly to the Kubernetes API server, often using a kubeconfig with a client certificate. The API server is started with the flag:\n/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ...  The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.\n The API server can have many clients of various kinds  However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.\n--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group   API server clients can reach the API server through an authenticating proxy  So far, so good. But what happens if malicious user “Mallory” tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?\n What happens when a client bypasses the proxy, connecting directly to the API server?  With a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header “X-Remote-Group: system:masters”.\nYou only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.\nThe kubectl tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP requests manually.\nWe worked on improving the Kubernetes documentation to make clearer that this configuration should be avoided.\nScenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.\n The API server is mostly a component that receives requests  However, there are exceptions. Some kubectl commands will trigger the API server to open a new connection to the Kubelet. Kubectl exec is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a HTTP-302 redirection to the Container Runtime Interface (CRI). Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because the Kubelet and the CRI component run on the same worker node.\n But the API server also initiates some connections, for example, to worker nodes  It’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with “host” volumes.\nIn contrast, users \u0026mdash; even those with “system:masters” permissions or “root” rights \u0026mdash; are often not given access to the control plane. On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network in the control plane.\nWhat would happen if a user was tampering with the Kubelet to make it maliciously redirect kubectl exec requests to a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.\n The API server is tricked to connect to other components  The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the AWS metadata service) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different EC2 instance profile for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.\nWe have reported this issue to the Kubernetes Security mailing list and the public pull request that addresses the issue has been merged PR#66516. It provides a way to enforce HTTP redirect validation (disabled by default).\nBut there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with Kubernetes Network Policies, EC2 Security Groups or just iptables directly. Following the defense in depth principle, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.\nIn Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the announcements on the Gardener mailing list. This is tracked in CVE-2018-2475.\nTo be protected from this issue, stakeholders should:\n Use the latest version of Gardener Ensure the seed cluster’s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn’t support network policies.  Scenario 3: Reading Private AWS Metadata via Grafana For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.\n Prometheus and Grafana can be used to monitor worker nodes  Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.\n Credentials can be retrieved from the debugging console of Chrome   Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets  In that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.\nThere are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.\nConclusion The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_46/",
	"title": "Auditing Kubernetes for Secure Setup",
	"tags": [],
	"description": "",
	"content": "In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\n Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\n..read some more on Auditing Kubernetes for Secure Setup.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/oidc-login/",
	"title": "Authenticating with an Identity Provider",
	"tags": [],
	"description": "Authenticating with an Identity Provider using OpenID Connect",
	"content": " Authenticating In this blog you will learn how to:\n Configure an Identity Provider using OpenID Connect. Configure a local kubectl plugin to enable oidc-login . Configure the K8s API Server of Gardener managed Kubernetes cluster. Create an RBAC rule to authorize an authenticated user.  Motivation As a project owner of Gardener, I want my Kubernetes level user to be authenticated by an identity provider.\nPrerequisite Knowledge Please read the following background material on Authenticating\nInsights About Gardener The Gardener allows the administrator to modify every aspect of the control plane setup, e.g. all feature gateways and even configurations are programmatically accessible. In this way, every administrative user of the Gardener has full control of how the control plane should be parameterized. But with this power, the user can easily configure a control plane that is beyond any SLA that the Gardener team can arguably support. Therefore, use this power wisely! A configuration that enables experimental features for production becomes an operational responsibility of the cluster owner\u0026rsquo;s team.\nBut Gardener does not stop you from experimenting!\nThere are currently no default IdP parameters.\nConfigure an Identity Provider Create a tenant in an OpenID-Connect compatible Identity Provider. For sake of simplicity, we shall use Auth0, which has a free plan for experimentations.\nIn your tenant, setup a native client/application that will use the authentication: Configure the client to have a callback url of http://localhost:8000. This callback will connect to your local kubectl oidc-login plugin: Note down the following parameters:\n Domain or Issuer url. It must be an https-secured endpoint (In case of Auth0, notice the trailing / at the end). Client ID Client Secret  Verify that https://\u0026lt;Issuer\u0026gt;/.well-known/openid-configuration is reachable.\nNow create some users (or connect to a user store): Notice that the users must have a verified email address. In doubt, just override that setting manually.\nConfigure kubectl oidc-login Please install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\n$ kubectl krew install oidc-login Updated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login  Prepare a kubeconfig for later use:\n$ cp ~/.kube/config ~/.kube/config-oidc  Modify the configurations as follows:\napiVersion: v1 kind: Config ... contexts: - context: cluster: shoot--project--mycluster user: my-oidc name: shoot--project--mycluster ... users: - name: my-oidc user: auth-provider: config: client-id: \u0026lt;Client ID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt; idp-issuer-url: \u0026quot;https://\u0026lt;Issuer\u0026gt;/\u0026quot; extra-scopes: email,offline_access,profile name: oidc  Ensure that the modified context is the active context current-context: shoot--project--mycluster.\nConfigure the Gardener Shoot Spec Modify the Gardener shoot/cluster manifest as follows:\napiVersion: garden.sapcloud.io/v1beta1 kind: Shoot metadata: name: mycluster namespace: garden-project ... spec: kubernetes: kubeAPIServer: oidcConfig: clientID: \u0026lt;Client ID\u0026gt; issuerURL: \u0026quot;https://\u0026lt;Issuer\u0026gt;/\u0026quot; usernameClaim: email  This change of the shoot manifest triggers a reconciliation. Once the reconciliation is finished, your oidc configuration is applied. It does not invalidate other certificate based authentication methods. Wait for Gardener to reconcile the change. It can take upto 5min.\nAuthorize an authenticated user For simplicity, we will just authorize a single user with the all encompassing cluster role cluster-admin:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-test roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: test@test.com  As administrator, activate/apply the above cluster role binding for test@test.com.\nVerify the Result Now activate the prepared kubeconfig-oidc and perform a login:\n$ export KUBECONFIG=~/.kube/config-oidc $ kubectl oidc-login Open http://localhost:8000 for authentication  The plugin opens a browser for an interctive authentication session, and in parallel serves a local webserver for the configured callback.\nIf you successfully verified your user, then the console will display the validity of your returned token:\nYou got a valid token until 2019-08-14 06:26:49 +0200 CEST  Inspect the kubeconfig-oidc. You will find two additional parameters:\n... users: - name: my-oidc user: auth-provider: config: client-id: \u0026lt;Client ID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt; idp-issuer-url: \u0026quot;https://\u0026lt;Issuer\u0026gt;/\u0026quot; extra-scopes: email,offline_access,profile id-token: eyJ0eX ... 4In0.QQKS ... TTTw refresh-token: LFt ... 0Skj name: oidc  The plugin persisted the id-token and refresh-token in your configuration file.\nVerify that your user actually has the cluster-admin role:\n$ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system blackbox-exporter-954dd954b-tk9vl 1/1 Running 0 7d5h kube-system calico-kube-controllers-5f4b46ffb5-ggb7z 1/1 Running 0 7d5h ... $ kubectl who-can create clusterrolebinding No subjects found with permissions to create clusterrolebinding assigned through RoleBindings CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group cluster-admin-test test@test.com User ...  Congratulations, you have just configured your cluster to authenticate against an Identity Provider using OpenID Connect!\nComing Improvements The Gardener team is working on a standard oidc shema to offer automatic configuration options on a per project basis.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/kubectl-apiserver/",
	"title": "Automated deployment",
	"tags": [],
	"description": "Automated deployment with kubectl",
	"content": " Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don\u0026rsquo;t want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites  Create a service account user   kubectl create serviceaccount deploy-user -n default   Bind a role to the newly created serviceuser \u0026gt;!!! warning !!! \u0026ldquo;In this example the preconfigured role \\\u0026ldquo;edit\\\u0026rdquo; and the namespace \\\u0026ldquo;default\\\u0026rdquo; is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\u0026quot;\u0026lt;   kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default   Get the URL of your API-server   APISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;)   Get the service account\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})  Generate a token for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)   Usage You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\n kubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml  "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_07/",
	"title": "Big things come in small packages",
	"tags": [],
	"description": "",
	"content": " Microservices tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - size of the technology stack.\nGeneral purpose technology stack There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight technology stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\nAdditionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize and deploy a microservice without impacting other subsystems.\n"
},
{
	"uri": "https://gardener.cloud/components/bouquet/",
	"title": "Bouquet",
	"tags": [],
	"description": "",
	"content": " Bouquet Bouquet is a draft addon manager for the Gardener. It incorporates some of the requested features of the community but not yet all of them.\n Caution: This software is early alpha. It is not meant for production use and shall (currently) only serve as a possible outlook of what is possible with pre-deployed software on Gardener Kubernetes clusters.\n Installation If you want to deploy Bouquet on a target Gardener cluster, run the following:\nhelm install charts/bouquet \\ --name gardener-bouquet \\ --namespace garden  This will deploy Bouquet with the required permissions into your garden cluster.\nStructure As of now, Bouquet comes with two new custom resources: AddonManifest and AddonInstance.\nAn AddonManifest can be considered equivalent to a Helm template. The manifest itself only contains metadata (like the name, default values etc.). The actual content of a manifest is specified via its source attribute. Currently, the only available source is a ConfigMap.\nAn AddonInstance references an AddonManifest and a target Shoot. It may also contain value overrides in its spec. As soon as an AddonInstance is created, Bouquet will apply the values to the templates and then ensure that the objects exist in the target shoot. If an AddonInstance is deleted, Bouquet will also make sure that the created objects are deleted as well.\nExample use case Say you want your cluster to contain istio right from the start. How can you do that?\nFirst you need to get the .yaml files necessary to deploy istio into your cluster. Download an istio release as follows:\nwget -O istio.yaml https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/istio/noauth/istio.yaml  This will fetch a .yaml file containing all necessary kubernetes objects of istio. To make this data available in your garden cluster, create a configmap in your cluster via\nkubectl -n garden create configmap istio-files --from-file ./istio.yaml  Now you need to create an AddonManifest that references this file and push it to Kubernetes. The file could look like the following:\napiVersion: \u0026quot;garden.sapcloud.io/v1alpha1\u0026quot; kind: \u0026quot;AddonManifest\u0026quot; metadata: name: \u0026quot;istio-0.0.1\u0026quot; spec: configMap: \u0026quot;istio-files\u0026quot;  You can submit this manifest to Kubernetes via kubectl (given that you saved the file to addonmanifest.yaml:\nkubectl -n garden apply -f addonmanifest.yaml  Once this is done, the only thing left to do is to create an AddonInstance referencing both your target Shoot and your AddonManifest. This AddonInstance has to be in the same namespace as your target Shoot:\napiVersion: \u0026quot;garden.sapcloud.io/v1alpha1\u0026quot; kind: \u0026quot;AddonInstance\u0026quot; metadata: name: \u0026quot;example\u0026quot; finalizers: - \u0026quot;bouquet\u0026quot; spec: manifest: namespace: \u0026quot;garden\u0026quot; name: \u0026quot;istio\u0026quot; version: \u0026quot;0.0.1\u0026quot; target: shoot: \u0026quot;addon-test\u0026quot;  And apply it via kubectl (given that you saved the file to addoninstance.yaml):\nkubectl -n garden-addon-test apply -f addoninstance.yaml  Bouquet will then start deploying your objects to the target Shoot once it is ready.\nOutlook / Future Since this is just a tech-preview, features like value / chart updates, more efficient templating, company addon guidelines etc. are not yet implemented / yet to come / yet to be discussed. It is also not yet clear whether this should eventually move into the Gardener or remain as a stand-alone component.\nCore points that have to be tackled are: * Fire and forget mode (only deploy objects once, don\u0026rsquo;t monitor afterwards) * Reconciliation (currently, updating behavior is not correctly implemented) * Updates of an addon (-\u0026gt; Update strategies) * Dependent addons / dependency resolution / dependency lifecycle\nAs such, contributions and help on shaping this topic is highly appreciated.\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/25_markup/button/",
	"title": "Button",
	"tags": [],
	"description": "Nice buttons on your page.",
	"content": "A button is a just a clickable button with optional icon.\n{{% button href=\u0026quot;https://getgrav.org/\u0026quot; %}}Get Grav{{% /button %}} {{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fa fa-download\u0026quot; %}}Get Grav with icon{{% /button %}} {{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fa fa-download\u0026quot; icon-position=\u0026quot;right\u0026quot; %}}Get Grav with icon right{{% /button %}}  Get Grav   Get Grav with icon  Get Grav with icon right   "
},
{
	"uri": "https://gardener.cloud/using-gardener/developer/experience/",
	"title": "By Skill",
	"tags": [],
	"description": "",
	"content": "Learning Material Everything you need to know about running your software.     by Topic      by Experience Level  \n\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/14_cicd/",
	"title": "CI/CD",
	"tags": [],
	"description": "",
	"content": " CI/CD As an execution environment for CI/CD workloads, we use Concourse. We however abstract from the underlying \u0026ldquo;build executor\u0026rdquo; and instead offer a Pipeline Definition Contract, through which components declare their build pipelines as required.\nIn order to run continuous delivery workloads for all components contributing to the Gardener project, we operate a central service.\nTypical workloads encompass the execution of tests and builds of a variety of technologies, as well as building and publishing container images, typically containing build results.\nWe are building our CI/CD offering around some principles:\n container-native - each workload is executed within a container environment. Components may customise used container images automation - pipelines are generated without manual interaction self-service - components customise their pipelines by changing their sources standardisation  Learn more on our: Build Pipeline Reference Manual\n"
},
{
	"uri": "https://gardener.cloud/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/components/cert--broker/",
	"title": "Cert Broker",
	"tags": [],
	"description": "",
	"content": " cert-broker Cert-Broker is a complementary component for Cert-Manager. It enables certificate management for Kubernetes clusters which don\u0026rsquo;t operate their own (in-cluster) Cert-Manager, e.g. for organizational resposibilities the Cert-Manager is located in another cluster.\nConcept To use or contribute to Cert-Broker it is fundamental to understand its main concept of control and target cluster.\n Control cluster: The cluster which operates an instance of Cert-Manager. Target cluster: The cluster which demands TLS certificates by Cert-Manager through the Cert-Broker.  Cert-Broker replicates Ingress resources from the target cluster to a predefined namespace in the control cluster. After the matching TLS Secret resource was created by Cert-Manager, Cert-Broker copies it to the appropriate Namespace in the target cluster. This works similarily in case the certificate is renewed.\nInstallation To install Cert-Broker on the control cluster, fill out the place holders and run\nhelm install charts/cert-broker \\ --name cert-broker \\ --namespace \u0026lt;Namespace\u0026gt; \\ --set certbroker.targetClusterSecret=\u0026lt;Target cluster Kubeconfig\u0026gt; \\ --set certmanager.dns=\u0026quot;{\u0026quot;\u0026lt;Domain\u0026gt;\u0026quot;.\u0026quot;\u0026lt;DNS Provider\u0026gt;\u0026quot;, \u0026quot;\u0026lt;Domain\u0026gt;\u0026quot;.\u0026quot;\u0026lt;DNS Provider\u0026gt;\u0026quot;}\u0026quot; \\ --set certmanager.clusterissuer=\u0026quot;\u0026lt;Issuer Name\u0026gt;\u0026quot;  Limitations In case Cert-Manager issues certificates for the target cluster with Let\u0026rsquo;s Encrypt, the domain\u0026rsquo;s ownership can only be proven by DNS records, i.e. DNS01 Challenges must be used.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/app/node-overprovisioning/",
	"title": "Cluster Overprovisioning",
	"tags": [],
	"description": "Cluster Overprovisioning",
	"content": " Cluster Overprovisioning This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work loads that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n Overprovisioning: Allocating more computer resources than is strictly necessary\nhttps://en.wikipedia.org/wiki/Overprovisioning\n When does the autoscaler change the size of the cluster? Below is a description of how the cluster behaves when there is a requirement to scale.\nScaling without overprovisioning  load hits the cluster (or a node is crashed) cannot schedule application-pods due to insufficient resources, scaling fails 💀 cluster-autoscaler notices and begins to provision new instance wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the application-pods and will schedule them  Scaling with Overprovisioning  load hits the cluster (or a node is crashed) placeholder-pods are evicted, scaling of application-pod is immediately successful placeholder-pods cannot be scheduled due to insufficient resources wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the placeholder pods and will schedule them  You can apply the above scenario one-to-one to the case when a node of the Hyperscaler dies.\nReal Scenario Test We executed normal and overprovisioning tests on a gardener cluster on different infrastructure provider (aws, azure, gcp, alicloud). All of them tested the downtime of the application pod running in the cluster, when a node dies.\nThe test results for the different IaaS provider are shown below.\nResults The results provided should only show how long the downtimes can be approximately. \u0026gt; The downtime results could vary +- 1 min, because the minimum request interval in UpTime is 1 minute\nAmazon Normal Overprovisioning Azure Normal Overprovisioning GCP Normal Overprovisioning AliCloud Normal Overprovisioning Summary of results Normal    Provider AWS Azure GCP AliCloud     Node deleted 08:56 09:32 09:39 09:53   Pod rescheduled 09:17 09:50 09:53 10:14   Downtime 21 min 18 min 14 min 21 min    Overprovisioning    Provider AWS Azure GCP AliCloud     Node deleted 14:20 06:00 06:05 08:23   Pod rescheduled 14:22 06:02 06:06 08:25   Downtime 2 min 2 min 1 min 2 min    Test Description We deployed a nginx web server and a service of type LoadBalancer to expose it. So we are able to call our endpoint with external tools like UpTime to check the availability of our nginx. It takes only a few seconds to deploy a nginx web server on kubernetes, so we could say: when your endpoint works, your node is up and running.\nWe wanted to test how much time it takes, when your node gets killed and your cluster has to create a new one to run your application on it.\nkubectl get nodes # select the node where your nginx is running on kubectl delete node \u0026lt;NGINX-HOSTED-NODE\u0026gt;  The downtime is tested with UpTime, which does every minute a request to our endpoint. Further we checked manually, if the node startup time and the timestamps on UpTime are almost similar.\nNext, deploy the overprovisioned version of our demo application and kill the node with the NGINX. As you can see - the pod comes up very fast and can serve content again.\n"
},
{
	"uri": "https://gardener.cloud/blog/2019_week_21/",
	"title": "Cluster Overprovisioning",
	"tags": [],
	"description": "",
	"content": "This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work load that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n..read some more on Cluster Overprovisioning.\n"
},
{
	"uri": "https://gardener.cloud/contribute/code/",
	"title": "Code",
	"tags": [],
	"description": "",
	"content": " Contributing Code How to Contribute to the Open Source Project Gardener    You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions: * Contributions must be licensed under the Apache 2.0 License * You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.\n "
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/15_conf_secrets/",
	"title": "Configuration and Usage",
	"tags": [],
	"description": "",
	"content": " Gardener Configuration and Usage Gardener automates the full lifecycle of Kubernetes clusters as a service. Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle. As a consequence, there are several configuration options for the various custom resources that are partially required.\nThis document describes the\n configuration and usage of Gardener as operator/administrator. configuration and usage of Gardener as end-user/stakeholder/customer.  Configuration and Usage of Gardener as Operator/Administrator When we use the terms \u0026ldquo;operator/administrator\u0026rdquo; we refer to both the people deploying and operating Gardener. Gardener consists out of four components:\n gardener-apiserver, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like Seeds and Shoots), and a component that contains multiple admission plugins. gardener-controller-manager, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining Shoots, reconciling Plants, etc.). gardener-scheduler, a component that assigns newly created Shoot clusters to appropriate Seed clusters. gardenlet, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of Shoots).  Each of these components have various configuration options. The gardener-apiserver uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags. The two other components are using so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener scheduler The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file. Please take a look at this example configuration. Information about the concepts of the Gardener scheduler can be found here\nConfiguration file for Gardenlet The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file. Please take a look at this example configuration. Information about the concepts of the Gardenlet can be found here\nSystem configuration After successful deployment of the four components you need to setup the system. Let\u0026rsquo;s first focus on some \u0026ldquo;static\u0026rdquo; configuration. When the gardenlet starts it scans the garden namespace of the garden cluster for Secrets that have influence on its reconciliation loops, mainly the Shoot reconciliation:\n Internal domain secret, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called \u0026ldquo;internal\u0026rdquo; DNS records for the Shoot clusters, please see this for an example.\n This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components. The DNS records are normal DNS records but called \u0026ldquo;internal\u0026rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters. It is forbidden to change the internal domain secret if there are existing shoot clusters.  Default domain secrets (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., example.com), please see this for an example.\n Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster. As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don\u0026rsquo;t specify their own domain.   :warning: Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not tainted with seed.gardener.cloud/disable-dns. Seeds with this taint don\u0026rsquo;t create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don\u0026rsquo;t need to create the domain secrets.\n Alerting secrets (optional), contain the alerting configuration and credentials for the AlertManager to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this for an example.\n If email alerting is configured: An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster. Gardener will inject the SMTP credentials into the configuration of the AlertManager. The AlertManager will send emails to the configured email address in case any alerts are firing. If an external AlertManager is configured: Each shoot has a Prometheus responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret. This external AlertManager is not managed by Gardener and can be configured however the operator sees fit. Supported authentication types are no authentication, basic, or mutual TLS.  OpenVPN Diffie-Hellmann Key secret (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this for an example.\n If you don\u0026rsquo;t specify a custom key then a default key is used, but for productive landscapes it\u0026rsquo;s recommend to create a landscape-specific key and define it.  Global monitoring secrets (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.\n These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.   Apart from this \u0026ldquo;static\u0026rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener. As an operator/administrator you have to configure some of them to make the system work.\nConfiguration and Usage of Gardener as End-User/Stakeholder/Customer As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team. You don\u0026rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed. Take a look at this document - it describes which resources are offered by Gardener. You may want to have a more detailed look for Projects, SecretBindings, Shoots, Plants, and (Cluster)OpenIDConnectPresets.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/ssh-into-node/",
	"title": "Connecting to a worker node using SSH",
	"tags": [],
	"description": "Connecting to a worker node using SSH",
	"content": " Term clarification We are talking about connection via SSH to a node, and not open a shell in an existing pod or rather container.\nFor ways how to connect via SSH into container please check official kubernetes tutorial\nWhy When we hear this question, we respond with another question: \u0026ldquo;Why would you need to?\u0026rdquo;. The background of this question is that all VMs are ephemeral (cattle, no pets). Any machine can be terminated any time. When updating a cluster, this will even happen to all machines (one by one). Anyway, sometimes curiosity is the driving factor and in this case, that\u0026rsquo;s a good thing.\nHow create a new file privileged-pod.yaml with the content:\napiVersion: v1 kind: Pod metadata: name: privileged-pod namespace: default spec: containers: - name: busybox image: busybox resources: limits: cpu: 200m memory: 100Mi requests: cpu: 100m memory: 50Mi stdin: true securityContext: privileged: true volumeMounts: - name: host-root-volume mountPath: /host readOnly: true volumes: - name: host-root-volume hostPath: path: / hostNetwork: true hostPID: true restartPolicy: Never  kubectl create -f privileged-pod.yaml  Now you can look around in the pod:\nkubectl exec -ti privileged-pod sh ps aux ip a ls -la /host  Run as root using the node\u0026rsquo;s filesystem instead of the filesystem in the container running on the node:\nchroot /host/  Then you can run commands such as docker ps\nDon\u0026rsquo;t forget to delete your pod afterwards:\nkubectl delete pod privileged-pod  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/missing-registry-permission/",
	"title": "Container image not pulled",
	"tags": [],
	"description": "Wrong Container Image or Invalid Registry Permissions",
	"content": " Problem Two of the most common problems are specifying the wrong container image or trying to use private images without providing registry credentials.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let\u0026rsquo;s see an example. We\u0026rsquo;ll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456  the command prompt doesn\u0026rsquo;t return and you can press ctrl+c\nError analysis We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\n$ (minikube) kubectl get pods NAME READY STATUS RESTARTS AGE client-5b65b6c866-cs4ch 1/1 Running 1 1m fail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u0026lt;invalid\u0026gt; vuejs-578574b75f-5x98z 1/1 Running 0 1d $ (minikube)  For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8  As you can see in the events section, your image can\u0026rsquo;t be pulled\nName:\tfail-6667d7685d-7v6w8 Namespace:\tdefault Node:\tminikube/192.168.64.10 Start Time:\tWed, 22 Nov 2017 10:01:59 +0100 Labels:\tpod-template-hash=2223832418 run=fail Annotations:\tkubernetes.io/created-by={\u0026quot;kind\u0026quot;:\u0026quot;SerializedReference\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;reference\u0026quot;:{\u0026quot;kind\u0026quot;:\u0026quot;ReplicaSet\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;fail-6667d7685d\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\u0026quot;,\u0026quot;a... . . . . Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube 1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026quot;default-token-9fr6r\u0026quot; 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \u0026quot;tutum/curl:1.123456\u0026quot; 1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \u0026quot;tutum/curl:1.123456\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found 1m\t\u0026lt;invalid\u0026gt;\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod 1m\t\u0026lt;invalid\u0026gt;\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \u0026quot;tutum/curl:1.123456\u0026quot;  Why couldn\u0026rsquo;t Kubernetes pull the image? There are three primary candidates besides network connectivity issues: - The image tag is incorrect - The image doesn\u0026rsquo;t exist - Kubernetes doesn\u0026rsquo;t have permissions to pull that image\nIf you don\u0026rsquo;t notice a typo in your image tag, then it\u0026rsquo;s time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn\u0026rsquo;t have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u0026lt;username\u0026gt; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;email\u0026gt;  If the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn\u0026rsquo;t exist. Go to the Docker registry and check which tags are available for this image.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/image-pull-policy/",
	"title": "Container image not updating",
	"tags": [],
	"description": "Updating Images in your cluster during development",
	"content": " Preface A container image should use a fixed tag or the content hash of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Many Kubernetes users have run into this problem. The story goes something like this:\n Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update your deployment Realize that the bug is still present Rinse and repeat steps 3 to 5 until you recognize this doesn\u0026rsquo;t work  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, hence it doesn\u0026rsquo;t attempt to do a docker pull. When the new Pods come up, they still use the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push the build result to the registry.\n#!/usr/bin/env bash # Set the docker image name and the corresponding repository # Ensure that you change them in the deployment.yml as well. # You must be logged in with docker login… # # CHANGE THIS TO YOUR Docker.io SETTINGS # PROJECT=awesomeapp REPOSITORY=cp-enablement # exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x # build my nodeJS app # npm run build # get latest version IDs from the Docker.io registry and increment them # VERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e 's/[][]//g' -e 's/\u0026quot;//g' -e 's/ //g' | tr '}' '\\n' | awk -F: '{print $3}' | grep v| tail -n 1) VERSION=${VERSION:1} ((VERSION++)) VERSION=\u0026quot;v$VERSION\u0026quot; # build a new docker image # echo '\u0026gt;\u0026gt;\u0026gt; Building new image' # Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875) docker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log RESULT=$(cat /tmp/docker_build_result.log | tail -n 1) if [[ \u0026quot;$RESULT\u0026quot; != *Successfully* ]]; then exit -1 fi echo '\u0026gt;\u0026gt;\u0026gt; Push new image' docker push $REPOSITORY/$PROJECT:$VERSION  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/",
	"title": "Content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/contribute/",
	"title": "Contribute",
	"tags": [],
	"description": "",
	"content": " graph TB; B(Contributor) B -- C{I want to..} C --|Code| D(\"fa:fa-code-fork \u0026lt;a href\u0026#61;\u0026#39;./code\u0026#39;\u0026gt;Code Contribute\u0026lt;/a\u0026gt;\") C --|Docs| E(\"fa:fa-paragraph \u0026lt;a href\u0026#61;\u0026#39;./docs\u0026#39;\u0026gt;Doc Contribute\u0026lt;/a\u0026gt;\")  Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n If you are a new contributor see: Steps to Contribute\n If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nIndividual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project\u0026rsquo;s license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist  Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n Add tests relevant to the fixed bug or new feature.\n  Issues and Planning We use GitHub issues to track bugs and enhancement requests and ZenHub for planning. * Install the ZenHub Chrome plugin * Login to ZenHub * Open the Gardener ZenHub workspace\nSecurity Release Process See Security Release Process\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/10-contribution_guide/",
	"title": "Contribution Guide",
	"tags": [],
	"description": "",
	"content": " Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n If you are a new contributor see: Steps to Contribute\n If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  Individual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project\u0026rsquo;s license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist  Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. “Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.”).\n Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.   Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_51/",
	"title": "Cookies are dangerous...",
	"tags": [],
	"description": "",
	"content": "\u0026hellip;they mess up the figure.\nFor a team event during the Christmas season we decided to completely reinterpret the topic cookies\u0026hellip; since the vegetables have gone on a well-deserved vacation. :-)\nGet recipe on Gardener Cookies.\n"
},
{
	"uri": "https://gardener.cloud/api-reference/core/",
	"title": "Core",
	"tags": [],
	"description": "",
	"content": "Packages:\n  core.gardener.cloud/v1beta1   core.gardener.cloud/v1beta1  Package v1beta1 is a version of the API.\nResource Types:  BackupBucket  BackupEntry  CloudProfile  ControllerInstallation  ControllerRegistration  Plant  Project  Quota  SecretBinding  Seed  Shoot  BackupBucket   BackupBucket holds details about backup bucket\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec     Specification of the Backup Bucket.\n     provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n       status  BackupBucketStatus     Most recently observed status of the Backup Bucket.\n    BackupEntry   BackupEntry holds details about shoot backup.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec     (Optional) Spec contains the specification of the Backup Entry.\n     bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n       status  BackupEntryStatus     (Optional) Status contains the most recently observed status of the Backup Entry.\n    CloudProfile   CloudProfile represents certain properties about a provider environment.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  CloudProfile    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  CloudProfileSpec     (Optional) Spec defines the provider environment properties.\n     caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  ProviderConfig     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n       ControllerInstallation   ControllerInstallation represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerInstallation    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerInstallationSpec     Spec contains the specification of this installation.\n     registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n       status  ControllerInstallationStatus     Status contains the status of this installation.\n    ControllerRegistration   ControllerRegistration represents a registration of an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerRegistration    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerRegistrationSpec     Spec contains the specification of this registration.\n     resources  []ControllerResource     Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n       Plant      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Plant    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  PlantSpec     Spec contains the specification of this Plant.\n     secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n       status  PlantStatus     Status contains the status of this Plant.\n    Project   Project holds certain properties about a Gardener project.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Project    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ProjectSpec     (Optional) Spec defines the project properties.\n     createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n       status  ProjectStatus     (Optional) Most recently observed status of the Project.\n    Quota      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Quota    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  QuotaSpec     (Optional) Spec defines the Quota constraints.\n     clusterLifetimeDays  int    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n       SecretBinding      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  SecretBinding    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret object in the same or another namespace.\n    quotas  []Kubernetes core/v1.ObjectReference     (Optional) Quotas is a list of references to Quota objects in the same or another namespace.\n    Seed   Seed represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Seed    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     Spec contains the specification of this installation.\n     backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n       status  SeedStatus     Status contains the status of this installation.\n    Shoot      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Shoot    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the Shoot cluster.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n       status  ShootStatus     (Optional) Most recently observed status of the Shoot cluster.\n    Addon   (Appears on: KubernetesDashboard, NginxIngress)  Addon also enabling or disabling a specific addon and is used to derive from.\n   Field Description      enabled  bool    Enabled indicates whether the addon is enabled or not.\n    Addons   (Appears on: ShootSpec)  Addons is a collection of configuration for specific addons which are managed by the Gardener.\n   Field Description      kubernetesDashboard  KubernetesDashboard     (Optional) KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n    nginxIngress  NginxIngress     (Optional) NginxIngress holds configuration settings for the nginx-ingress addon.\n    AdmissionPlugin   (Appears on: KubeAPIServerConfig)  AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n   Field Description      name  string    Name is the name of the plugin.\n    config  ProviderConfig     (Optional) Config is the configuration of the plugin.\n    Alerting   (Appears on: Monitoring)  Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n   Field Description      emailReceivers  []string    (Optional) MonitoringEmailReceivers is a list of recipients for alerts\n    AuditConfig   (Appears on: KubeAPIServerConfig)  AuditConfig contains settings for audit of the api server\n   Field Description      auditPolicy  AuditPolicy     (Optional) AuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n    AuditPolicy   (Appears on: AuditConfig)  AuditPolicy contains audit policy for kube-apiserver\n   Field Description      configMapRef  Kubernetes core/v1.ObjectReference     (Optional) ConfigMapRef is a reference to a ConfigMap object in the same namespace, which contains the audit policy for the kube-apiserver.\n    AvailabilityZone   (Appears on: Region)  AvailabilityZone is an availability zone.\n   Field Description      name  string    Name is an an availability zone name.\n    unavailableMachineTypes  []string    (Optional) UnavailableMachineTypes is a list of machine type names that are not availability in this zone.\n    unavailableVolumeTypes  []string    (Optional) UnavailableVolumeTypes is a list of volume type names that are not availability in this zone.\n    BackupBucketProvider   (Appears on: BackupBucketSpec)  BackupBucketProvider holds the details of cloud provider of the object store.\n   Field Description      type  string    Type is the type of provider.\n    region  string    Region is the region of the bucket.\n    BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the specification of a Backup Bucket.\n   Field Description      provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus holds the most recently observed status of the Backup Bucket.\n   Field Description      lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupBucket.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the BackupBucket\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the specification of a Backup Entry.\n   Field Description      bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus holds the most recently observed status of the Backup Entry.\n   Field Description      lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupEntry.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the BackupEntry\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    CloudInfo   (Appears on: ClusterInfo)  CloudInfo contains information about the cloud\n   Field Description      type  string    Type is the cloud type\n    region  string    Region is the cloud region\n    CloudProfileSpec   (Appears on: CloudProfile)  CloudProfileSpec is the specification of a CloudProfile. It must contain exactly one of its defined keys.\n   Field Description      caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  ProviderConfig     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    ClusterAutoscaler   (Appears on: Kubernetes)  ClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n   Field Description      scaleDownDelayAfterAdd  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 10 mins).\n    scaleDownDelayAfterDelete  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (defaults to ScanInterval).\n    scaleDownDelayAfterFailure  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n    scaleDownUnneededTime  Kubernetes meta/v1.Duration     (Optional) ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 10 mins).\n    scaleDownUtilizationThreshold  float64    (Optional) ScaleDownUtilizationThreshold defines the threshold in % under which a node is being removed\n    scanInterval  Kubernetes meta/v1.Duration     (Optional) ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n    ClusterInfo   (Appears on: PlantStatus)  ClusterInfo contains information about the Plant cluster\n   Field Description      cloud  CloudInfo     Cloud describes the cloud information\n    kubernetes  KubernetesInfo     Kubernetes describes kubernetes meta information (e.g., version)\n    Condition   (Appears on: ControllerInstallationStatus, PlantStatus, SeedStatus, ShootStatus)  Condition holds the information about the state of a resource.\n   Field Description      type  ConditionType     Type of the Shoot condition.\n    status  ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastTransitionTime  Kubernetes meta/v1.Time     Last time the condition transitioned from one status to another.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the condition was updated.\n    reason  string    The reason for the condition\u0026rsquo;s last transition.\n    message  string    A human readable message indicating details about the transition.\n    ConditionStatus (string alias)\n  (Appears on: Condition)  ConditionStatus is the status of a condition.\nConditionType (string alias)\n  (Appears on: Condition)  ConditionType is a string alias.\nControllerDeployment   (Appears on: ControllerRegistrationSpec)  ControllerDeployment contains information for how this controller is deployed.\n   Field Description      type  string    Type is the deployment type.\n    providerConfig  ProviderConfig     (Optional) ProviderConfig contains type-specific configuration.\n    ControllerInstallationSpec   (Appears on: ControllerInstallation)  ControllerInstallationSpec is the specification of a ControllerInstallation.\n   Field Description      registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n    ControllerInstallationStatus   (Appears on: ControllerInstallation)  ControllerInstallationStatus is the status of a ControllerInstallation.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a ControllerInstallations\u0026rsquo;s current state.\n    providerStatus  ProviderConfig     (Optional) ProviderStatus contains type-specific status.\n    ControllerRegistrationSpec   (Appears on: ControllerRegistration)  ControllerRegistrationSpec is the specification of a ControllerRegistration.\n   Field Description      resources  []ControllerResource     Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n    ControllerResource   (Appears on: ControllerRegistrationSpec)  ControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, \u0026hellip;) and the actual type for this kind (aws-route53, gcp, auditlog, \u0026hellip;).\n   Field Description      kind  string    Kind is the resource kind, for example \u0026ldquo;OperatingSystemConfig\u0026rdquo;.\n    type  string    Type is the resource type, for example \u0026ldquo;coreos\u0026rdquo; or \u0026ldquo;ubuntu\u0026rdquo;.\n    globallyEnabled  bool    (Optional) GloballyEnabled determines if this ControllerResource is required by all Shoot clusters.\n    reconcileTimeout  Kubernetes meta/v1.Duration     (Optional) ReconcileTimeout defines how long Gardener should wait for the resource reconciliation.\n    DNS   (Appears on: ShootSpec)  DNS holds information about the provider, the hosted zone id and the domain.\n   Field Description      domain  string    (Optional) Domain is the external available domain of the Shoot cluster. This domain will be written into the kubeconfig that is handed out to end-users.\n    providers  []DNSProvider     (Optional) Providers is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if not a default domain is used.\n    DNSIncludeExclude   (Appears on: DNSProvider)     Field Description      include  []string    (Optional) Include is a list of resources that shall be included.\n    exclude  []string    (Optional) Exclude is a list of resources that shall be excluded.\n    DNSProvider   (Appears on: DNS)  DNSProvider contains information about a DNS provider.\n   Field Description      domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    secretName  string    (Optional) SecretName is a name of a secret containing credentials for the stated domain and the provider. When not specified, the Gardener will use the cloud provider credentials referenced by the Shoot and try to find respective credentials there. Specifying this field may override this behavior, i.e. forcing the Gardener to only look into the given secret.\n    type  string    (Optional) Type is the DNS provider type for the Shoot. Only relevant if not the default domain is used for this shoot.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    Endpoint   (Appears on: PlantSpec)  Endpoint is an endpoint for monitoring, logging and other services around the plant.\n   Field Description      name  string    Name is the name of the endpoint\n    url  string    URL is the url of the endpoint\n    purpose  string    Purpose is the purpose of the endpoint\n    ErrorCode (string alias)\n  (Appears on: LastError)  ErrorCode is a string alias.\nExpirableVersion   (Appears on: KubernetesSettings, MachineImage)  ExpirableVersion contains a version and an expiration date.\n   Field Description      version  string    Version is the version identifier.\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which this version expires.\n    Extension   (Appears on: ShootSpec)  Extension contains type and provider information for Shoot extensions.\n   Field Description      type  string    Type is the type of the extension resource.\n    providerConfig  ProviderConfig     (Optional) ProviderConfig is the configuration passed to extension resource.\n    Gardener   (Appears on: SeedStatus, ShootStatus)  Gardener holds the information about the Gardener version that operated a resource.\n   Field Description      id  string    ID is the Docker container id of the Gardener which last acted on a resource.\n    name  string    Name is the hostname (pod name) of the Gardener which last acted on a resource.\n    version  string    Version is the version of the Gardener which last acted on a resource.\n    GardenerDuration   (Appears on: HorizontalPodAutoscalerConfig)  GardenerDuration is a workaround for missing OpenAPI functions on metav1.Duration struct.\n   Field Description      Duration  time.Duration         Hibernation   (Appears on: ShootSpec)  Hibernation contains information whether the Shoot is suspended or not.\n   Field Description      enabled  bool    (Optional) Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot\u0026rsquo;s desired state is to be hibernated. If it is false or nil, the Shoot\u0026rsquo;s desired state is to be awaken.\n    schedules  []HibernationSchedule     (Optional) Schedules determine the hibernation schedules.\n    HibernationSchedule   (Appears on: Hibernation)  HibernationSchedule determines the hibernation schedule of a Shoot. A Shoot will be regularly hibernated at each start time and will be woken up at each end time. Start or End can be omitted, though at least one of each has to be specified.\n   Field Description      start  string    (Optional) Start is a Cron spec at which time a Shoot will be hibernated.\n    end  string    (Optional) End is a Cron spec at which time a Shoot will be woken up.\n    location  string    (Optional) Location is the time location in which both start and and shall be evaluated.\n    HorizontalPodAutoscalerConfig   (Appears on: KubeControllerManagerConfig)  HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      cpuInitializationPeriod  GardenerDuration     (Optional) The period after which a ready pod transition is considered to be the first.\n    downscaleDelay  GardenerDuration     (Optional) The period since last downscale, before another downscale can be performed in horizontal pod autoscaler.\n    downscaleStabilization  GardenerDuration     (Optional) The configurable window at which the controller will choose the highest recommendation for autoscaling.\n    initialReadinessDelay  GardenerDuration     (Optional) The configurable period at which the horizontal pod autoscaler considers a Pod “not yet ready” given that it’s unready and it has transitioned to unready during that time.\n    syncPeriod  GardenerDuration     (Optional) The period for syncing the number of pods in horizontal pod autoscaler.\n    tolerance  float64    (Optional) The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n    upscaleDelay  GardenerDuration     (Optional) The period since last upscale, before another upscale can be performed in horizontal pod autoscaler.\n    KubeAPIServerConfig   (Appears on: Kubernetes)  KubeAPIServerConfig contains configuration settings for the kube-apiserver.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     admissionPlugins  []AdmissionPlugin     (Optional) AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding configuration.\n    apiAudiences  []string    (Optional) APIAudiences are the identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. If serviceAccountConfig.issuer is configured and this is not, this defaults to a single element list containing the issuer URL.\n    auditConfig  AuditConfig     (Optional) AuditConfig contains configuration settings for the audit of the kube-apiserver.\n    enableBasicAuthentication  bool    (Optional) EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n    oidcConfig  OIDCConfig     (Optional) OIDCConfig contains configuration settings for the OIDC provider.\n    runtimeConfig  map[string]bool    (Optional) RuntimeConfig contains information about enabled or disabled APIs.\n    serviceAccountConfig  ServiceAccountConfig     (Optional) ServiceAccountConfig contains configuration settings for the service account handling of the kube-apiserver.\n    KubeControllerManagerConfig   (Appears on: Kubernetes)  KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     horizontalPodAutoscaler  HorizontalPodAutoscalerConfig     (Optional) HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n    nodeCIDRMaskSize  int32    (Optional) NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24)\n    KubeProxyConfig   (Appears on: Kubernetes)  KubeProxyConfig contains configuration settings for the kube-proxy.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     mode  ProxyMode     (Optional) Mode specifies which proxy mode to use. defaults to IPTables.\n    KubeSchedulerConfig   (Appears on: Kubernetes)  KubeSchedulerConfig contains configuration settings for the kube-scheduler.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     KubeletConfig   (Appears on: Kubernetes, WorkerKubernetes)  KubeletConfig contains configuration settings for the kubelet.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     cpuCFSQuota  bool    (Optional) CPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n    cpuManagerPolicy  string    (Optional) CPUManagerPolicy allows to set alternative CPU management policies (default: none).\n    evictionHard  KubeletConfigEviction     (Optional) EvictionHard describes a set of eviction thresholds (e.g. memory.available   evictionMaxPodGracePeriod  int32    (Optional) EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. Default: 90\n    evictionMinimumReclaim  KubeletConfigEvictionMinimumReclaim     (Optional) EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure. Default: 0 for each resource\n    evictionPressureTransitionPeriod  Kubernetes meta/v1.Duration     (Optional) EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. Default: 4m0s\n    evictionSoft  KubeletConfigEviction     (Optional) EvictionSoft describes a set of eviction thresholds (e.g. memory.available   evictionSoftGracePeriod  KubeletConfigEvictionSoftGracePeriod     (Optional) EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction. Default: memory.available: 1m30s nodefs.available: 1m30s nodefs.inodesFree: 1m30s imagefs.available: 1m30s imagefs.inodesFree: 1m30s\n    maxPods  int32    (Optional) MaxPods is the maximum number of Pods that are allowed by the Kubelet. Default: 110\n    podPidsLimit  int64    (Optional) PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n    KubeletConfigEviction   (Appears on: KubeletConfig)  KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n   Field Description      memoryAvailable  string    (Optional) MemoryAvailable is the threshold for the free memory on the host server.\n    imageFSAvailable  string    (Optional) ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  string    (Optional) ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n    nodeFSAvailable  string    (Optional) NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  string    (Optional) NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n    KubeletConfigEvictionMinimumReclaim   (Appears on: KubeletConfig)  KubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.\n   Field Description      memoryAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MemoryAvailable is the threshold for the memory reclaim on the host server.\n    imageFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n    nodeFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n    KubeletConfigEvictionSoftGracePeriod   (Appears on: KubeletConfig)  KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n   Field Description      memoryAvailable  Kubernetes meta/v1.Duration     (Optional) MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n    imageFSAvailable  Kubernetes meta/v1.Duration     (Optional) ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n    imageFSInodesFree  Kubernetes meta/v1.Duration     (Optional) ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n    nodeFSAvailable  Kubernetes meta/v1.Duration     (Optional) NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n    nodeFSInodesFree  Kubernetes meta/v1.Duration     (Optional) NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n    Kubernetes   (Appears on: ShootSpec)  Kubernetes contains the version and configuration variables for the Shoot control plane.\n   Field Description      allowPrivilegedContainers  bool    (Optional) AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot (default: true).\n    clusterAutoscaler  ClusterAutoscaler     (Optional) ClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n    kubeAPIServer  KubeAPIServerConfig     (Optional) KubeAPIServer contains configuration settings for the kube-apiserver.\n    kubeControllerManager  KubeControllerManagerConfig     (Optional) KubeControllerManager contains configuration settings for the kube-controller-manager.\n    kubeScheduler  KubeSchedulerConfig     (Optional) KubeScheduler contains configuration settings for the kube-scheduler.\n    kubeProxy  KubeProxyConfig     (Optional) KubeProxy contains configuration settings for the kube-proxy.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    version  string    Version is the semantic Kubernetes version to use for the Shoot cluster.\n    KubernetesConfig   (Appears on: KubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)  KubernetesConfig contains common configuration fields for the control plane components.\n   Field Description      featureGates  map[string]bool    (Optional) FeatureGates contains information about enabled feature gates.\n    KubernetesDashboard   (Appears on: Addons)  KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     authenticationMode  string    (Optional) AuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n    KubernetesInfo   (Appears on: ClusterInfo)  KubernetesInfo contains the version and configuration variables for the Plant cluster.\n   Field Description      version  string    Version is the semantic Kubernetes version to use for the Plant cluster.\n    KubernetesSettings   (Appears on: CloudProfileSpec)  KubernetesSettings contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n   Field Description      versions  []ExpirableVersion     (Optional) Versions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n    LastError   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastError indicates the last occurred error for an operation on a resource.\n   Field Description      description  string    A human readable message indicating details about the last error.\n    taskID  string    (Optional) ID of the task which caused this last error\n    codes  []ErrorCode     (Optional) Well-defined error codes of the last error(s).\n    lastUpdateTime  Kubernetes meta/v1.Time     (Optional) Last time the error was reported\n    LastOperation   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastOperation indicates the type and the state of the last operation, along with a description message and a progress indicator.\n   Field Description      description  string    A human readable message indicating details about the last operation.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the operation state transitioned from one to another.\n    progress  int    The progress in percentage (0-100) of the last operation.\n    state  LastOperationState     Status of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.\n    type  LastOperationType     Type of the last operation, one of Create, Reconcile, Delete.\n    LastOperationState (string alias)\n  (Appears on: LastOperation)  LastOperationState is a string alias.\nLastOperationType (string alias)\n  (Appears on: LastOperation)  LastOperationType is a string alias.\nMachine   (Appears on: Worker)  Machine contains information about the machine type and image.\n   Field Description      type  string    Type is the machine type of the worker group.\n    image  ShootMachineImage     (Optional) Image holds information about the machine image to use for all nodes of this pool. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    MachineImage   (Appears on: CloudProfileSpec)  MachineImage defines the name and multiple versions of the machine image in any environment.\n   Field Description      name  string    Name is the name of the image.\n    versions  []ExpirableVersion     Versions contains versions and expiration dates of the machine image\n    MachineType   (Appears on: CloudProfileSpec)  MachineType contains certain properties of a machine type.\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     CPU is the number of CPUs for this machine type.\n    gpu  k8s.io/apimachinery/pkg/api/resource.Quantity     GPU is the number of GPUs for this machine type.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     Memory is the amount of memory for this machine type.\n    name  string    Name is the name of the machine type.\n    storage  MachineTypeStorage     (Optional) Storage is the amount of storage associated with the root volume of this machine type.\n    usable  bool    (Optional) Usable defines if the machine type can be used for shoot clusters.\n    MachineTypeStorage   (Appears on: MachineType)  MachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n   Field Description      class  string    Class is the class of the storage type.\n    size  k8s.io/apimachinery/pkg/api/resource.Quantity     Size is the storage size.\n    type  string    Type is the type of the storage.\n    Maintenance   (Appears on: ShootSpec)  Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n   Field Description      autoUpdate  MaintenanceAutoUpdate     (Optional) AutoUpdate contains information about which constraints should be automatically updated.\n    timeWindow  MaintenanceTimeWindow     (Optional) TimeWindow contains information about the time window for maintenance operations.\n    MaintenanceAutoUpdate   (Appears on: Maintenance)  MaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n   Field Description      kubernetesVersion  bool    KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).\n    machineImageVersion  bool    MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n    MaintenanceTimeWindow   (Appears on: Maintenance)  MaintenanceTimeWindow contains information about the time window for maintenance operations.\n   Field Description      begin  string    Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, a random value will be computed.\n    end  string    End is the end of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, the value will be computed based on the \u0026ldquo;Begin\u0026rdquo; value.\n    Monitoring   (Appears on: ShootSpec)  Monitoring contains information about the monitoring configuration for the shoot.\n   Field Description      alerting  Alerting     (Optional) Alerting contains information about the alerting configuration for the shoot cluster.\n    Networking   (Appears on: ShootSpec)  Networking defines networking parameters for the shoot cluster.\n   Field Description      type  string    Type identifies the type of the networking plugin.\n    providerConfig  ProviderConfig     (Optional) ProviderConfig is the configuration passed to network resource.\n    pods  string    (Optional) Pods is the CIDR of the pod network.\n    nodes  string    (Optional) Nodes is the CIDR of the entire node network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    NginxIngress   (Appears on: Addons)  NginxIngress describes configuration values for the nginx-ingress addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     loadBalancerSourceRanges  []string    (Optional) LoadBalancerSourceRanges is list of whitelist IP sources for NginxIngress\n    config  map[string]string    (Optional) Config contains custom configuration for the nginx-ingress-controller configuration. See https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n    externalTrafficPolicy  Kubernetes core/v1.ServiceExternalTrafficPolicyType     (Optional) ExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service exposing the nginx-ingress. Defaults to Cluster.\n    OIDCConfig   (Appears on: KubeAPIServerConfig)  OIDCConfig contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientAuthentication  OpenIDConnectClientAuthentication     (Optional) ClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n    clientID  string    (Optional) The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    (Optional) The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n    requiredClaims  map[string]string    (Optional) ATTENTION: Only meaningful for Kubernetes \u0026gt;= 1.11 key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \u0026ldquo;sub\u0026rdquo;)\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OIDCConfig)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    secret  string    (Optional) The client Secret for the OpenID Connect client.\n    PlantSpec   (Appears on: Plant)  PlantSpec is the specification of a Plant.\n   Field Description      secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n    PlantStatus   (Appears on: Plant)  PlantStatus is the status of a Plant.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Plant\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Plant. It corresponds to the Plant\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterInfo  ClusterInfo     ClusterInfo is additional computed information about the newly added cluster (Plant)\n    ProjectMember   (Appears on: ProjectSpec)  ProjectMember is a member of a project.\n   Field Description      Subject  Kubernetes rbac/v1.Subject      (Members of Subject are embedded into this type.) Subject is representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    role  string    Role represents the role of this member.\n    ProjectPhase (string alias)\n  (Appears on: ProjectStatus)  ProjectPhase is a label for the condition of a project at the current time.\nProjectSpec   (Appears on: Project)  ProjectSpec is the specification of a Project.\n   Field Description      createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    ProjectStatus   (Appears on: Project)  ProjectStatus holds the most recently observed status of the project.\n   Field Description      observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this project.\n    phase  ProjectPhase     Phase is the current phase of the project.\n    Provider   (Appears on: ShootSpec)  Provider contains provider-specific information that are handed-over to the provider-specific extension controller.\n   Field Description      type  string    Type is the type of the provider.\n    controlPlaneConfig  ProviderConfig     (Optional) ControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete definition in the documentation of your provider extension.\n    infrastructureConfig  ProviderConfig     (Optional) InfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete definition in the documentation of your provider extension.\n    workers  []Worker     Workers is a list of worker groups.\n    ProviderConfig   (Appears on: AdmissionPlugin, CloudProfileSpec, ControllerDeployment, ControllerInstallationStatus, Extension, Networking, Provider, ShootMachineImage, Worker)  ProviderConfig is a workaround for missing OpenAPI functions on runtime.RawExtension struct. https://github.com/kubernetes/kubernetes/issues/55890 https://github.com/kubernetes-sigs/cluster-api/issues/137\n   Field Description      RawExtension  k8s.io/apimachinery/pkg/runtime.RawExtension      (Members of RawExtension are embedded into this type.)     ProxyMode (string alias)\n  (Appears on: KubeProxyConfig)  ProxyMode available in Linux platform: \u0026lsquo;userspace\u0026rsquo; (older, going to be EOL), \u0026lsquo;iptables\u0026rsquo; (newer, faster), \u0026lsquo;ipvs\u0026rsquo; (newest, better in performance and scalability). As of now only \u0026lsquo;iptables\u0026rsquo; and \u0026lsquo;ipvs\u0026rsquo; is supported by Gardener. In Linux platform, if the iptables proxy is selected, regardless of how, but the system\u0026rsquo;s kernel or iptables versions are insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to \u0026lsquo;ipvs\u0026rsquo;, and the fall back path is firstly iptables and then userspace.\nQuotaSpec   (Appears on: Quota)  QuotaSpec is the specification of a Quota.\n   Field Description      clusterLifetimeDays  int    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n    Region   (Appears on: CloudProfileSpec)  Region contains certain properties of a region.\n   Field Description      name  string    Name is a region name.\n    zones  []AvailabilityZone     (Optional) Zones is a list of availability zones in this region.\n    SeedBackup   (Appears on: SeedSpec)  SeedBackup contains the object store configuration for backups for shoot (currently only etcd).\n   Field Description      provider  string    Provider is a provider name.\n    region  string    (Optional) Region is a region name.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.\n    SeedDNS   (Appears on: SeedSpec)  SeedDNS contains DNS-relevant information about this seed cluster.\n   Field Description      ingressDomain  string    IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters.\n    SeedNetworks   (Appears on: SeedSpec)  SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network.\n    pods  string    Pods is the CIDR of the pod network.\n    services  string    Services is the CIDR of the service network.\n    shootDefaults  ShootNetworks     (Optional) ShootDefaults contains the default networks CIDRs for shoots.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    SeedProvider   (Appears on: SeedSpec)  SeedProvider defines the provider type and region for this Seed cluster.\n   Field Description      type  string    Type is the name of the provider.\n    region  string    Region is a name of a region.\n    SeedSpec   (Appears on: Seed)  SeedSpec is the specification of a Seed.\n   Field Description      backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    SeedStatus   (Appears on: Seed)  SeedStatus is the status of a Seed.\n   Field Description      gardener  Gardener     (Optional) Gardener holds information about the Gardener which last acted on the Shoot.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the Kubernetes version of the seed cluster.\n    conditions  []Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the Seed\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    SeedTaint   (Appears on: SeedSpec)  SeedTaint describes a taint on a seed.\n   Field Description      key  string    Key is the taint key to be applied to a seed.\n    value  string    (Optional) Value is the taint value corresponding to the taint key.\n    SeedVolume   (Appears on: SeedSpec)  SeedVolume contains settings for persistentvolumes created in the seed cluster.\n   Field Description      minimumSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinimumSize defines the minimum size that should be used for PVCs in the seed.\n    providers  []SeedVolumeProvider     (Optional) Providers is a list of storage class provisioner types for the seed.\n    SeedVolumeProvider   (Appears on: SeedVolume)  SeedVolumeProvider is a storage class provisioner type.\n   Field Description      purpose  string    Purpose is the purpose of this provider.\n    name  string    Name is the name of the storage class provisioner type.\n    ServiceAccountConfig   (Appears on: KubeAPIServerConfig)  ServiceAccountConfig is the kube-apiserver configuration for service accounts.\n   Field Description      issuer  string    (Optional) Issuer is the identifier of the service account token issuer. The issuer will assert this identifier in \u0026ldquo;iss\u0026rdquo; claim of issued tokens. This value is a string or URI.\n    signingKeySecretName  Kubernetes core/v1.LocalObjectReference     (Optional) SigningKeySecret is a reference to a secret that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the \u0026lsquo;TokenRequest\u0026rsquo; feature gate.)\n    ShootMachineImage   (Appears on: Machine)  ShootMachineImage defines the name and the version of the shoot\u0026rsquo;s machine image in any environment. Has to be defined in the respective CloudProfile.\n   Field Description      name  string    Name is the name of the image.\n    providerConfig  ProviderConfig     (Optional) ProviderConfig is the shoot\u0026rsquo;s individual configuration passed to an extension resource.\n    version  string    Version is the version of the shoot\u0026rsquo;s image.\n    ShootNetworks   (Appears on: SeedNetworks)  ShootNetworks contains the default networks CIDRs for shoots.\n   Field Description      pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    ShootSpec   (Appears on: Shoot)  ShootSpec is the specification of a Shoot.\n   Field Description      addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    ShootStatus   (Appears on: Shoot)  ShootStatus holds the most recently observed status of the Shoot cluster.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Shoots\u0026rsquo;s current state.\n    constraints  []Condition     (Optional) Constraints represents conditions of a Shoot\u0026rsquo;s current state that constraint some operations on it.\n    gardener  Gardener     Gardener holds information about the Gardener which last acted on the Shoot.\n    hibernated  bool    IsHibernated indicates whether the Shoot is currently hibernated.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the Shoot.\n    lastErrors  []LastError     (Optional) LastErrors holds information about the last occurred error(s) during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the Shoot\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    retryCycleStartTime  Kubernetes meta/v1.Time     (Optional) RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation must be retried until we give up).\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n    technicalID  string    TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and basically everything that is related to this particular Shoot.\n    uid  k8s.io/apimachinery/pkg/types.UID     UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters. It is used to compute unique hashes.\n    Volume   (Appears on: Worker)  Volume contains information about the volume type and size.\n   Field Description      type  string    (Optional) Type is the machine type of the worker group.\n    size  string    Size is the size of the root volume.\n    VolumeType   (Appears on: CloudProfileSpec)  VolumeType contains certain properties of a volume type.\n   Field Description      class  string    Class is the class of the volume type.\n    name  string    Name is the name of the volume type.\n    usable  bool    (Optional) Usable defines if the volume type can be used for shoot clusters.\n    Worker   (Appears on: Provider)  Worker is the base definition of a worker group.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n    kubernetes  WorkerKubernetes     (Optional) Kubernetes contains configuration for Kubernetes components related to this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    name  string    Name is the name of the worker group.\n    machine  Machine     Machine contains information about the machine type and image.\n    maximum  int32    Maximum is the maximum number of VMs to create.\n    minimum  int32    Minimum is the minimum number of VMs to create.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    providerConfig  ProviderConfig     (Optional) ProviderConfig is the provider-specific configuration for this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    volume  Volume     (Optional) Volume contains information about the volume type and size.\n    zones  []string    (Optional) Zones is a list of availability zones that are used to evenly distribute this worker pool. Optional as not every provider may support availability zones.\n    WorkerKubernetes   (Appears on: Worker)  WorkerKubernetes contains configuration for Kubernetes components related to this worker pool.\n   Field Description      kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for all kubelets of this worker pool.\n      Generated with gen-crd-api-reference-docs on git commit 79c676930. \n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/create-delete-shoot/",
	"title": "Create / Delete a Shoot cluster",
	"tags": [],
	"description": "Creating / Deleting a Shoot cluster",
	"content": " Create a Shoot Cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f your-shoot-aws.yaml  You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called \u0026ldquo;project\u0026rdquo;) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nDelete a Shoot Cluster In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don\u0026rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/delete shoot johndoe-1 johndoe  ( hack bash script can be found here https://github.com/gardener/gardener/blob/master/hack/delete)\nConfigure a Shoot cluster alert receiver The receiver of the Shoot alerts can be configured by adding the annotation garden.sapcloud.io/operatedBy to the Shoot resource. The value of the annotation has to be a valid mail address.\nThe alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the Shoot resource is annotated with the garden.sapcloud.io/operatedBy annotation and if a SMTP secret exists.\nIf the annotation gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the annotation is added to an existing cluster.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/create-shoot-into-existing-aws-vpc/",
	"title": "Create a Shoot cluster into existing AWS VPC",
	"tags": [],
	"description": "Create a Shoot cluster into existing AWS VPC",
	"content": " Create a Shoot cluster into existing AWS VPC Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC. The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP.\nTL;DR If .spec.cloud.aws.networks.vpc.cidr is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.\nIf .spec.cloud.aws.networks.vpc.id is specified, Gardener will use the existing VPC and respectively won\u0026rsquo;t delete it on Shoot deletion.\n It\u0026rsquo;s not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.\nGardener won\u0026rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.\n 1. Configure AWS CLI The aws configure command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.\n$ aws configure AWS Access Key ID [None]: \u0026lt;ACCESS_KEY_ID\u0026gt; AWS Secret Access Key [None]: \u0026lt;SECRET_ACCESS_KEY\u0026gt; Default region name [None]: \u0026lt;DEFAULT_REGION\u0026gt; Default output format [None]: \u0026lt;DEFAULT_OUTPUT_FORMAT\u0026gt;  2. Create VPC $ aws ec2 create-vpc --cidr-block \u0026lt;cidr-block\u0026gt; { \u0026quot;Vpc\u0026quot;: { \u0026quot;VpcId\u0026quot;: \u0026quot;vpc-ff7bbf86\u0026quot;, \u0026quot;InstanceTenancy\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;Tags\u0026quot;: [], \u0026quot;CidrBlockAssociations\u0026quot;: [ { \u0026quot;AssociationId\u0026quot;: \u0026quot;vpc-cidr-assoc-6e42b505\u0026quot;, \u0026quot;CidrBlock\u0026quot;: \u0026quot;10.0.0.0/16\u0026quot;, \u0026quot;CidrBlockState\u0026quot;: { \u0026quot;State\u0026quot;: \u0026quot;associated\u0026quot; } } ], \u0026quot;Ipv6CidrBlockAssociationSet\u0026quot;: [], \u0026quot;State\u0026quot;: \u0026quot;pending\u0026quot;, \u0026quot;DhcpOptionsId\u0026quot;: \u0026quot;dopt-38f7a057\u0026quot;, \u0026quot;CidrBlock\u0026quot;: \u0026quot;10.0.0.0/16\u0026quot;, \u0026quot;IsDefault\u0026quot;: false } }  3. Create Internet Gateway Gardener also requires that an internet gateway is attached to the VPC. You can create one using:\n$ aws ec2 create-internet-gateway { \u0026quot;InternetGateway\u0026quot;: { \u0026quot;Tags\u0026quot;: [], \u0026quot;InternetGatewayId\u0026quot;: \u0026quot;igw-c0a643a9\u0026quot;, \u0026quot;Attachments\u0026quot;: [] } }  and attach it to the VPC using:\n$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86  4. Create the Shoot Prepare your Shoot manifest (you could check the example manifests). Put your VPC id in .spec.cloud.aws.networks.vpc.id:\n# ... aws: networks: vpc: id: vpc-ff7bbf86 # ...  Apply your Shoot manifest.\n$ kubectl apply -f your-shoot-aws.yaml  Ensure that the Shoot cluster is properly created.\n$ kubectl get shoot $SHOOT_NAME -n $PROJECT_NAME NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE \u0026lt;SHOOT_NAME\u0026gt; aws 1.15.0 aws \u0026lt;SHOOT_DOMAIN\u0026gt; Succeeded 100 True True True True 20m  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/gardener_aws/",
	"title": "Create a kubernetes cluster in AWS with Gardener",
	"tags": [],
	"description": "How to create a Kubernetes Cluster with Gardener in AWS",
	"content": " Introduction Creating a Kubernetes cluster in an AWS Account is easy and the Gardener UI should be self-explanatory/.\nGardener Create a new Project in Gardener Create new Project\nCopy policy from the Gardener AWS Create new policy Create new policy\nCreate a new technical user Create a new technical user\nsave the keys of the user, you will need them later on\nGardener Add AWS Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/gardener_gcp/",
	"title": "Create a kubernetes cluster on GCP with Gardener",
	"tags": [],
	"description": "How to create a Kubernetes Cluster with Gardener on GCP",
	"content": " Introduction Creating a Kubernetes cluster in the GCP Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCheck which roles are required by the Gardener GCP Create a new serviceaccount and assign roles Create a new serviceaccount\nCreate key for the serviceaccount Download the key of the serviceaccount as json save the keys of the user, you will need it later on\nEnable the Google compute API Enable the Google compute API Enable the Google IAM API Enable the Google IAM API Gardener Add GCP Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "
},
{
	"uri": "https://gardener.cloud/curated-links/",
	"title": "Curated Links",
	"tags": [],
	"description": "",
	"content": "Curated Links A curated list of Kubernetes resources and projects    A curated list of awesome kubernetes sources Inspired by @sindresorhus\u0026rsquo; awesome\nSetup  Install Docker for Mac Install Docker for Windows Run a Kubernetes Cluster on your local machine  A place that marks the beginning of a journey  Kubernetes Community Overview and Contributions Guide by Ihor Dvoretskyi An Intro to Google’s Kubernetes and How to Use It by Laura Frank Getting Started on Kubernetes by Rajdeep Dua Kubernetes: The Future of Cloud Hosting by Meteorhacks Kubernetes by Google by Gaston Pantana Key Concepts by Arun Gupta Application Containers: Kubernetes and Docker from Scratch by Keith Tenzer Learn the Kubernetes Key Concepts in 10 Minutes by Omer Dawelbeit Top Reasons Businesses Should Move to Kubernetes Now by Mike Johnston The Children\u0026rsquo;s Illustrated Guide to Kubernetes by Deis :-) The ‘kubectl run’ command by Michael Hausenblas Docker Kubernetes Lab Handbook by Peng Xiao  Interactive Learning Environments Learn Kubernetes using an interactive environment without requiring downloads or configuration\n Interactive Tutorial Katacoda Play with Kubernetes Kubernetes Bootcamp  Massive Open Online Courses / Tutorials List of available free online courses(MOOC) and tutorials\nCourses  Scalable Microservices with Kubernetes at Udacity Introduction to Kubernetes at edX  Tutorials  Kubernetes Tutorials by Kubernetes Team Kubernetes By Example by OpenShift Team Kubernetes Tutorial by Tutorialspoint   Package Managers  Helm KPM   RPC  gRPC Micro  Secret generation and management  Vault auth plugin backend: Kubernetes Vault controller kube-lego k8sec kubernetes-vault kubesec - Secure Secret management  Machine Learning  TensorFlow k8s mxnet-operator - Tools for ML/MXNet on Kubernetes. kubeflow - Machine Learning Toolkit for Kubernetes. seldon-core - Open source framework for deploying machine learning models on Kubernetes  Raspberry Pi Some of the awesome findings and experiments on using Kubernetes with Raspberry Pi.\n Kubecloud Setting up a Kubernetes on ARM cluster Setup Kubernetes on a Raspberry Pi Cluster easily the official way! by Mathias Renner and Lucas Käldström How to Build a Kubernetes Cluster with ARM Raspberry Pi then run .NET Core on OpenFaas by Scott Hanselman  Contributing Contributions are most welcome!\nThis list is just getting started, please contribute to make it super awesome.\n "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/secure-seccomp/",
	"title": "Custom Seccomp profile",
	"tags": [],
	"description": "Custom Seccomp profile",
	"content": " Custom Seccomp profile Context Seccomp (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.\nStarting from Kubernetes v1.3.0 the Seccomp feature is in Alpha. To configure it on a Pod, the following annotations can be used:\n seccomp.security.alpha.kubernetes.io/pod: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to all containers in a Pod. container.seccomp.security.alpha.kubernetes.io/\u0026lt;container-name\u0026gt;: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to \u0026lt;container-name\u0026gt; in a Pod.  More details can be found in the PodSecurityPolicy documentation.\nInstallation of custom profile By default, kubelet loads custom Seccomp profiles from /var/lib/kubelet/seccomp/. There are two ways in which Seccomp profiles can be added to a Node:\n to be baked in the machine image to be added at runtime.  This guide focuses on creating those profiles via a DaemonSet.\nCreate a file called seccomp-profile.yaml with the following content:\napiVersion: v1 kind: ConfigMap metadata: name: seccomp-profile namespace: kube-system data: my-profile.json: | { \u0026quot;defaultAction\u0026quot;: \u0026quot;SCMP_ACT_ALLOW\u0026quot;, \u0026quot;syscalls\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;chmod\u0026quot;, \u0026quot;action\u0026quot;: \u0026quot;SCMP_ACT_ERRNO\u0026quot; } ] }   The policy above is a very simple one and not siutable for complex applications. The default docker profile can be used a reference. Feel free to modify it to your needs.\n Apply the ConfigMap in your cluster:\n$ kubectl apply -f seccomp-profile.yaml configmap/seccomp-profile created  The next steps is to create the DaemonSet seccomp installer. It\u0026rsquo;s going to copy the policy from above in /var/lib/kubelet/seccomp/my-profile.json.\nCreate a file called seccomp-installer.yaml with the following content:\napiVersion: apps/v1 kind: DaemonSet metadata: name: seccomp namespace: kube-system labels: security: seccomp spec: selector: matchLabels: security: seccomp template: metadata: labels: security: seccomp spec: initContainers: - name: installer image: alpine:3.10.0 command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;cp -r -L /seccomp/*.json /host/seccomp/\u0026quot;] volumeMounts: - name: profiles mountPath: /seccomp - name: hostseccomp mountPath: /host/seccomp readOnly: false containers: - name: pause image: k8s.gcr.io/pause:3.1 terminationGracePeriodSeconds: 5 volumes: - name: hostseccomp hostPath: path: /var/lib/kubelet/seccomp - name: profiles configMap: name: seccomp-profile  Create the installer and wait until it\u0026rsquo;s ready on all Nodes:\n$ kubectl apply -f seccomp-installer.yaml daemonset.apps/seccomp-installer created $ kubectl -n kube-system get pods -l security=seccomp NAME READY STATUS RESTARTS AGE seccomp-installer-wjbxq 1/1 Running 0 21s  Create a Pod using custom Seccomp profile Finally we want to create a profile which uses our new Seccomp profile my-profile.json.\nCreate a file called my-seccomp-pod.yaml with the following content:\napiVersion: v1 kind: Pod metadata: name: seccomp-app namespace: default annotations: seccomp.security.alpha.kubernetes.io/pod: \u0026quot;localhost/my-profile.json\u0026quot; # you can specify seccomp profile per container. If you add another profile you can configure # it for a specific container - 'pause' in this case. # container.seccomp.security.alpha.kubernetes.io/pause: \u0026quot;localhost/some-other-profile.json\u0026quot; spec: containers: - name: pause image: k8s.gcr.io/pause:3.1  Create the Pod and see that\u0026rsquo;s running:\n$ kubectl apply -f my-seccomp-pod.yaml pod/seccomp-app created $ kubectl get pod seccomp-app NAME READY STATUS RESTARTS AGE seccomp-app 1/1 Running 0 42s  Throubleshooting If an invalid or not existing profile is used then the Pod will be stuck in ContainerCreating phase:\nbroken-seccomp-pod.yaml:\napiVersion: v1 kind: Pod metadata: name: broken-seccomp namespace: default annotations: seccomp.security.alpha.kubernetes.io/pod: \u0026quot;localhost/not-existing-profile.json\u0026quot; spec: containers: - name: pause image: k8s.gcr.io/pause:3.1  $ kubectl apply -f broken-seccomp-pod.yaml pod/broken-seccomp created $ kubectl get pod broken-seccomp NAME READY STATUS RESTARTS AGE broken-seccomp 1/1 ContainerCreating 0 2m $ kubectl describe pod broken-seccomp Name: broken-seccomp Namespace: default .... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned kube-system/broken-seccomp to docker-desktop Warning FailedCreatePodSandBox 4s (x2 over 18s) kubelet, docker-desktop Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod \u0026quot;broken-seccomp\u0026quot;: failed to generate sandbox security options for sandbox \u0026quot;broken-seccomp\u0026quot;: failed to generate seccomp security options for container: cannot load seccomp profile \u0026quot;/var/lib/kubelet/seccomp/not-existing-profile.json\u0026quot;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory  Further reading  https://en.wikipedia.org/wiki/Seccomp https://docs.docker.com/engine/security/seccomp https://lwn.net/Articles/656307/ http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf  "
},
{
	"uri": "https://gardener.cloud/components/dns-cm/",
	"title": "DNS Controller Manager",
	"tags": [],
	"description": "",
	"content": " External DNS Management The main artefact of this project is the DNS controller manager for managing DNS records, also nicknamed as the Gardener \u0026ldquo;DNS Controller\u0026rdquo;.\nIt contains provisioning controllers for creating DNS records in one of the DNS cloud services - Amazon Route53, - Google CloudDNS, - AliCloud DNS, - Azure DNS, or - OpenStack Designate,\nand source controllers for services and ingresses to create DNS entries by annotations.\nThe configuration for the external DNS service is specified in a custom resource DNSProvider. Multiple DNSProvider can be used simultaneously and changed without restarting the DNS controller.\nDNS records are either created directly for a corresponding custom resource DNSEntry or by annotating a service or ingress.\nFor a detailed explanation of the model, see section The Model.\nFor extending or adapting this project with your own source or provisioning controllers, see section Extensions\nQuick start To install the DNS controller manager in your Kubernetes cluster, follow these steps.\n Prerequisites\n Check out or download the project to get a copy of the Helm charts. It is recommended to check out the tag of the last release, so that Helm values reference the newest released container image for the deployment.\n Make sure, that you have installed Helm client (helm) locally and Helm server (tiller) on the Kubernetes cluster. See e.g. Helm installation for more details.\n  Install the DNS controller manager\nAs multiple Gardener DNS controllers can act on the same DNS Hosted Zone concurrently, each instance needs an owner identifier. Therefore choose an identifier sufficiently unique across these instances.\nThen install the DNS controller manager with\nhelm install charts/external-dns-management --name dns-controller --namespace=\u0026lt;my-namespace\u0026gt; --set configuration.identifier=\u0026lt;my-identifier\u0026gt;  This will use the default configuration with all source and provisioning controllers enabled. The complete set of configuration variables can be found in charts/external-dns-management/values.yaml. Their meaning is explained by their corresponding command line options in section Using the DNS controller manager\nBy default, the DNS controller looks for custom resources in all namespaces. The choosen namespace is only relevant for the deployment itself.\n Create a DNSProvider\n  To specify a DNS provider, you need to create a custom resource DNSProvider and a secret containing the credentials for your account at the provider. E.g. if you want to use AWS Route53, create a secret and provider with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-credentials namespace: default type: Opaque data: # replace '...' with values encoded as base64 # see https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html AWS_ACCESS_KEY_ID: ... AWS_SECRET_ACCESS_KEY: ... EOF  and\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSProvider metadata: name: aws namespace: default spec: type: aws-route53 secretRef: name: aws-credentials domains: include: # this must be replaced with a (sub)domain of the hosted zone - my.own.domain.com EOF  Check the successful creation with\nkubectl get dnspr  You should see something like\n NAME TYPE STATUS AGE aws aws-route53 Ready 12s   Create a DNSEntry  Create an DNS entry with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSEntry metadata: name: mydnsentry namespace: default spec: dnsName: \u0026quot;myentry.my-own-domain.com\u0026quot; ttl: 600 targets: - 1.2.3.4 EOF  Check the status of the DNS entry with\n```bash kubectl get dnsentry ``` You should see something like ``` NAME DNS TYPE PROVIDER STATUS AGE mydnsentry myentry.my-own-domain.com aws-route53 default/aws Ready 24s ``` As soon as the status of the entry is `Ready`, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, it may take up to a few minutes before the domain name can be resolved.   Wait for/check DNS record  To check the DNS resolution, use nslookup or dig.\nnslookup myentry.my-own-domain.com  or with dig\n# or with dig dig +short myentry.my-own-domain.com  Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)\ndig @8.8.8.8 +short myentry.my-own-domain.com  For more examples about the custom resources and the annotations for services and ingresses see the examples directory.\nThe Model This project provides a flexible model allowing to add DNS source objects and DNS provisioning environments by adding new independent controllers.\nThere is no single DNS controller anymore. The decoupling between the handling of DNS source objects, like ingresses or services, and the provisioning of DNS entries in an external DNS provider like Route53 or CloudDNS is achieved by introducing a new custom resource DNSEntry.\nThese objects can either be explicitly created to request dedicated DNS entries, or they are managed based on other resources like ingresses or services. For the latter dedicated DNS Source Controllers are used. There might be any number of such source controllers. They do not need to know anything about the various DNS environments. Their task is to figure out which DNS entries are required in their realm and manage appropriate DNSEntry objects. From these objects they can also read the provisioning status and report it back to the original source.\nProvisioning of DNS entries in external DNS providers is done by DNS Provisioning Controllers. They don\u0026rsquo;t need to know anything about the various DNS source objects. They watch DNSEntry objects and check whether they are responsible for such an object. If a provisioning controller feels responsible for an entry it manages the corresponding settings in the external DNS environment and reports the provisioning status back to the corresponding DNSEntry object.\nTo do this a provisioning controller is responsible for a dedicated environment (for example Route53). For every such environment the controller uses a dedicated type key. This key is used to look for DNSProvider objects. There might be multiple such objects per environment, specifying the credentials needed to access different external accounts. These accounts are then scanned for DNS zones and domain names they support. This information is then used to dynamically assign DNSEntry objects to dedicated DNSProvider objects. If such an assignment can be done by a provisioning controller then it is responsible for this entry and manages the corresponding entries in the external environment. DNSProvider objects can specify explicit inclusion and exclusion sets of domain names and/or DNS zone identifiers to override the scanning results of the account.\nOwner Identifiers Every DNS Provisioning Controller is responsible for a set of Owner Identifiers. DNS records in an external DNS environment are attached to such an identifier. This is used to identify the records in the DNS environment managed by a dedicated controller (manager). Every controller manager hosting DNS Provisioning Controllers offers an option to specify a default identifier. Additionally there might be dedicated DNSOwner objects that enable or disable additional owner ids.\nEvery DNSEntry object may specify a dedicated owner that is used to tag the records in the DNS environment. A DNS provisioning controller only acts of DNS entries it is responsible for. Other resources in the external DNS environment are not touched at all.\nThis way it is possbible to - identify records in the external DNS management environment that are managed by the actual controller instance - distinguish different DNS source environments sharing the same hosted zones in the external management environment - cleanup unused entries, even if the whole resource set is already gone - move the responsibility for dedicated sets of DNS entries among different kubernetes clusters or DNS source environments running different DNS Provisioning Controller without loosing the entries during the migration process.\nIf multiple DNS controller instances have access to the same DNS zones, it is very important, that every instance uses a unique owner identifier! Otherwise the cleanup of stale DNS record will delete entries created by another instance if they use the same identifier.\nDNS Classes Multiple sets of controllers of the DNS ecosystem can run in parallel in a kubernetes cluster working on different object set. They are separated by using different DNS Classes. Adding a DNS class annotation to an object of the DNS ecosytems assigns this object to such a dedicated set of DNS controllers. This way it is possible to maintain clearly separated set of DNS objects in a single kubernetes cluster.\nUsing the DNS controller manager The controllers to run can be selected with the --controllers option. Here the following controller groups can be used: - dnssources: all DNS Source Controllers. It includes the conrollers - ingress-dns: handle DNS annotations for the standard kubernetes ingress resource - service-dns: handle DNS annotations for the standard kubernetes service resource\n dnscontrollers: all DNS Provisioning Controllers. It includes the controllers\n alicloud-dns: aws-route53: azure-dns: google-clouddns: openstack-designate:   all: (default) all controllers\n  It is also possible to list dedicated controllers by their name.\nIf a DNS Provisioning Controller is enabled it is important to specify a unique controller identity using the --identifier option. This identifier is stored in the DNS system to identify the DNS entries managed by a dedicated controller. There should never be two DNS controllers with the same identifier running at the same time for the same DNS domains/accounts.\nHere is the complete list of options provided:\nUsage: dns-controller-manager [flags] Flags: --alicloud-dns.cache-dir string Directory to store zone caches (for reload after restart) --alicloud-dns.cache-ttl int Time-to-live for provider hosted zone cache --alicloud-dns.default.pool.size int Worker pool size for pool default of controller alicloud-dns (default: 2) --alicloud-dns.disable-zone-state-caching disable use of cached dns zone state on changes --alicloud-dns.dns-class string Identifier used to differentiate responsible controllers for entries --alicloud-dns.dns-delay duration delay between two dns reconcilations --alicloud-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller alicloud-dns (default: 15m0s) --alicloud-dns.dns.pool.size int Worker pool size for pool dns of controller alicloud-dns (default: 1) --alicloud-dns.dry-run just check, don't modify --alicloud-dns.identifier string Identifier used to mark DNS entries --alicloud-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller alicloud-dns (default: 1) --alicloud-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller alicloud-dns (default: 10m0s) --alicloud-dns.providers.pool.size int Worker pool size for pool providers of controller alicloud-dns (default: 2) --alicloud-dns.reschedule-delay duration reschedule delay after losing provider --alicloud-dns.secrets.pool.size int Worker pool size for pool secrets of controller alicloud-dns (default: 2) --alicloud-dns.setup int number of processors for controller setup --alicloud-dns.ttl int Default time-to-live for DNS entries --aws-route53.cache-dir string Directory to store zone caches (for reload after restart) --aws-route53.cache-ttl int Time-to-live for provider hosted zone cache --aws-route53.default.pool.size int Worker pool size for pool default of controller aws-route53 (default: 2) --aws-route53.disable-zone-state-caching disable use of cached dns zone state on changes --aws-route53.dns-class string Identifier used to differentiate responsible controllers for entries --aws-route53.dns-delay duration delay between two dns reconcilations --aws-route53.dns.pool.resync-period duration Period for resynchronization of pool dns of controller aws-route53 (default: 15m0s) --aws-route53.dns.pool.size int Worker pool size for pool dns of controller aws-route53 (default: 1) --aws-route53.dry-run just check, don't modify --aws-route53.identifier string Identifier used to mark DNS entries --aws-route53.ownerids.pool.size int Worker pool size for pool ownerids of controller aws-route53 (default: 1) --aws-route53.providers.pool.resync-period duration Period for resynchronization of pool providers of controller aws-route53 (default: 10m0s) --aws-route53.providers.pool.size int Worker pool size for pool providers of controller aws-route53 (default: 2) --aws-route53.reschedule-delay duration reschedule delay after losing provider --aws-route53.secrets.pool.size int Worker pool size for pool secrets of controller aws-route53 (default: 2) --aws-route53.setup int number of processors for controller setup --aws-route53.ttl int Default time-to-live for DNS entries --azure-dns.cache-dir string Directory to store zone caches (for reload after restart) --azure-dns.cache-ttl int Time-to-live for provider hosted zone cache --azure-dns.default.pool.size int Worker pool size for pool default of controller azure-dns (default: 2) --azure-dns.disable-zone-state-caching disable use of cached dns zone state on changes --azure-dns.dns-class string Identifier used to differentiate responsible controllers for entries --azure-dns.dns-delay duration delay between two dns reconcilations --azure-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller azure-dns (default: 15m0s) --azure-dns.dns.pool.size int Worker pool size for pool dns of controller azure-dns (default: 1) --azure-dns.dry-run just check, don't modify --azure-dns.identifier string Identifier used to mark DNS entries --azure-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller azure-dns (default: 1) --azure-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller azure-dns (default: 10m0s) --azure-dns.providers.pool.size int Worker pool size for pool providers of controller azure-dns (default: 2) --azure-dns.reschedule-delay duration reschedule delay after losing provider --azure-dns.secrets.pool.size int Worker pool size for pool secrets of controller azure-dns (default: 2) --azure-dns.setup int number of processors for controller setup --azure-dns.ttl int Default time-to-live for DNS entries --cache-dir string default for all controller \u0026quot;cache-dir\u0026quot; options --cache-ttl int default for all controller \u0026quot;cache-ttl\u0026quot; options -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,source,target,all) (default \u0026quot;all\u0026quot;) --cpuprofile string set file for cpu profiling --disable-namespace-restriction disable access restriction for namespace local access only --disable-zone-state-caching default for all controller \u0026quot;disable-zone-state-caching\u0026quot; options --dns-class string default for all controller \u0026quot;dns-class\u0026quot; options --dns-delay duration default for all controller \u0026quot;dns-delay\u0026quot; options --dns-target-class string default for all controller \u0026quot;dns-target-class\u0026quot; options --dnsentry-source.default.pool.resync-period duration Period for resynchronization of pool default of controller dnsentry-source (default: 2m0s) --dnsentry-source.default.pool.size int Worker pool size for pool default of controller dnsentry-source (default: 2) --dnsentry-source.dns-class string identifier used to differentiate responsible controllers for entries --dnsentry-source.dns-target-class string identifier used to differentiate responsible dns controllers for target entries --dnsentry-source.exclude-domains stringArray excluded domains --dnsentry-source.key string selecting key for annotation --dnsentry-source.target-creator-label-name string label name to store the creator for generated DNS entries --dnsentry-source.target-creator-label-value string label value for creator label --dnsentry-source.target-name-prefix string name prefix in target namespace for cross cluster generation --dnsentry-source.target-namespace string target namespace for cross cluster generation --dnsentry-source.target-owner-id string owner id to use for generated DNS entries --dnsentry-source.target-realms string realm(s) to use for generated DNS entries --dnsentry-source.target-set-ignore-owners mark generated DNS entries to omit owner based access control --dnsentry-source.targets.pool.size int Worker pool size for pool targets of controller dnsentry-source (default: 2) --dry-run default for all controller \u0026quot;dry-run\u0026quot; options --exclude-domains stringArray default for all controller \u0026quot;exclude-domains\u0026quot; options --google-clouddns.cache-dir string Directory to store zone caches (for reload after restart) --google-clouddns.cache-ttl int Time-to-live for provider hosted zone cache --google-clouddns.default.pool.size int Worker pool size for pool default of controller google-clouddns (default: 2) --google-clouddns.disable-zone-state-caching disable use of cached dns zone state on changes --google-clouddns.dns-class string Identifier used to differentiate responsible controllers for entries --google-clouddns.dns-delay duration delay between two dns reconcilations --google-clouddns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller google-clouddns (default: 15m0s) --google-clouddns.dns.pool.size int Worker pool size for pool dns of controller google-clouddns (default: 1) --google-clouddns.dry-run just check, don't modify --google-clouddns.identifier string Identifier used to mark DNS entries --google-clouddns.ownerids.pool.size int Worker pool size for pool ownerids of controller google-clouddns (default: 1) --google-clouddns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller google-clouddns (default: 10m0s) --google-clouddns.providers.pool.size int Worker pool size for pool providers of controller google-clouddns (default: 2) --google-clouddns.reschedule-delay duration reschedule delay after losing provider --google-clouddns.secrets.pool.size int Worker pool size for pool secrets of controller google-clouddns (default: 2) --google-clouddns.setup int number of processors for controller setup --google-clouddns.ttl int Default time-to-live for DNS entries --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for dns-controller-manager --identifier string default for all controller \u0026quot;identifier\u0026quot; options --ingress-dns.default.pool.resync-period duration Period for resynchronization of pool default of controller ingress-dns (default: 2m0s) --ingress-dns.default.pool.size int Worker pool size for pool default of controller ingress-dns (default: 2) --ingress-dns.dns-class string identifier used to differentiate responsible controllers for entries --ingress-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries --ingress-dns.exclude-domains stringArray excluded domains --ingress-dns.key string selecting key for annotation --ingress-dns.target-creator-label-name string label name to store the creator for generated DNS entries --ingress-dns.target-creator-label-value string label value for creator label --ingress-dns.target-name-prefix string name prefix in target namespace for cross cluster generation --ingress-dns.target-namespace string target namespace for cross cluster generation --ingress-dns.target-owner-id string owner id to use for generated DNS entries --ingress-dns.target-realms string realm(s) to use for generated DNS entries --ingress-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control --ingress-dns.targets.pool.size int Worker pool size for pool targets of controller ingress-dns (default: 2) --key string default for all controller \u0026quot;key\u0026quot; options --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default -D, --log-level string logrus log level --name string name used for controller manager --namespace string namespace for lease -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --openstack-designate.cache-dir string Directory to store zone caches (for reload after restart) --openstack-designate.cache-ttl int Time-to-live for provider hosted zone cache --openstack-designate.default.pool.size int Worker pool size for pool default of controller openstack-designate (default: 2) --openstack-designate.disable-zone-state-caching disable use of cached dns zone state on changes --openstack-designate.dns-class string Identifier used to differentiate responsible controllers for entries --openstack-designate.dns-delay duration delay between two dns reconcilations --openstack-designate.dns.pool.resync-period duration Period for resynchronization of pool dns of controller openstack-designate (default: 15m0s) --openstack-designate.dns.pool.size int Worker pool size for pool dns of controller openstack-designate (default: 1) --openstack-designate.dry-run just check, don't modify --openstack-designate.identifier string Identifier used to mark DNS entries --openstack-designate.ownerids.pool.size int Worker pool size for pool ownerids of controller openstack-designate (default: 1) --openstack-designate.providers.pool.resync-period duration Period for resynchronization of pool providers of controller openstack-designate (default: 10m0s) --openstack-designate.providers.pool.size int Worker pool size for pool providers of controller openstack-designate (default: 2) --openstack-designate.reschedule-delay duration reschedule delay after losing provider --openstack-designate.secrets.pool.size int Worker pool size for pool secrets of controller openstack-designate (default: 2) --openstack-designate.setup int number of processors for controller setup --openstack-designate.ttl int Default time-to-live for DNS entries --plugin-dir string directory containing go plugins --pool.resync-period duration default for all controller \u0026quot;pool.resync-period\u0026quot; options --pool.size int default for all controller \u0026quot;pool.size\u0026quot; options --providers string cluster to look for provider objects --providers.disable-deploy-crds disable deployment of required crds for cluster provider --providers.id string id for cluster provider --reschedule-delay duration default for all controller \u0026quot;reschedule-delay\u0026quot; options --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-dns.default.pool.resync-period duration Period for resynchronization of pool default of controller service-dns (default: 2m0s) --service-dns.default.pool.size int Worker pool size for pool default of controller service-dns (default: 2) --service-dns.dns-class string identifier used to differentiate responsible controllers for entries --service-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries --service-dns.exclude-domains stringArray excluded domains --service-dns.key string selecting key for annotation --service-dns.target-creator-label-name string label name to store the creator for generated DNS entries --service-dns.target-creator-label-value string label value for creator label --service-dns.target-name-prefix string name prefix in target namespace for cross cluster generation --service-dns.target-namespace string target namespace for cross cluster generation --service-dns.target-owner-id string owner id to use for generated DNS entries --service-dns.target-realms string realm(s) to use for generated DNS entries --service-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control --service-dns.targets.pool.size int Worker pool size for pool targets of controller service-dns (default: 2) --setup int default for all controller \u0026quot;setup\u0026quot; options --target string target cluster for dns requests --target-creator-label-name string default for all controller \u0026quot;target-creator-label-name\u0026quot; options --target-creator-label-value string default for all controller \u0026quot;target-creator-label-value\u0026quot; options --target-name-prefix string default for all controller \u0026quot;target-name-prefix\u0026quot; options --target-namespace string default for all controller \u0026quot;target-namespace\u0026quot; options --target-owner-id string default for all controller \u0026quot;target-owner-id\u0026quot; options --target-realms string default for all controller \u0026quot;target-realms\u0026quot; options --target-set-ignore-owners default for all controller \u0026quot;target-set-ignore-owners\u0026quot; options --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --ttl int default for all controller \u0026quot;ttl\u0026quot; options  Extensions This project can also be used as library to implement own source and provisioning controllers.\nHow to implement Source Controllers Based on the provided source controller library a source controller must implement the source.DNSSource interface and provide an appropriate creator function.\nA source controller can be implemented following this example:\npackage service import ( \u0026quot;github.com/gardener/controller-manager-library/pkg/resources\u0026quot; \u0026quot;github.com/gardener/external-dns-management/pkg/dns/source\u0026quot; ) var _MAIN_RESOURCE = resources.NewGroupKind(\u0026quot;core\u0026quot;, \u0026quot;Service\u0026quot;) func init() { source.DNSSourceController(source.NewDNSSouceTypeForExtractor(\u0026quot;service-dns\u0026quot;, _MAIN_RESOURCE, GetTargets),nil). FinalizerDomain(\u0026quot;dns.gardener.cloud\u0026quot;). MustRegister(source.CONTROLLER_GROUP_DNS_SOURCES) }  Complete examples can be found in the sub packages of pkg/controller/source.\nHow to implement Provisioning Controllers Provisioning controllers can be implemented based on the provisioning controller library in this repository and must implement the provider.DNSHandlerFactory interface. This factory returns implementations of the provider.DNSHandler interface that does the effective work for a dedicated set of hosted zones.\nThese factories can be embedded into a final controller manager (the runnable instance) in several ways:\n The factory can be used to create a dedicated controller. This controller can then be embedded into a controller manager, either in its own controller manger or together with other controllers. The factory can be added to a compound factory, able to handle multiple infrastructures. This one can then be used to create a dedicated controller, again.  Embedding a Factory into a Controller A provisioning controller can be implemented following this example:\npackage controller import ( \u0026quot;github.com/gardener/external-dns-management/pkg/dns/provider\u0026quot; ) const CONTROLLER_NAME = \u0026quot;route53-dns-controller\u0026quot; func init() { provider.DNSController(CONTROLLER_NAME, \u0026amp;Factory{}). FinalizerDomain(\u0026quot;dns.gardener.cloud\u0026quot;). MustRegister(provider.CONTROLLER_GROUP_DNS_CONTROLLERS) }  This controller can be embedded into a controller manager just by using an anonymous import of the controller package in the main package of a dedicated controller manager.\nComplete examples are available in the sub packages of pkg/controller/provider. They also show a typical set of implementation structures that help to structure the implementation of such controllers.\nThe provider implemented in this project always follow the same structure: - the provider package contains the provider code - the factory source file registers the factory at a default compound factory - it contains a sub package controller, which contains the embedding of the factory into a dedicated controller\nEmbedding a Factory into a Compound Factory A provisioning controller based on a Compound Factory can be extended by a new provider factory by registering this factory at the compound factory. This could be done, for example, by using the default compound factory provided in package pkg/controller/provider/compound as shown here, where NewHandler is a function creating a dedicated handler for a dedicated provider type:\npackage aws import ( \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/compound\u0026quot; \u0026quot;github.com/gardener/external-dns-management/pkg/dns/provider\u0026quot; ) const TYPE_CODE = \u0026quot;aws-route53\u0026quot; var Factory = provider.NewDNSHandlerFactory(TYPE_CODE, NewHandler) func init() { compound.MustRegister(Factory) }  The compound factory is then again embedded into a provisioning controller as shown in the previous section (see the controllersub package).\nSetting Up a Controller Manager One or multiple controller packages can be bundled into a controller manager, by implementing a main package like this:\npackage main import ( \u0026quot;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026quot; _ \u0026quot;github.com/\u0026lt;your controller package\u0026gt;\u0026quot; ... ) func main() { controllermanager.Start(\u0026quot;my-dns-controller-manager\u0026quot;, \u0026quot;dns controller manager\u0026quot;, \u0026quot;some description\u0026quot;) }  Using the standard Compound Provisioning Controller If the standard Compound Provisioning Controller should be used it is required to additionally add the anonymous imports for the providers intended to be embedded into the compound factory like this:\n Example Coding\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/\u0026lt;your provider\u0026gt;\u0026quot; ... ) func main() { controllermanager.Start(\u0026quot;dns-controller-manager\u0026quot;, \u0026quot;dns controller manager\u0026quot;, \u0026quot;nothing\u0026quot;) }  \nMultiple Cluster Support The controller implementations provided in this project are prepared to work with multiple clusters by using the features of the used controller manager library.\nThe DNS Source Controllers support two clusters: - the default cluster is used to scan for source objects - the logical cluster target is used to maintain the DNSEnry objects.\nThe DNS Provisioning Controllers also support two clusters: - the default cluster is used to scan for DNSEntry objects. It is mapped to the logical cluster target - the logical cluster provider is used to look to the DNSProvider objects and their related secrets.\nIf those controller types should be combined in a single controller manager, it can be configured to support three potential clusters with the source objects, the one for the entry objects and the one with provider objects using cluster mappings.\nThis is shown in a complete example using the dns source controllers, the compound provisioning controller configured to support all the included DNS provider type factories:\n Example Coding\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026quot; \u0026quot;github.com/gardener/controller-manager-library/pkg/controllermanager/cluster\u0026quot; \u0026quot;github.com/gardener/controller-manager-library/pkg/controllermanager/controller\u0026quot; \u0026quot;github.com/gardener/controller-manager-library/pkg/controllermanager/controller/mappings\u0026quot; dnsprovider \u0026quot;github.com/gardener/external-dns-management/pkg/dns/provider\u0026quot; dnssource \u0026quot;github.com/gardener/external-dns-management/pkg/dns/source\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/alicloud\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/aws\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/azure\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/google\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/provider/openstack\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/source/ingress\u0026quot; _ \u0026quot;github.com/gardener/external-dns-management/pkg/controller/source/service\u0026quot; ) func init() { // target cluster already defined in dns source controller package cluster.Configure( dnsprovider.PROVIDER_CLUSTER, \u0026quot;providers\u0026quot;, \u0026quot;cluster to look for provider objects\u0026quot;, ).Fallback(dnssource.TARGET_CLUSTER) mappings.ForControllerGroup(dnsprovider.CONTROLLER_GROUP_DNS_CONTROLLERS). Map(controller.CLUSTER_MAIN, dnssource.TARGET_CLUSTER).MustRegister() } func main() { controllermanager.Start(\u0026quot;dns-controller-manager\u0026quot;, \u0026quot;dns controller manager\u0026quot;, \u0026quot;nothing\u0026quot;) }  \nThose clusters can the be separated by registering their names together with command line option names. These can be used to specify different kubeconfig files for those clusters.\nBy default all logical clusters are mapped to the default physical cluster specified via --kubeconfig or default cluster access.\nIf multiple physical clusters are defined they can be specified by a corresponding cluster option defining the kubeconfig file used to access this cluster. If no such option is specified the default is used.\nTherefore, even if the configuration is prepared for multiple clusters, such a controller manager can easily work on a single cluster if no special options are given on the command line.\nWhy not using the community external-dns solution? Some of the reasons are context-specific, i.e. relate to Gardener\u0026rsquo;s highly dynamic requirements.\n Custom resource for DNS entries  DNS entries are explicitly specified as custom resources. As an important side effect, each DNS entry provides an own status. Simply by querying the Kubernetes API, a client can check if a requested DNS entry has been successfully added to the DNS backend, or if an update has already been deployed, or if not to reason about the cause. It also opens for easy extensibility, as DNS entries can be created directly via the Kubernetes API. And it simplifies Day 2 operations, e.g. automatic cleanup of unused entries if a DNS provider is deleted.\n Management of multiple DNS providers  The Gardener DNS controller uses a custom resource DNSProvider to dynamically manage the backend DNS services. While with external-dns you have to specify the single provider during startup, in the Gardener DNS controller you can add/update/delete providers during runtime with different credentials and/or backends. This is important for a multi-tenant environment as in Gardener, where users can bring their own accounts.\nA DNS provider can also restrict its actions on subset of the DNS domains (includes and excludes) for which the credentials are capable to edit.\nEach provider can define a separate “owner” identifier, to differentiate DNS entries in the same DNS zone from different providers.\n Multi cluster support  The Gardener DNS controller distinguish three different logical Kubernetes clusters: Source cluster, target cluster and runtime cluster. The source cluster is monitored by the DNS source controllers for annotations on ingress and service resources. These controllers then create DNS entries in the target cluster. DNS entries in the target cluster are then reconciliated/synchronized with the corresponding DNS backend service by the provider controller. The runtime cluster is the cluster the DNS controller runs on. For example, this enables needed flexibility in the Gardener deployment. The DNS controller runs on the seed cluster. This is also the target cluster. DNS providers and entries resources are created in the corresponding namespace of the shoot control plane, while the source cluster is the shoot cluster itself.\n Optimizations for handling hundreds of DNS entries  Some DNS backend services are restricted on the API calls per second (e.g. the AWS Route 53 API). To manage hundreds of DNS entries it is important to minimize the number of API calls. The Gardener DNS controller heavily makes usage of caches and batch processing for this reason.\n"
},
{
	"uri": "https://gardener.cloud/components/dashboard/",
	"title": "Dashboard",
	"tags": [],
	"description": "",
	"content": " Gardener Dashboard  \nDemo Development Setup Install Install all dependencies\nyarn  Configuration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport: 3030 logLevel: debug logFormat: text apiServerUrl: https://minkube # garden cluster kube-apiserver url sessionSecret: c2VjcmV0 # symetric key used for encryption oidc: issuer: https://minikube:32001 client_id: dashboard client_secret: c2VjcmV0 # oauth client secret redirect_uri: http://localhost:8080/auth/callback scope: 'openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl' clockTolerance: 15 frontend: dashboardUrl: pathname: /api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/ defaultHibernationSchedule: evaluation: - start: 00 17 * * 1,2,3,4,5 development: - start: 00 17 * * 1,2,3,4,5 end: 00 08 * * 1,2,3,4,5 production: ~  Run locally (during development) Concurrently run the backend server (port 3030) and the frontend server (port 8080) both with hot reload enabled.\nyarn serve  All request to /api, /auth and /config.json will be proxied by default to the backend server.\nBuild Build docker image locally.\nmake build  Push Push docker image to Google Container Registry.\nmake push  This command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener is_active: true name: gardener properties: core: account: john.doe@example.org project: johndoe-1008  People The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2019 The Gardener Authors\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/20_dependencies/",
	"title": "Dependencies",
	"tags": [],
	"description": "",
	"content": " Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\nmake test # runs tests make verify # runs static code checks and test  There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\nmake test-cov open gardener.coverage.html make test-clean  Dependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u0026lt;PACKAGE\u0026gt;@\u0026lt;VERSION\u0026gt; or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy. go mod vendor resets the main module\u0026rsquo;s vendor directory to include all packages needed to build and test all the main module\u0026rsquo;s packages. It does not include test code for vendored packages. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module\u0026rsquo;s packages and dependencies, and it removes unused modules that don\u0026rsquo;t provide any relevant packages.\nmake revendor   The dependencies are installed into the vendor folder which should be added to the VCS.  :warning: Make sure that you test the code after you have updated the dependencies!\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/30_deploy_seed_into_aks/",
	"title": "Deploy into AKS",
	"tags": [],
	"description": "",
	"content": " Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS: - Deploying the Gardener and a Seed into an AKS cluster - High Level Overview - Prerequisites - AWS credentials for Route 53 Hosted Zone - Deploy AKS cluster - Initialize Helm on the Cluster - Deploy stable/nginx-ingress chart to AKS - Create wildcard DNS record for the ingress - Create Azure Service Principle to get Azure credentials - Install gardenctl - Install Gardener - Create garden namespace - Deploy etcd - Deploy Gardener Helm Chart - Create a CloudProfile - Define Seed cluster in Gardener - Create the Seed resource definition with its Secret - Create a Shoot cluster - Create a Project (namespace) for Shoots - Create a SecretBinding and related Secret - Create the Shoot resource - Cluster Resources After Shoot is Created - Troubleshooting Shoot Creation Issues - Access Shoot cluster - Delete Shoot cluster\nPrerequisites Summary of prerequisites: - An Azure AKS cluster with: - Helm initialized, - an ingress controller deployed, - a wildcard DNS record pointing the ingress, - az command line client configured for Azure subscription, - An Azure service principle to provide Azure credentials to Gardener, - A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone, - aws command line client configured for this account, - gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig\nNote: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here  Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin  Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller  Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress  Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text) INGRESS_DOMAIN=\u0026quot;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026quot; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template '{{(index .status.loadBalancer.ingress 0).ip}}') awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026quot;*.$INGRESS_DOMAIN\u0026quot; \\ value=$LB_IP \\ type=A \\ ttl=300  Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026quot;Contributor\u0026quot; Retrying role assignment creation: 1/36 { \u0026quot;appId\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot;, #az_client_id \u0026quot;displayName\u0026quot;: \u0026quot;azure-cli-2018-05-23-16-15-49\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;http://azure-cli-2018-05-23-16-15-49\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot;, #az_client_secret \u0026quot;tenant\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot; #az_tenant_id }  Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here  Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config  Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml  Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls=  Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m  Deploy Gardener Helm Chart Check (current releases)[https://github.com/gardener/gardener/releases] and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1  gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file: * global.controller.internalDomain.hostedZoneID * global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here * global.controller.internalDomain.credentials * global.controller.internalDomain.secretAccessKey\nHOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} \\ --query 'HostedZone.Name' \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026quot;garden-1.${HOSTED_ZONE_DOMAIN}\u0026quot; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026quot;GARDENER_RELEASE is missing\u0026quot;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026quot;GARDENER_RELEASE is missing\u0026quot;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF  After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026quot;GARDENER_RELEASE is missing\u0026quot;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml  Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026quot;Failed to list *v1beta1...\u0026quot; messages  Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml  Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml  Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment: * data.subscriptionID: you can learn this one with az account show * data.tenantID: from az ad sp create-for-rbac output as you can see above * data.clientID: from az ad sp create-for-rbac output as you can see above * data.clientSecret: from az ad sp create-for-rbac output as you can see above * data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)\nNote: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r '.[] | select(.isDefault == true) | .id') TENANT_ID=$(az account show -o tsv --query 'tenantId') KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026quot;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(uuid-of-tenant)@$(echo \u0026quot;$TENANT_ID\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(uuid-of-client)@$(echo \u0026quot;${CLIENT_ID:?\u0026quot;CLIENT_ID is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(client-secret)@$(echo \u0026quot;${CLIENT_SECRET:?\u0026quot;CLIENT_SECRET is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026quot;$KUBECONFIG_FOR_SEED_CLUSTER\u0026quot; | base64 -w 0)@\u0026quot; \\ example/40-secret-seed-azure.yaml  After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml  Before creating Seed, we need to update the example/50-seed-azure.yaml file and update: * spec.networks: IP ranges used in your AKS cluster. * spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. * spec.cloud.region: eastus (the region of the existing AKS cluster)\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text) INGRESS_DOMAIN=\u0026quot;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026quot; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r '.[] | .subnets[] | .addressPrefix') POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026quot;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026quot; \\ -e \u0026quot;s/region: westeurope/region: eastus/\u0026quot; \\ -e \u0026quot;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026quot; \\ -e \u0026quot;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026quot; \\ -e \u0026quot;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026quot; \\ example/50-seed-azure.yaml  Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml  Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure  If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026quot;conditions\u0026quot;: [ { \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2018-05-31T14:56:49Z\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;all checks passed\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;Passed\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Available\u0026quot; } ] }  Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml  You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev  Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment: * data.subscriptionID: you can learn this one with az account show * data.tenantID: from az ad sp create-for-rbac output as you can see above * data.clientID: from az ad sp create-for-rbac output as you can see above * data.clientSecret: from az ad sp create-for-rbac output as you can see above * data.accessKeyID: You need to add this field for Route53 records to be updated. * data.secretAccessKey: You need to add this field for Route53 records to be updated.\nNote: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r '.[] | select(.isDefault == true) | .id') TENANT_ID=$(az account show -o tsv --query 'tenantId') ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026quot;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(uuid-of-tenant)@$(echo \u0026quot;$TENANT_ID\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(uuid-of-client)@$(echo \u0026quot;${CLIENT_ID:?\u0026quot;CLIENT_ID is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;s@base64(client-secret)@$(echo \u0026quot;${CLIENT_SECRET:?\u0026quot;CLIENT_SECRET is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\ -e \u0026quot;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d '\\n' | base64 )\u0026quot; \\ -e \u0026quot;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d '\\n' | base64 )\u0026quot; \\ example/70-secret-cloudprovider-azure.yaml  After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml  And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e 's/# namespace: .*/ namespace: garden-dev/' \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml  Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml: * spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) * spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here * spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text) SHOOT_DOMAIN=\u0026quot;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026quot; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026quot;s/region: westeurope/region: eastus/\u0026quot; \\ -e \u0026quot;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026quot; \\ -e \u0026quot;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026quot; \\ -e \u0026quot;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026quot; \\ example/90-deprecated-shoot-azure.yaml  And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml  After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s  Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026quot;2018-06-09T07:35:45Z\u0026quot; level=info msg=\u0026quot;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026quot; time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Starting flow Shoot cluster creation\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployExternalDomainDNSRecord\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployNamespace\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployKubeAPIServerService\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:51Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:51Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).MoveBackupTerraformResources\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:56Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:57Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:35:57Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:36:01Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:36:02Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026quot;2018-06-09T07:36:02Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ...  At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Warning ReconcileError 48m gardener-controller-manager [2HAbm45D] Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).EnsureIngressDNSRecord' returned '`.status.loadBalancer.ingress[]` has no elements yet, i.e. external load balancer has not been created (is your quota limit exceeded/reached?)' Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Warning ReconcileError 35m gardener-controller-manager [rhL38ym4] Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).EnsureIngressDNSRecord' returned '`.status.loadBalancer.ingress[]` has no elements yet, i.e. external load balancer has not been created (is your quota limit exceeded/reached?)' Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state  Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026quot;project:dev\u0026quot; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener.  Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager  With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026quot;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).DeployExternalDomainDNSRecord' returned 'Terraform execution ... lastOperation: description: \u0026quot;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).DeployExternalDomainDNSRecord' returned 'Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile  Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # '--' is required if you want to # pass any args starting with '-' # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager  Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl  The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath='{.data.kubeconfig}' | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml  Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev  "
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/27_deploy_into_cluster/",
	"title": "Deploy into a Cluster",
	"tags": [],
	"description": "",
	"content": " Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component. Also note that all resources and deployments need to be created in the garden namespace (not overrideable).\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\ --namespace garden \\ --name gardener-controlplane \\ -f gardener-values.yaml \\ --wait  Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) The Gardenlet requires a bootstrap token as well as a bootstrap kubeconfig in order to properly register itself with the Gardener control plane.\nThe configuration values depict the various options to configure it. Please consult this document to get a detailed explanation of what can be configured.\nPrepare your values in a separate gardenlet-values.yaml file:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster (see this and this). Create a bootstrap kubeconfig containing this token:  apiVersion: v1 kind: Config current-context: gardenlet-bootstrap@default clusters: - cluster: certificate-authority-data: \u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt; name: default contexts: - context: cluster: default user: gardenlet-bootstrap name: gardenlet-bootstrap@default users: - name: gardenlet-bootstrap user: token: \u0026lt;bootstrap-token\u0026gt;   Provide this bootstrap kubeconfig together with a desired name and namespace to the Gardenlet Helm chart values here:  gardenClientConnection: bootstrapKubeconfig: name: gardenlet-kubeconfig-bootstrap namespace: garden kubeconfig: | \u0026lt;bootstrap-kubeconfig\u0026gt;   Define a name and namespace where the Gardenlet shall store the real kubeconfig it creates during the bootstrap process here:  gardenClientConnection: kubeconfigSecret: name: gardenlet-kubeconfig namespace: garden   Define either seedSelector or seedConfig (see this document  Now you are ready to deploy the Helm chart:\nhelm install charts/gardener/gardenlet \\ --namespace garden \\ --name gardenlet \\ -f gardenlet-values.yaml \\ --wait  :warning: A current prerequisite of Kubernetes clusters that are used as seeds is to have a pre-deployed nginx-ingress-controller to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/dockerfile_pitfall/",
	"title": "Dockerfile pitfalls",
	"tags": [],
	"description": "Common Dockerfile pitfalls",
	"content": " Using latest tag for an image Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile FROM alpine  While simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn\u0026rsquo;t actually make any changes.\nGood Dockerfile A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.\nFROM alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430  Running apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with its own problems.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won\u0026rsquo;t actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nAvoid big container images Building small container image will reduce the time needed to start or restart pods. An image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB). For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE postgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB postgres 9.6 d92dad241eff 13 days ago 235.4 MB postgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB  In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker\u0026rsquo;s support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here.\nGoogle\u0026rsquo;s distroless image is also a good base image.\n"
},
{
	"uri": "https://gardener.cloud/contribute/docs/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": " Contributing Documentation How to Contribute to the Open Source Project Gardener    You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions: * Contributions must be licensed under the Creative Commons Attribution 4.0 International License * You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.\n "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/dynamic-pvc/",
	"title": "Dynamic Volume Provisioning",
	"tags": [],
	"description": "How to dynamically provision volume",
	"content": " Introduction The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database\nRun postgres database Define the following Kubernetes resources in a yaml file\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgresdb-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 9Gi storageClassName: 'default'  This defines a PVC using storage class default. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {\u0026ldquo;storageclass.kubernetes.io/is-default-class\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}.\n$ kubectl describe sc default Name: default IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;storage.k8s.io/v1beta1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;StorageClass\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;:\u0026quot;true\u0026quot;},\u0026quot;labels\u0026quot;:{\u0026quot;addonmanager.kubernetes.io/mode\u0026quot;:\u0026quot;Exists\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;\u0026quot;},\u0026quot;parameters\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;gp2\u0026quot;},\u0026quot;provisioner\u0026quot;:\u0026quot;kubernetes.io/aws-ebs\u0026quot;} ,storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt;  A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as \u0026ldquo;postgresdb-pvc\u0026rdquo;, and a corresponding PV \u0026ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026rdquo; is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml persistentvolumeclaim \u0026quot;postgresdb-pvc\u0026quot; created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s  Notice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one is Retain. (A third policy Recycle has been deprecated). In case of Delete, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.\nOn the other hand, PV with Retain policy will not be deleted when the PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nYou can use the kubectl patch command to change the reclaim policy as described here here or use kubectl edit pv \u0026lt;pv-name\u0026gt; to edit online as below:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m # change the relcaim policy from \u0026quot;Delete\u0026quot; to \u0026quot;Retain\u0026quot; $ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb persistentvolume \u0026quot;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026quot; edited # check the reclaim policy afterwards $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m  Deployment Once a PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two paths in the container are mounted to subfolders in the volume.\napiVersion: apps/v1 kind: Deployment metadata: name: postgres namespace: default labels: app: postgres annotations: deployment.kubernetes.io/revision: \u0026quot;1\u0026quot; spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 selector: matchLabels: app: postgres template: metadata: name: postgres labels: app: postgres spec: containers: - name: postgres image: \u0026quot;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\u0026quot; env: - name: POSTGRES_USER value: postgres - name: POSTGRES_PASSWORD value: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ - name: POSTGRES_INITDB_XLOGDIR value: \u0026quot;/var/log/postgresql/logs\u0026quot; ports: - containerPort: 5432 volumeMounts: - mountPath: /var/lib/postgresql/data name: postgre-db subPath: data # https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found) - mountPath: /var/log/postgresql/logs name: postgre-db subPath: logs volumes: - name: postgre-db persistentVolumeClaim: claimName: postgresdb-pvc readOnly: false imagePullSecrets: - name: cpettechregistry  To check the mount points in the container:\n$ kubectl get po NAME READY STATUS RESTARTS AGE postgres-7f485fd768-c5jf9 1/1 Running 0 32m $ kubectl exec -it postgres-7f485fd768-c5jf9 bash root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/ base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/ 000000010000000000000001 archive_status  Deleting a PersistentVolumeClaim In case of \u0026ldquo;Delete\u0026rdquo; policy, deleting a PVC will also delete its associated PV. If \u0026ldquo;Retain\u0026rdquo; is the reclaim policy, the PV will change status from Bound to Released when PVC is deleted.\n# Check pvc and pv before deletion $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m # delete pvc $ kubectl delete pvc postgresdb-pvc persistentvolumeclaim \u0026quot;postgresdb-pvc\u0026quot; deleted # pv changed to status \u0026quot;Released\u0026quot; $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m  "
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/13_env/",
	"title": "Enviroment",
	"tags": [],
	"description": "",
	"content": " Preparing the Setup Conceptually, all Gardener components are designated to run inside as a Pod inside a Kubernetes cluster. The API server extends the Kubernetes API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details could be found in\n Principles of Kubernetes, and its components Kubernetes Development Guide Architecture of Gardener  This setup is based on minikube, a Kubernetes cluster running on a single node. Docker for Desktop and kind are also supported.\nInstalling Golang environment Install latest version of Golang. For MacOS you could use Homebrew:\nbrew install golang  For other OS, please check Go installation documentation.\nInstalling kubectl and helm As already mentioned in the introduction, the communication with the Gardener happens via the Kubernetes (Garden) cluster it is targeting. To interact with that cluster, you need to install kubectl. Please make sure that the version of kubectl is at least v1.11.x.\nOn MacOS run\nbrew install kubernetes-cli  Please check the kubectl installation documentation for other OS.\nYou may also need to develop Helm charts or interact with Tiller using the Helm CLI:\nOn MacOS run\nbrew install kubernetes-helm  On other OS please check the Helm installation documentation.\nInstalling git We use git as VCS which you need to install.\nOn MacOS run\nbrew install git  On other OS, please check the Git installation documentation.\nInstalling openvpn We use OpenVPN to establish network connectivity from the control plane running in the Seed cluster to the Shoot\u0026rsquo;s worker nodes running in private networks. To harden the security we need to generate another secret to encrypt the network traffic (details). Please install the openvpn binary. On MacOS run\nbrew install openvpn export PATH=$(brew --prefix openvpn)/sbin:$PATH  On other OS, please check the OpenVPN downloads page.\nInstalling Minikube You\u0026rsquo;ll need to have minikube installed and running. \u0026gt; Note: Gardener is working only with self-contained kubeconfig files because of security issue. You can configure your minikube to create self-contained kubeconfig files via: \u0026gt; bash \u0026gt; minikube config set embed-certs true \u0026gt;\nAlternatively, you can also install Docker for Desktop and kind.\nIn case you want to use the \u0026ldquo;Docker for Mac Kubernetes\u0026rdquo; or if you want to build Docker images for the Gardener you have to install Docker itself. On MacOS, please use Docker for MacOS which can be downloaded here.\nOn other OS, please check the Docker installation documentation.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration.\nOn MacOS run\nbrew install iproute2mac  Installing yaml2json and jq go get -u github.com/bronze1man/yaml2json brew install jq  [MacOS only] Install GNU core utilities When running on MacOS you have to install the GNU core utilities:\nbrew install coreutils gnu-sed  This will create symbolic links for the GNU utilities with g prefix in /usr/local/bin, e.g., gsed or gbase64. To allow using them without the g prefix please put /usr/local/opt/coreutils/libexec/gnubin at the beginning of your PATH environment variable, e.g., export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH.\n[Optional] Installing gcloud SDK In case you have to create a new release or a new hotfix of the Gardener you have to push the resulting Docker image into a Docker registry. Currently, we are using the Google Container Registry (this could change in the future). Please follow the official installation instructions from Google.\nLocal Gardener setup This setup is only meant to be used for developing purposes, which means that only the control plane of the Gardener cluster is running on your machine.\nGet the sources Clone the repository from GitHub.\ngit clone git@github.com:gardener/gardener.git cd gardener  Start the Gardener :warning: Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of Gardener, and what the various clusters are used for.  The development of the Gardener could happen by targeting any cluster. You basically need a Garden cluster (e.g., a Minikube cluster) and one Seed cluster per cloud provider.\nThe commands below will configure your minikube with the absolute minimum resources to launch Gardener API Server and Gardener Controller Manager on your local machine.\nStart minikube minikube start --embed-certs # `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files. 😄 minikube v1.5.2 on darwin (amd64) 🔥 Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... [...] 🏄 Done! Thank you for using minikube!  Prepare the Gardener make dev-setup Found Minikube ... namespace/garden created namespace/garden-dev created deployment.apps/etcd created service/etcd created service/gardener-apiserver created service/gardener-controller-manager created endpoints/gardener-apiserver created endpoints/gardener-controller-manager created apiservice.apiregistration.k8s.io/v1beta1.garden.sapcloud.io created apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created validatingwebhookconfiguration.admissionregistration.k8s.io/gardener-controller-manager created  Optionally, you can switch off the Logging feature gate of Gardenlet to save resources:\nsed -i -e 's/Logging: true/Logging: false/g' dev/20-componentconfig-gardenlet.yaml  The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\nkubectl apply -f example/10-secret-internal-domain-unmanaged.yaml secret/internal-domain-unmanaged created  Run the Gardener Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the Makefile.\nmake start-apiserver Found Minikube ... I0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin \u0026quot;ResourceReferenceManager\u0026quot; I0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin \u0026quot;DeletionConfirmation\u0026quot; [...] I0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443 [...]  (Optional) Now you are ready to launch the Gardener Controller Manager.\nmake start-controller-manager time=\u0026quot;2019-03-06T15:24:17+02:00\u0026quot; level=info msg=\u0026quot;Starting Gardener controller manager...\u0026quot; time=\u0026quot;2019-03-06T15:24:17+02:00\u0026quot; level=info msg=\u0026quot;Feature Gates: \u0026quot; time=\u0026quot;2019-03-06T15:24:17+02:00\u0026quot; level=info msg=\u0026quot;Starting HTTP server on 0.0.0.0:2718\u0026quot; time=\u0026quot;2019-03-06T15:24:17+02:00\u0026quot; level=info msg=\u0026quot;Acquired leadership, starting controllers.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Starting HTTPS server on 0.0.0.0:2719\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Successfully bootstrapped the Garden cluster.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Gardener controller manager (version 1.0.0-dev) initialized.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;ControllerRegistration controller initialized.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;SecretBinding controller initialized.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Project controller initialized.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Quota controller initialized.\u0026quot; time=\u0026quot;2019-03-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;CloudProfile controller initialized.\u0026quot; [...]  (Optional) Now you are ready to launch the Gardener Scheduler.\nmake start-scheduler time=\u0026quot;2019-05-02T16:31:50+02:00\u0026quot; level=info msg=\u0026quot;Starting Gardener scheduler ...\u0026quot; time=\u0026quot;2019-05-02T16:31:50+02:00\u0026quot; level=info msg=\u0026quot;Starting HTTP server on 0.0.0.0:10251\u0026quot; time=\u0026quot;2019-05-02T16:31:50+02:00\u0026quot; level=info msg=\u0026quot;Acquired leadership, starting scheduler.\u0026quot; time=\u0026quot;2019-05-02T16:31:50+02:00\u0026quot; level=info msg=\u0026quot;Gardener scheduler initialized (with Strategy: SameRegion)\u0026quot; time=\u0026quot;2019-05-02T16:31:50+02:00\u0026quot; level=info msg=\u0026quot;Scheduler controller initialized.\u0026quot; [...]  (Optional) Now you are ready to launch the Gardenlet.\nmake start-gardenlet time=\u0026quot;2019-11-06T15:24:17+02:00\u0026quot; level=info msg=\u0026quot;Starting Gardenlet...\u0026quot; time=\u0026quot;2019-11-06T15:24:17+02:00\u0026quot; level=info msg=\u0026quot;Feature Gates: HVPA=true, Logging=true\u0026quot; time=\u0026quot;2019-11-06T15:24:17+02:00\u0026quot; level=info msg=\u0026quot;Acquired leadership, starting controllers.\u0026quot; time=\u0026quot;2019-11-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026quot; time=\u0026quot;2019-11-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Gardenlet (version 1.0.0-dev) initialized.\u0026quot; time=\u0026quot;2019-11-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;ControllerInstallation controller initialized.\u0026quot; time=\u0026quot;2019-11-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Shoot controller initialized.\u0026quot; time=\u0026quot;2019-11-06T15:24:18+02:00\u0026quot; level=info msg=\u0026quot;Seed controller initialized.\u0026quot; [...]  :warning: The Gardenlet will handle all your seeds for this development scenario, although, for productive usage it is recommended to run it once per seed, see this document for more information.\nRun the following command to install extension controllers - make sure that you install all of them required for your local development. Also, please refer to this document for further information about how extensions are registered in case you want to use other versions than the latest releases.\nmake dev-setup-extensions \u0026gt; Found extension 'os-coreos'. Do you want to install it into your local Gardener setup? (y/n) ...  Alternatively, you may also want to take a look at the Gardener Extension Manager.\nThe Gardener should now be ready to operate on Shoot resources. You can use\nkubectl get shoots No resources found.  to operate against your local running Gardener API Server.\n Note: It may take several seconds until the minikube cluster recognizes that the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Limitations of local development setup You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a \u0026ldquo;real\u0026rdquo; provider. Currently, it is not possible to run Gardener entirely isolated from any cloud provider. We are planning to support such a setup based on KubeVirt (see this for details), however, it does not yet exist. This means that - after you have setup Gardener - you need to register an external seed cluster (e.g., one created in AWS). Only after that step you can start creating shoot clusters with your locally running Gardener.\nSome time ago, we had a local setup based on VirtualBox/Vagrant. However, as we have progressed with the Extensibility epic we noticed that this implementation/setup does no longer fit into how we envision external providers to be. Moreover, it hid too many things and came with a bunch of limitations, making the development scenario too \u0026ldquo;artificial\u0026rdquo;:\n No integration with machine-controller-manager. The Shoot API Server is exposed via a NodePort. In a cloud setup a LoadBalancer would be used. It was not possible to create Shoot clusters consisting of more than one worker node. Cluster auto-scaling therefore is not supported. It was not possible to create two or more Shoot clusters in parallel. The communication between the Seed and the Shoot Clusters uses VPN tunnel. In this setup tunnels are not needed since all components run on localhost.  Additional information In order to ensure that a specific Seed cluster will be chosen, add the .spec.cloud.seed field (see here for an example Shoot manifest).\nPlease take a look at the example manifests folder to see which resource objects you need to install into your Garden cluster.\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/25_markup/expand/",
	"title": "Expand",
	"tags": [],
	"description": "Displays an expandable/collapsible section of text on your page",
	"content": " The Expand shortcode displays an expandable/collapsible section of text on your page. Here is an example\n  Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  Usage this shortcode takes exactly one optional parameter to define the text that appears next to the expand/collapse icon. (default is \u0026ldquo;Expand me\u0026hellip;\u0026rdquo;)\n{{%expand \u0026quot;Is this learn theme rocks ?\u0026quot; %}}Yes !.{{% /expand%}}    Is this learn theme rocks ?   Yes !  \nDemo {{%expand%}} Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. {{% /expand%}}    Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  "
},
{
	"uri": "https://gardener.cloud/api-reference/extensions/",
	"title": "Extensions",
	"tags": [],
	"description": "",
	"content": "Packages:\n  extensions.gardener.cloud/v1alpha1   extensions.gardener.cloud/v1alpha1  Package v1alpha1 is the v1alpha1 version of the API.\nResource Types:  BackupBucket  BackupEntry  Cluster  ControlPlane  Extension  Infrastructure  Network  OperatingSystemConfig  Worker  BackupBucket   BackupBucket is a specification for backup bucket.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupBucketStatus         BackupEntry   BackupEntry is a specification for backup Entry.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupEntryStatus         Cluster   Cluster is a specification for a Cluster resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Cluster    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterSpec          cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n       ControlPlane   ControlPlane is a specification for a ControlPlane resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ControlPlane    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControlPlaneSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for this control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n       status  ControlPlaneStatus         Extension   Extension is a specification for a Extension resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Extension    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ExtensionSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration for the respective extension controller.\n       status  ExtensionStatus         Infrastructure   Infrastructure is a specification for cloud provider infrastructure.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Infrastructure    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  InfrastructureSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for this infrastructure.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n       status  InfrastructureStatus         Network   Network is the specification for cluster networking.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Network    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  NetworkSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains plugin-specific configuration.\n       status  NetworkStatus         OperatingSystemConfig   OperatingSystemConfig is a specification for a OperatingSystemConfig resource\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  OperatingSystemConfig    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  OperatingSystemConfigSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the machine-controller-manager to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to extension resource.\n       status  OperatingSystemConfigStatus         Worker   Worker is a specification for a Worker resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Worker    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  WorkerSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n       status  WorkerStatus         BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the spec for an BackupBucket resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus is the status for an BackupBucket resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the spec for an BackupEntry resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus is the status for an BackupEntry resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    CloudConfig   (Appears on: OperatingSystemConfigStatus)  CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n   Field Description      secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    ClusterSpec   (Appears on: Cluster)  ClusterSpec is the spec for a Cluster resource.\n   Field Description      cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n    ControlPlaneSpec   (Appears on: ControlPlane)  ControlPlaneSpec is the spec of a ControlPlane resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for this control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    ControlPlaneStatus   (Appears on: ControlPlane)  ControlPlaneStatus is the status of a ControlPlane resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific output for this control plane.\n    DefaultSpec   (Appears on: BackupBucketSpec, BackupEntrySpec, ControlPlaneSpec, ExtensionSpec, InfrastructureSpec, NetworkSpec, OperatingSystemConfigSpec, WorkerSpec)  DefaultSpec contains common status fields for every extension resource.\n   Field Description      type  string    Type contains the instance of the resource\u0026rsquo;s kind.\n    DefaultStatus   (Appears on: BackupBucketStatus, BackupEntryStatus, ControlPlaneStatus, ExtensionStatus, InfrastructureStatus, NetworkStatus, OperatingSystemConfigStatus, WorkerStatus)  DefaultStatus contains common status fields for every extension resource.\n   Field Description      conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    lastError  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    lastOperation  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation     (Optional) LastOperation holds information about the last operation on the resource.\n    observedGeneration  int64    ObservedGeneration is the most recent generation observed for this resource.\n    state  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) State can be filled by the operating controller with what ever data it needs.\n    DropIn   (Appears on: Unit)  DropIn is a drop-in configuration for a systemd unit.\n   Field Description      name  string    Name is the name of the drop-in.\n    content  string    Content is the content of the drop-in.\n    ExtensionSpec   (Appears on: Extension)  ExtensionSpec is the spec for a Extension resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration for the respective extension controller.\n    ExtensionStatus   (Appears on: Extension)  ExtensionStatus is the status for a Extension resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific output for this extension.\n    File   (Appears on: OperatingSystemConfigSpec)  File is a file that should get written to the host\u0026rsquo;s file system. The content can either be inlined or referenced from a secret in the same namespace.\n   Field Description      path  string    Path is the path of the file system where the file should get written to.\n    permissions  int32    (Optional) Permissions describes with which permissions the file should get written to the file system. Should be defaulted to octal 0644.\n    content  FileContent     Content describe the file\u0026rsquo;s content.\n    FileContent   (Appears on: File)  FileContent can either reference a secret or contain inline configuration.\n   Field Description      secretRef  FileContentSecretRef     (Optional) SecretRef is a struct that contains information about the referenced secret.\n    inline  FileContentInline     (Optional) Inline is a struct that contains information about the inlined data.\n    FileContentInline   (Appears on: FileContent)  FileContentInline contains keys for inlining a file content\u0026rsquo;s data and encoding.\n   Field Description      encoding  string    Encoding is the file\u0026rsquo;s encoding (e.g. base64).\n    data  string    Data is the file\u0026rsquo;s data.\n    FileContentSecretRef   (Appears on: FileContent)  FileContentSecretRef contains keys for referencing a file content\u0026rsquo;s data from a secret in the same namespace.\n   Field Description      name  string    Name is the name of the secret.\n    dataKey  string    DataKey is the key in the secret\u0026rsquo;s .data field that should be read.\n    InfrastructureSpec   (Appears on: Infrastructure)  InfrastructureSpec is the spec for an Infrastructure resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for this infrastructure.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n    InfrastructureStatus   (Appears on: Infrastructure)  InfrastructureStatus is the status for an Infrastructure resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific output for this infrastructure.\n    LastError   LastError is the last error on an object.\nLastOperation   LastOperation is the last operation on an object.\nMachineDeployment   (Appears on: WorkerStatus)  MachineDeployment is a created machine deployment.\n   Field Description      name  string    Name is the name of the MachineDeployment resource.\n    minimum  int    Minimum is the minimum number for this machine deployment.\n    maximum  int    Maximum is the maximum number for this machine deployment.\n    MachineImage   (Appears on: WorkerPool)  MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n   Field Description      name  string    Name is the logical name of the machine image.\n    version  string    Version is the version of the machine image.\n    NetworkSpec   (Appears on: Network)  NetworkSpec is the spec for an Network resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains plugin-specific configuration.\n    NetworkStatus   (Appears on: Network)  NetworkStatus is the status for an Network resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains plugin-specific output.\n    Object   Object is an extension object resource.\nOperatingSystemConfigPurpose (string alias)\n  (Appears on: OperatingSystemConfigSpec)  OperatingSystemConfigPurpose is a string alias.\nOperatingSystemConfigSpec   (Appears on: OperatingSystemConfig)  OperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the machine-controller-manager to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to extension resource.\n    OperatingSystemConfigStatus   (Appears on: OperatingSystemConfig)  OperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    cloudConfig  CloudConfig     (Optional) CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n    command  string    (Optional) Command is the command whose execution renews/reloads the cloud config on an existing VM, e.g. \u0026ldquo;/usr/bin/reload-cloud-config -from-file=\u0026rdquo;. The  is optionally provided by Gardener in the .spec.reloadConfigFilePath field.\n    units  []string    (Optional) Units is a list of systemd unit names that are part of the generated Cloud Config and shall be restarted when a new version has been downloaded.\n    Purpose (string alias)\n  (Appears on: ControlPlaneSpec)  Purpose is a string alias.\nSpec   Spec is the spec section of an Object.\nStatus   Status is the status of an Object.\nUnit   (Appears on: OperatingSystemConfigSpec)  Unit is a unit for the operating system configuration (usually, a systemd unit).\n   Field Description      name  string    Name is the name of a unit.\n    command  string    (Optional) Command is the unit\u0026rsquo;s command.\n    enable  bool    (Optional) Enable describes whether the unit is enabled or not.\n    content  string    (Optional) Content is the unit\u0026rsquo;s content.\n    dropIns  []DropIn     (Optional) DropIns is a list of drop-ins for this unit.\n    Volume   (Appears on: WorkerPool)  Volume contains information about the root disks that should be used for worker pools.\n   Field Description      type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the size of the volume.\n    WorkerPool   (Appears on: WorkerSpec)  WorkerPool is the definition of a specific worker pool.\n   Field Description      machineType  string    MachineType contains information about the machine type that should be used for this worker pool.\n    maximum  int    Maximum is the maximum size of the worker pool.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    machineImage  MachineImage     MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n    minimum  int    Minimum is the minimum size of the worker pool.\n    name  string    Name is the name of this worker pool.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is a provider specific configuration for the worker pool.\n    userData  []byte    UserData is a base64-encoded string that contains the data that is sent to the provider\u0026rsquo;s APIs when a new machine/VM that is part of this worker pool shall be spawned.\n    volume  Volume     (Optional) Volume contains information about the root disks that should be used for this worker pool.\n    zones  []string    (Optional) Zones contains information about availability zones for this worker pool.\n    WorkerSpec   (Appears on: Worker)  WorkerSpec is the spec for a Worker resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n    WorkerStatus   (Appears on: Worker)  WorkerStatus is the status for a Worker resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    machineDeployments  []MachineDeployment     MachineDeployments is a list of created machine deployments. It will be used to e.g. configure the cluster-autoscaler properly.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific output for this worker.\n      Generated with gen-crd-api-reference-docs on git commit 79c676930. \n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/app/featureflag/",
	"title": "Feature Flags in Kubernetes Applications",
	"tags": [],
	"description": "Feature Flags in Kubernetes Applications",
	"content": " Feature Flags in Kubernetes Applications Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nIn Kubernetes, labels are part of the identity of a resource and can be used through selectors. Annotations are similar, but do not participate in the identity of a resource and cannot be used to select resources. Nevertheless, they can still be used as feature flags to enable/disable application logic.\nPossible Use Cases  turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  How does this work We’ll use the Kubernetes downwardAPI ) to expose labels and annotations directly to our application. We’ll end up with two files (labels and annotations) in /etc/podinfo. First we add the downward api to spec.volumes. Note that it is possible to adding both labels and annotations into the same volume.\nHow to update/toggle the feature After the deployment of the demo application is done you can easily switch a feature in the application on or off. This is done very easily with kubectl by changing an annotation in the Pods.\nDeploy demo app/pod kubectl apply -f ./yaml/deployment.yaml  Show the log kubectl logs featureflag-example -f  Use business feature 2 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation2  Use business feature 1 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation1  Conclusion As you can see in the log of the Pod the application switches very fast between the implementations. Everything was controlled by annotations on the deployment or Pod. On the whole a very simple and maintainable solution to configure parts of the application without restarting the whole application.\nWrangling labels and annotations from the shell. # Add a label $ kubectl label pod my-pod-name a-label=foo # Show labels $ kubectl get pods --show-labels # If you only want to show specific labels, use -L=\u0026lt;label1\u0026gt;,\u0026lt;label2\u0026gt; # Update a label $ kubectl label pod my-pod-name a-label=bar --override # Delete a label .Remember the \u0026quot;-\u0026quot; at the end of the line. Required to remove a label $ kubectl label pod my-pod-name a-label- # Add an annotation $ kubectl annotatate pod my-pod-name an-annotation=foo # Show annotations $ kubectl describe pod my-pod-name # Update an annotation $ kubectl annotation pod my-pod-name an-annotation=foo --override # Delete an annotation. Remember the \u0026quot;-\u0026quot; at the end of the line. Required to remove a annotation $ kubectl annotation pod my-pod-name an-annotation-  "
},
{
	"uri": "https://gardener.cloud/blog/2019_week_21_2/",
	"title": "Feature Flags in Kubernetes Applications",
	"tags": [],
	"description": "",
	"content": "Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nPossible Use Cases - turn on/off a specific instance - turn on/off profiling of a specific instance - change the logging level, to capture detailed logs during a specific event - change caching strategy at runtime - change timeouts in production - toggle on/off some special verification\n..read some more on Feature Flags for App.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_17/",
	"title": "Frontend HTTPS",
	"tags": [],
	"description": "",
	"content": " For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nCreate a secret in the namespace of the ingress containing the TLS private key and certificate. Then configure the secret name in the TLS configuration section of the ingress specification.\n..read on HTTPS - Self Signed Certificates how to configure it.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/gpu/",
	"title": "GPU Enabled Cluster",
	"tags": [],
	"description": "Setting up a GPU Enabled Cluster for Deep Learning",
	"content": " Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, contributions are highly appreciated to update this guide.\nCreate a Cluster First thing first, let’s create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it\u0026rsquo;s the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. This costs around 1€/hour per GPU\nInstall NVidia Driver as Daemonset apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-driver-installer namespace: kube-system labels: k8s-app: nvidia-driver-installer spec: selector: matchLabels: name: nvidia-driver-installer k8s-app: nvidia-driver-installer template: metadata: labels: name: nvidia-driver-installer k8s-app: nvidia-driver-installer spec: hostPID: true initContainers: - image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972 name: modulus args: - compile - nvidia - \u0026quot;410.104\u0026quot; securityContext: privileged: true env: - name: MODULUS_CHROOT value: \u0026quot;true\u0026quot; - name: MODULUS_INSTALL value: \u0026quot;true\u0026quot; - name: MODULUS_INSTALL_DIR value: /opt/drivers - name: MODULUS_CACHE_DIR value: /opt/modulus/cache - name: MODULUS_LD_ROOT value: /root - name: IGNORE_MISSING_MODULE_SYMVERS value: \u0026quot;1\u0026quot; volumeMounts: - name: etc-coreos mountPath: /etc/coreos readOnly: true - name: usr-share-coreos mountPath: /usr/share/coreos readOnly: true - name: ld-root mountPath: /root - name: module-cache mountPath: /opt/modulus/cache - name: module-install-dir-base mountPath: /opt/drivers - name: dev mountPath: /dev containers: - image: \u0026quot;gcr.io/google-containers/pause:3.1\u0026quot; name: pause tolerations: - key: \u0026quot;nvidia.com/gpu\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; operator: \u0026quot;Exists\u0026quot; volumes: - name: etc-coreos hostPath: path: /etc/coreos - name: usr-share-coreos hostPath: path: /usr/share/coreos - name: ld-root hostPath: path: / - name: module-cache hostPath: path: /opt/modulus/cache - name: dev hostPath: path: /dev - name: module-install-dir-base hostPath: path: /opt/drivers  Install Device Plugin apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-gpu-device-plugin namespace: kube-system labels: k8s-app: nvidia-gpu-device-plugin #addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: nvidia-gpu-device-plugin template: metadata: labels: k8s-app: nvidia-gpu-device-plugin annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-node-critical volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins - name: dev hostPath: path: /dev containers: - image: \u0026quot;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d\u0026quot; command: [\u0026quot;/usr/bin/nvidia-gpu-device-plugin\u0026quot;, \u0026quot;-logtostderr\u0026quot;, \u0026quot;-host-path=/opt/drivers/nvidia\u0026quot;] name: nvidia-gpu-device-plugin resources: requests: cpu: 50m memory: 10Mi limits: cpu: 50m memory: 10Mi securityContext: privileged: true volumeMounts: - name: device-plugin mountPath: /device-plugin - name: dev mountPath: /dev updateStrategy: type: RollingUpdate  Test To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026amp; Keras\napiVersion: apps/v1 kind: Deployment metadata: name: deeplearning-workbench namespace: default spec: replicas: 1 selector: matchLabels: app: deeplearning-workbench template: metadata: labels: app: deeplearning-workbench spec: containers: - name: deeplearning-workbench image: afritzler/deeplearning-workbench resources: limits: nvidia.com/gpu: 1 tolerations: - key: \u0026quot;nvidia.com/gpu\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; operator: \u0026quot;Exists\u0026quot;  Note: the tolerations section above is not required if you deploy the ExtendedResourceToleration admission controller to your cluster. You can do this in the kubernetes section of your Gardener cluster shoot.yaml as follows:\n kubernetes: kubeAPIServer: admissionPlugins: - name: ExtendedResourceToleration  Now exec into the container and start an example Keras training\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash cd /keras/example python imdb_cnn.py  Acknowledgments \u0026amp; References  Andreas Fritzler from the Gardener Core team for the R\u0026amp;D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  "
},
{
	"uri": "https://gardener.cloud/api-reference/garden/",
	"title": "Garden",
	"tags": [],
	"description": "",
	"content": "Packages:\n  garden.sapcloud.io/v1beta1   garden.sapcloud.io/v1beta1  Package v1beta1 is a version of the API.\nResource Types:  CloudProfile  Project  Quota  SecretBinding  Seed  Shoot  CloudProfile   CloudProfile represents certain properties about a cloud environment.\n   Field Description      apiVersion string   garden.sapcloud.io/v1beta1      kind string  CloudProfile    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  CloudProfileSpec     (Optional) Spec defines the cloud environment properties.\n     aws  AWSProfile     (Optional) AWS is the profile specification for the Amazon Web Services cloud.\n    azure  AzureProfile     (Optional) Azure is the profile specification for the Microsoft Azure cloud.\n    gcp  GCPProfile     (Optional) GCP is the profile specification for the Google Cloud Platform cloud.\n    openstack  OpenStackProfile     (Optional) OpenStack is the profile specification for the OpenStack cloud.\n    alicloud  AlicloudProfile     (Optional) Alicloud is the profile specification for the Alibaba cloud.\n    packet  PacketProfile     (Optional) Packet is the profile specification for the Packet cloud.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of the Shoot cluster.\n       Project   Project holds certain properties about a Gardener project.\n   Field Description      apiVersion string   garden.sapcloud.io/v1beta1      kind string  Project    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ProjectSpec     (Optional) Spec defines the project properties.\n     createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []Kubernetes rbac/v1.Subject     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user that should be part of this project with full permissions to manage it.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    viewers  []Kubernetes rbac/v1.Subject     (Optional) Viewers is a list of subjects representing a user name, an email address, or any other identifier of a user that should be part of this project with limited permissions to only view some resources.\n       status  ProjectStatus     (Optional) Most recently observed status of the Project.\n    Quota      Field Description      apiVersion string   garden.sapcloud.io/v1beta1      kind string  Quota    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  QuotaSpec     (Optional) Spec defines the Quota constraints.\n     clusterLifetimeDays  int    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  QuotaScope     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n       SecretBinding      Field Description      apiVersion string   garden.sapcloud.io/v1beta1      kind string  SecretBinding    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret object in the same or another namespace.\n    quotas  []Kubernetes core/v1.ObjectReference     (Optional) Quotas is a list of references to Quota objects in the same or another namespace.\n    Seed   Seed holds certain properties about a Seed cluster.\n   Field Description      apiVersion string   garden.sapcloud.io/v1beta1      kind string  Seed    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     (Optional) Spec defines the Seed cluster properties.\n     cloud  SeedCloud     Cloud defines the cloud profile and the region this Seed cluster belongs to.\n    ingressDomain  string    IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    visible  bool    (Optional) Visible labels the Seed cluster as selectable for the seedfinder admission controller.\n    protected  bool    (Optional) Protected prevent that the Seed Cluster can be used for regular Shoot cluster control planes.\n    backup  BackupProfile     (Optional) Backup holds the object store configuration for the backups of shoot(currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for Shoots associated with this Seed. If backup field is present in Seed, then backups of the etcd from Shoot controlplane will be stored under the configured object store.\n       status  SeedStatus     (Optional) Most recently observed status of the Seed cluster.\n    Shoot      Field Description      apiVersion string   garden.sapcloud.io/v1beta1      kind string  Shoot    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the Shoot cluster.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloud  Cloud     Cloud contains information about the cloud environment and their specific settings.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n       status  ShootStatus     (Optional) Most recently observed status of the Shoot cluster.\n    AWSCloud   (Appears on: Cloud)  AWSCloud contains the Shoot specification for AWS.\n   Field Description      machineImage  ShootMachineImage     (Optional) ShootMachineImage holds information about the machine image to use for all workers. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    networks  AWSNetworks     Networks holds information about the Kubernetes and infrastructure networks.\n    workers  []AWSWorker     Workers is a list of worker groups.\n    zones  []string    Zones is a list of availability zones to deploy the Shoot cluster to.\n    AWSConstraints   (Appears on: AWSProfile)  AWSConstraints is an object containing constraints for certain values in the Shoot specification.\n   Field Description      dnsProviders  []DNSProviderConstraint     (Optional) DNSProviders contains constraints regarding allowed values of the \u0026lsquo;dns.provider\u0026rsquo; block in the Shoot specification.\n    kubernetes  KubernetesConstraints     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    volumeTypes  []VolumeType     VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    zones  []Zone     Zones contains constraints regarding allowed values for \u0026lsquo;zones\u0026rsquo; block in the Shoot specification.\n    AWSNetworks   (Appears on: AWSCloud)  AWSNetworks holds information about the Kubernetes and infrastructure networks.\n   Field Description      K8SNetworks  K8SNetworks      (Members of K8SNetworks are embedded into this type.)     vpc  AWSVPC     VPC indicates whether to use an existing VPC or create a new one.\n    internal  []string    Internal is a list of private subnets to create (used for internal load balancers).\n    public  []string    Public is a list of public subnets to create (used for bastion and load balancers).\n    workers  []string    Workers is a list of worker subnets (private) to create (used for the VMs).\n    AWSProfile   (Appears on: CloudProfileSpec)  AWSProfile defines certain constraints and definitions for the AWS cloud.\n   Field Description      constraints  AWSConstraints     Constraints is an object containing constraints for certain values in the Shoot specification.\n    AWSVPC   (Appears on: AWSNetworks)  AWSVPC contains either an id (of an existing VPC) or the CIDR (for a VPC to be created).\n   Field Description      id  string    (Optional) ID is the AWS VPC id of an existing VPC.\n    cidr  string    (Optional) CIDR is a CIDR range for a new VPC.\n    AWSWorker   (Appears on: AWSCloud)  AWSWorker is the definition of a worker group.\n   Field Description      Worker  Worker      (Members of Worker are embedded into this type.)     volumeType  string    VolumeType is the type of the root volumes.\n    volumeSize  string    VolumeSize is the size of the root volume.\n    Addon   (Appears on: AddonClusterAutoscaler, Heapster, HelmTiller, Kube2IAM, KubeLego, KubernetesDashboard, Monocular, NginxIngress)  Addon also enabling or disabling a specific addon and is used to derive from.\n   Field Description      enabled  bool    Enabled indicates whether the addon is enabled or not.\n    AddonClusterAutoscaler   (Appears on: Addons)  ClusterAutoscaler describes configuration values for the cluster-autoscaler addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     Addons   (Appears on: ShootSpec)  Addons is a collection of configuration for specific addons which are managed by the Gardener.\n   Field Description      kubernetes-dashboard  KubernetesDashboard     (Optional) KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n    nginx-ingress  NginxIngress     (Optional) NginxIngress holds configuration settings for the nginx-ingress addon. DEPRECATED: This field will be removed in a future version.\n    cluster-autoscaler  AddonClusterAutoscaler     (Optional) ClusterAutoscaler holds configuration settings for the cluster autoscaler addon. DEPRECATED: This field will be removed in a future version.\n    heapster  Heapster     (Optional) Heapster holds configuration settings for the heapster addon. DEPRECATED: This field will be removed in a future version.\n    kube2iam  Kube2IAM     (Optional) Kube2IAM holds configuration settings for the kube2iam addon (only AWS). DEPRECATED: This field will be removed in a future version.\n    kube-lego  KubeLego     (Optional) KubeLego holds configuration settings for the kube-lego addon. DEPRECATED: This field will be removed in a future version.\n    monocular  Monocular     (Optional) Monocular holds configuration settings for the monocular addon. DEPRECATED: This field will be removed in a future version.\n    AdmissionPlugin   (Appears on: KubeAPIServerConfig)  AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n   Field Description      name  string    Name is the name of the plugin.\n    config  github.com/gardener/gardener/pkg/apis/core/v1alpha1.ProviderConfig     (Optional) Config is the configuration of the plugin.\n    Alerting   (Appears on: Monitoring)  Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n   Field Description      emailReceivers  []string    (Optional) MonitoringEmailReceivers is a list of recipients for alerts\n    Alicloud   (Appears on: Cloud)  Alicloud contains the Shoot specification for Alibaba cloud\n   Field Description      machineImage  ShootMachineImage     (Optional) ShootMachineImage holds information about the machine image to use for all workers. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    networks  AlicloudNetworks     Networks holds information about the Kubernetes and infrastructure networks.\n    workers  []AlicloudWorker     Workers is a list of worker groups.\n    zones  []string    Zones is a list of availability zones to deploy the Shoot cluster to, currently, only one is supported.\n    AlicloudConstraints   (Appears on: AlicloudProfile)  AlicloudConstraints is an object containing constraints for certain values in the Shoot specification\n   Field Description      dnsProviders  []DNSProviderConstraint     (Optional) DNSProviders contains constraints regarding allowed values of the \u0026lsquo;dns.provider\u0026rsquo; block in the Shoot specification.\n    kubernetes  KubernetesConstraints     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []AlicloudMachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    volumeTypes  []AlicloudVolumeType     VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    zones  []Zone     Zones contains constraints regarding allowed values for \u0026lsquo;zones\u0026rsquo; block in the Shoot specification.\n    AlicloudMachineType   (Appears on: AlicloudConstraints)  AlicloudMachineType defines certain machine types and zone constraints.\n   Field Description      MachineType  MachineType      (Members of MachineType are embedded into this type.)     zones  []string        AlicloudNetworks   (Appears on: Alicloud)  AlicloudNetworks holds information about the Kubernetes and infrastructure networks.\n   Field Description      K8SNetworks  K8SNetworks      (Members of K8SNetworks are embedded into this type.)     vpc  AlicloudVPC     VPC indicates whether to use an existing VPC or create a new one.\n    workers  []string    Workers is a CIDR of a worker subnet (private) to create (used for the VMs).\n    AlicloudProfile   (Appears on: CloudProfileSpec)  AlicloudProfile defines constraints and definitions in Alibaba Cloud environment.\n   Field Description      constraints  AlicloudConstraints     Constraints is an object containing constraints for certain values in the Shoot specification.\n    AlicloudVPC   (Appears on: AlicloudNetworks)  AlicloudVPC contains either an id (of an existing VPC) or the CIDR (for a VPC to be created).\n   Field Description      id  string    (Optional) ID is the Alicloud VPC id of an existing VPC.\n    cidr  string    (Optional) CIDR is a CIDR range for a new VPC.\n    AlicloudVolumeType   (Appears on: AlicloudConstraints)  AlicloudVolumeType defines certain volume types and zone constraints.\n   Field Description      VolumeType  VolumeType      (Members of VolumeType are embedded into this type.)     zones  []string        AlicloudWorker   (Appears on: Alicloud)  AlicloudWorker is the definition of a worker group.\n   Field Description      Worker  Worker      (Members of Worker are embedded into this type.)     volumeType  string    VolumeType is the type of the root volumes.\n    volumeSize  string    VolumeSize is the size of the root volume.\n    AuditConfig   (Appears on: KubeAPIServerConfig)  AuditConfig contains settings for audit of the api server\n   Field Description      auditPolicy  AuditPolicy     (Optional) AuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n    AuditPolicy   (Appears on: AuditConfig)  AuditPolicy contains audit policy for kube-apiserver\n   Field Description      configMapRef  Kubernetes core/v1.ObjectReference     (Optional) ConfigMapRef is a reference to a ConfigMap object in the same namespace, which contains the audit policy for the kube-apiserver.\n    AzureCloud   (Appears on: Cloud)  AzureCloud contains the Shoot specification for Azure.\n   Field Description      machineImage  ShootMachineImage     (Optional) ShootMachineImage holds information about the machine image to use for all workers. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    networks  AzureNetworks     Networks holds information about the Kubernetes and infrastructure networks.\n    resourceGroup  AzureResourceGroup     (Optional) ResourceGroup indicates whether to use an existing resource group or create a new one.\n    workers  []AzureWorker     Workers is a list of worker groups.\n    zones  []string    (Optional) Zones is a list of availability zones to deploy the Shoot cluster to.\n    AzureConstraints   (Appears on: AzureProfile)  AzureConstraints is an object containing constraints for certain values in the Shoot specification.\n   Field Description      dnsProviders  []DNSProviderConstraint     (Optional) DNSProviders contains constraints regarding allowed values of the \u0026lsquo;dns.provider\u0026rsquo; block in the Shoot specification.\n    kubernetes  KubernetesConstraints     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    volumeTypes  []VolumeType     VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    zones  []Zone     (Optional) Zones contains constraints regarding allowed values for \u0026lsquo;zones\u0026rsquo; block in the Shoot specification.\n    AzureDomainCount   (Appears on: AzureProfile)  AzureDomainCount defines the region and the count for this domain count value.\n   Field Description      region  string    Region is a region in Azure.\n    count  int    Count is the count value for the respective domain count.\n    AzureNetworks   (Appears on: AzureCloud)  AzureNetworks holds information about the Kubernetes and infrastructure networks.\n   Field Description      K8SNetworks  K8SNetworks      (Members of K8SNetworks are embedded into this type.)     vnet  AzureVNet     VNet indicates whether to use an existing VNet or create a new one.\n    workers  string    Workers is a CIDR of a worker subnet (private) to create (used for the VMs).\n    serviceEndpoints  []string    (Optional) ServiceEndpoints is a list of Azure ServiceEndpoints which should be associated with the worker subnet.\n    AzureProfile   (Appears on: CloudProfileSpec)  AzureProfile defines certain constraints and definitions for the Azure cloud.\n   Field Description      constraints  AzureConstraints     Constraints is an object containing constraints for certain values in the Shoot specification.\n    countUpdateDomains  []AzureDomainCount     CountUpdateDomains is list of Azure update domain counts for each region.\n    countFaultDomains  []AzureDomainCount     CountFaultDomains is list of Azure fault domain counts for each region.\n    AzureResourceGroup   (Appears on: AzureCloud)  AzureResourceGroup indicates whether to use an existing resource group or create a new one.\n   Field Description      name  string    Name is the name of an existing resource group.\n    AzureVNet   (Appears on: AzureNetworks)  AzureVNet indicates whether to use an existing VNet or create a new one.\n   Field Description      name  string    (Optional) Name is the AWS VNet name of an existing VNet.\n    resourceGroup  string    (Optional) ResourceGroup is the resourceGroup where the VNet is located.\n    cidr  string    (Optional) CIDR is a CIDR range for a new VNet.\n    AzureWorker   (Appears on: AzureCloud)  AzureWorker is the definition of a worker group.\n   Field Description      Worker  Worker      (Members of Worker are embedded into this type.)     volumeType  string    VolumeType is the type of the root volumes.\n    volumeSize  string    VolumeSize is the size of the root volume.\n    BackupProfile   (Appears on: SeedSpec)  BackupProfile contains the object store configuration for backups for shoot(currently only etcd).\n   Field Description      provider  CloudProvider     Provider is a provider name.\n    region  string    (Optional) Region is a region name.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.\n    Cloud   (Appears on: ShootSpec)  Cloud contains information about the cloud environment and their specific settings. It must contain exactly one key of the below cloud providers.\n   Field Description      profile  string    Profile is a name of a CloudProfile object.\n    region  string    Region is a name of a cloud provider region.\n    secretBindingRef  Kubernetes core/v1.LocalObjectReference     SecretBindingRef is a reference to a SecretBinding object.\n    seed  string    (Optional) Seed is the name of a Seed object.\n    aws  AWSCloud     (Optional) AWS contains the Shoot specification for the Amazon Web Services cloud.\n    azure  AzureCloud     (Optional) Azure contains the Shoot specification for the Microsoft Azure cloud.\n    gcp  GCPCloud     (Optional) GCP contains the Shoot specification for the Google Cloud Platform cloud.\n    openstack  OpenStackCloud     (Optional) OpenStack contains the Shoot specification for the OpenStack cloud.\n    alicloud  Alicloud     (Optional) Alicloud contains the Shoot specification for the Alibaba cloud.\n    packet  PacketCloud     (Optional) Packet contains the Shoot specification for the Packet cloud.\n    CloudControllerManagerConfig   (Appears on: Kubernetes)  CloudControllerManagerConfig contains configuration settings for the cloud-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     CloudProfileSpec   (Appears on: CloudProfile)  CloudProfileSpec is the specification of a CloudProfile. It must contain exactly one of its defined keys.\n   Field Description      aws  AWSProfile     (Optional) AWS is the profile specification for the Amazon Web Services cloud.\n    azure  AzureProfile     (Optional) Azure is the profile specification for the Microsoft Azure cloud.\n    gcp  GCPProfile     (Optional) GCP is the profile specification for the Google Cloud Platform cloud.\n    openstack  OpenStackProfile     (Optional) OpenStack is the profile specification for the OpenStack cloud.\n    alicloud  AlicloudProfile     (Optional) Alicloud is the profile specification for the Alibaba cloud.\n    packet  PacketProfile     (Optional) Packet is the profile specification for the Packet cloud.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of the Shoot cluster.\n    CloudProvider (string alias)\n  (Appears on: BackupProfile)  CloudProvider is a string alias.\nClusterAutoscaler   (Appears on: Kubernetes)  ClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n   Field Description      scaleDownUtilizationThreshold  float64    (Optional) ScaleDownUtilizationThreshold defines the threshold in % under which a node is being removed\n    scaleDownUnneededTime  Kubernetes meta/v1.Duration     (Optional) ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 10 mins).\n    scaleDownDelayAfterAdd  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 10 mins).\n    scaleDownDelayAfterFailure  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n    scaleDownDelayAfterDelete  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (defaults to ScanInterval).\n    scanInterval  Kubernetes meta/v1.Duration     (Optional) ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n    DNS   (Appears on: ShootSpec)  DNS holds information about the provider, the hosted zone id and the domain.\n   Field Description      domain  string    (Optional) Domain is the external available domain of the Shoot cluster.\n    secretName  string    (Optional) SecretName is a name of a secret containing credentials for the stated domain and the provider. When not specified, the Gardener will use the cloud provider credentials referenced by the Shoot and try to find respective credentials there. Specifying this field may override this behavior, i.e. forcing the Gardener to only look into the given secret.\n    provider  string    (Optional) Provider is the DNS provider type for the Shoot. Only relevant if not the default domain is used for this shoot.\n    includeDomains  []string    (Optional) IncludeDomains is a list of domains that shall be included. Only relevant if not the default domain is used for this shoot.\n    excludeDomains  []string    (Optional) ExcludeDomains is a list of domains that shall be excluded. Only relevant if not the default domain is used for this shoot.\n    includeZones  []string    (Optional) IncludeZones is a list of hosted zone IDs that shall be included. Only relevant if not the default domain is used for this shoot.\n    excludeZones  []string    (Optional) ExcludeZones is a list of hosted zone IDs that shall be excluded. Only relevant if not the default domain is used for this shoot.\n    DNSProviderConstraint   (Appears on: AWSConstraints, AlicloudConstraints, AzureConstraints, GCPConstraints, OpenStackConstraints, PacketConstraints)  DNSProviderConstraint contains constraints regarding allowed values of the \u0026lsquo;dns.provider\u0026rsquo; block in the Shoot specification.\n   Field Description      name  string    Name is the name of the DNS provider.\n    Extension   (Appears on: ShootSpec)  Extension contains type and provider information for Shoot extensions.\n   Field Description      type  string    Type is the type of the extension resource.\n    providerConfig  github.com/gardener/gardener/pkg/apis/core/v1alpha1.ProviderConfig     (Optional) ProviderConfig is the configuration passed to extension resource.\n    GCPCloud   (Appears on: Cloud)  GCPCloud contains the Shoot specification for GCP.\n   Field Description      machineImage  ShootMachineImage     (Optional) ShootMachineImage holds information about the machine image to use for all workers. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    networks  GCPNetworks     Networks holds information about the Kubernetes and infrastructure networks.\n    workers  []GCPWorker     Workers is a list of worker groups.\n    zones  []string    Zones is a list of availability zones to deploy the Shoot cluster to.\n    GCPConstraints   (Appears on: GCPProfile)  GCPConstraints is an object containing constraints for certain values in the Shoot specification.\n   Field Description      dnsProviders  []DNSProviderConstraint     (Optional) DNSProviders contains constraints regarding allowed values of the \u0026lsquo;dns.provider\u0026rsquo; block in the Shoot specification.\n    kubernetes  KubernetesConstraints     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    volumeTypes  []VolumeType     VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    zones  []Zone     Zones contains constraints regarding allowed values for \u0026lsquo;zones\u0026rsquo; block in the Shoot specification.\n    GCPNetworks   (Appears on: GCPCloud)  GCPNetworks holds information about the Kubernetes and infrastructure networks.\n   Field Description      K8SNetworks  K8SNetworks      (Members of K8SNetworks are embedded into this type.)     vpc  GCPVPC     (Optional) VPC indicates whether to use an existing VPC or create a new one.\n    workers  []string    Workers is a list of CIDRs of worker subnets (private) to create (used for the VMs).\n    internal  string    (Optional) Internal is a private subnet (used for internal load balancers).\n    GCPProfile   (Appears on: CloudProfileSpec)  GCPProfile defines certain constraints and definitions for the GCP cloud.\n   Field Description      constraints  GCPConstraints     Constraints is an object containing constraints for certain values in the Shoot specification.\n    GCPVPC   (Appears on: GCPNetworks)  GCPVPC indicates whether to use an existing VPC or create a new one.\n   Field Description      name  string    Name is the name of an existing GCP VPC.\n    GCPWorker   (Appears on: GCPCloud)  GCPWorker is the definition of a worker group.\n   Field Description      Worker  Worker      (Members of Worker are embedded into this type.)     volumeType  string    VolumeType is the type of the root volumes.\n    volumeSize  string    VolumeSize is the size of the root volume.\n    Gardener   (Appears on: SeedStatus, ShootStatus)  Gardener holds the information about the Gardener\n   Field Description      id  string    ID is the Docker container id of the Gardener which last acted on a Shoot cluster.\n    name  string    Name is the hostname (pod name) of the Gardener which last acted on a Shoot cluster.\n    version  string    Version is the version of the Gardener which last acted on a Shoot cluster.\n    GardenerDuration   (Appears on: HorizontalPodAutoscalerConfig)  GardenerDuration is a workaround for missing OpenAPI functions on metav1.Duration struct.\n   Field Description      Duration  time.Duration         Heapster   (Appears on: Addons)  Heapster describes configuration values for the heapster addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     HelmTiller   HelmTiller describes configuration values for the helm-tiller addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     Hibernation   (Appears on: ShootSpec)  Hibernation contains information whether the Shoot is suspended or not.\n   Field Description      enabled  bool    (Optional) Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot\u0026rsquo;s desired state is to be hibernated. If it is false or nil, the Shoot\u0026rsquo;s desired state is to be awaken.\n    schedules  []HibernationSchedule     (Optional) Schedules determine the hibernation schedules.\n    HibernationSchedule   (Appears on: Hibernation)  HibernationSchedule determines the hibernation schedule of a Shoot. A Shoot will be regularly hibernated at each start time and will be woken up at each end time. Start or End can be omitted, though at least one of each has to be specified.\n   Field Description      start  string    (Optional) Start is a Cron spec at which time a Shoot will be hibernated.\n    end  string    (Optional) End is a Cron spec at which time a Shoot will be woken up.\n    location  string    (Optional) Location is the time location in which both start and and shall be evaluated.\n    HorizontalPodAutoscalerConfig   (Appears on: KubeControllerManagerConfig)  HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      downscaleDelay  GardenerDuration     (Optional) The period since last downscale, before another downscale can be performed in horizontal pod autoscaler.\n    syncPeriod  GardenerDuration     (Optional) The period for syncing the number of pods in horizontal pod autoscaler.\n    tolerance  float64    (Optional) The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n    upscaleDelay  GardenerDuration     (Optional) The period since last upscale, before another upscale can be performed in horizontal pod autoscaler.\n    downscaleStabilization  GardenerDuration     (Optional) The configurable window at which the controller will choose the highest recommendation for autoscaling.\n    initialReadinessDelay  GardenerDuration     (Optional) The configurable period at which the horizontal pod autoscaler considers a Pod “not yet ready” given that it’s unready and it has transitioned to unready during that time.\n    cpuInitializationPeriod  GardenerDuration     (Optional) The period after which a ready pod transition is considered to be the first.\n    K8SNetworks   (Appears on: AWSNetworks, AlicloudNetworks, AzureNetworks, GCPNetworks, Networking, OpenStackNetworks, PacketNetworks)  K8SNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network.\n    pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    Kube2IAM   (Appears on: Addons)  Kube2IAM describes configuration values for the kube2iam addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     roles  []Kube2IAMRole     (Optional) Roles is list of AWS IAM roles which should be created by the Gardener.\n    Kube2IAMRole   (Appears on: Kube2IAM)  Kube2IAMRole allows passing AWS IAM policies which will result in IAM roles.\n   Field Description      name  string    Name is the name of the IAM role. Will be extended by the Shoot name.\n    description  string    Description is a human readable message indiciating what this IAM role can be used for.\n    policy  string    Policy is an AWS IAM policy document.\n    KubeAPIServerConfig   (Appears on: Kubernetes)  KubeAPIServerConfig contains configuration settings for the kube-apiserver.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     admissionPlugins  []AdmissionPlugin     (Optional) AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding configuration.\n    apiAudiences  []string    (Optional) APIAudiences are the identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. If serviceAccountConfig.issuer is configured and this is not, this defaults to a single element list containing the issuer URL.\n    auditConfig  AuditConfig     (Optional) AuditConfig contains configuration settings for the audit of the kube-apiserver.\n    enableBasicAuthentication  bool    (Optional) EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n    oidcConfig  OIDCConfig     (Optional) OIDCConfig contains configuration settings for the OIDC provider.\n    runtimeConfig  map[string]bool    (Optional) RuntimeConfig contains information about enabled or disabled APIs.\n    serviceAccountConfig  ServiceAccountConfig     (Optional) ServiceAccountConfig contains configuration settings for the service account handling of the kube-apiserver.\n    KubeControllerManagerConfig   (Appears on: Kubernetes)  KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     horizontalPodAutoscaler  HorizontalPodAutoscalerConfig     (Optional) HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n    nodeCIDRMaskSize  int    (Optional) NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24)\n    KubeLego   (Appears on: Addons)  KubeLego describes configuration values for the kube-lego addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     email  string    (Optional) Mail is the email address to register at Let\u0026rsquo;s Encrypt.\n    KubeProxyConfig   (Appears on: Kubernetes)  KubeProxyConfig contains configuration settings for the kube-proxy.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     mode  ProxyMode     (Optional) Mode specifies which proxy mode to use. defaults to IPTables.\n    KubeSchedulerConfig   (Appears on: Kubernetes)  KubeSchedulerConfig contains configuration settings for the kube-scheduler.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     KubeletConfig   (Appears on: Kubernetes, Worker)  KubeletConfig contains configuration settings for the kubelet.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     podPidsLimit  int64    (Optional) PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n    cpuCFSQuota  bool    (Optional) CPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n    cpuManagerPolicy  string    (Optional) CPUManagerPolicy allows to set alternative CPU management policies (default: none).\n    maxPods  int32    (Optional) MaxPods is the maximum number of Pods that are allowed by the Kubelet. Default: 110\n    evictionHard  KubeletConfigEviction     (Optional) EvictionHard describes a set of eviction thresholds (e.g. memory.available   evictionSoft  KubeletConfigEviction     (Optional) EvictionSoft describes a set of eviction thresholds (e.g. memory.available   evictionSoftGracePeriod  KubeletConfigEvictionSoftGracePeriod     (Optional) EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction. Default: memory.available: 1m30s nodefs.available: 1m30s nodefs.inodesFree: 1m30s imagefs.available: 1m30s imagefs.inodesFree: 1m30s\n    evictionMinimumReclaim  KubeletConfigEvictionMinimumReclaim     (Optional) EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure. Default: 0 for each resource\n    evictionPressureTransitionPeriod  Kubernetes meta/v1.Duration     (Optional) EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. Default: 4m0s\n    evictionMaxPodGracePeriod  int32    (Optional) EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. Default: 90\n    KubeletConfigEviction   (Appears on: KubeletConfig)  KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n   Field Description      memoryAvailable  string    (Optional) MemoryAvailable is the threshold for the free memory on the host server.\n    imageFSAvailable  string    (Optional) ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  string    (Optional) ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n    nodeFSAvailable  string    (Optional) NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  string    (Optional) NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n    KubeletConfigEvictionMinimumReclaim   (Appears on: KubeletConfig)  KubeletConfigEviction contains configuration for the kubelet eviction minimum reclaim.\n   Field Description      memoryAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MemoryAvailable is the threshold for the memory reclaim on the host server.\n    imageFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n    nodeFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n    KubeletConfigEvictionSoftGracePeriod   (Appears on: KubeletConfig)  KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n   Field Description      memoryAvailable  Kubernetes meta/v1.Duration     (Optional) MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n    imageFSAvailable  Kubernetes meta/v1.Duration     (Optional) ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n    imageFSInodesFree  Kubernetes meta/v1.Duration     (Optional) ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n    nodeFSAvailable  Kubernetes meta/v1.Duration     (Optional) NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n    nodeFSInodesFree  Kubernetes meta/v1.Duration     (Optional) NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n    Kubernetes   (Appears on: ShootSpec)  Kubernetes contains the version and configuration variables for the Shoot control plane.\n   Field Description      allowPrivilegedContainers  bool    (Optional) AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot (default: true).\n    kubeAPIServer  KubeAPIServerConfig     (Optional) KubeAPIServer contains configuration settings for the kube-apiserver.\n    cloudControllerManager  CloudControllerManagerConfig     (Optional) CloudControllerManager contains configuration settings for the cloud-controller-manager.\n    kubeControllerManager  KubeControllerManagerConfig     (Optional) KubeControllerManager contains configuration settings for the kube-controller-manager.\n    kubeScheduler  KubeSchedulerConfig     (Optional) KubeScheduler contains configuration settings for the kube-scheduler.\n    kubeProxy  KubeProxyConfig     (Optional) KubeProxy contains configuration settings for the kube-proxy.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    version  string    Version is the semantic Kubernetes version to use for the Shoot cluster.\n    clusterAutoscaler  ClusterAutoscaler     ClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n    KubernetesConfig   (Appears on: CloudControllerManagerConfig, KubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)  KubernetesConfig contains common configuration fields for the control plane components.\n   Field Description      featureGates  map[string]bool    (Optional) FeatureGates contains information about enabled feature gates.\n    KubernetesConstraints   (Appears on: AWSConstraints, AlicloudConstraints, AzureConstraints, GCPConstraints, OpenStackConstraints, PacketConstraints)  KubernetesConstraints contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n   Field Description      versions  []string    (Optional) Versions is the list of allowed Kubernetes versions for Shoot clusters (e.g., 1.13.1).\n    offeredVersions  []KubernetesVersion     (Optional) OfferedVersions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n    KubernetesDashboard   (Appears on: Addons)  KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     authenticationMode  string    (Optional) AuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n    KubernetesVersion   (Appears on: KubernetesConstraints)  KubernetesVersion contains the version code and optional expiration date for a kubernetes version\n   Field Description      version  string    Version is the kubernetes version\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which this kubernetes version is not supported any more. This has the following implications: 1) A shoot that opted out of automatic kubernetes system updates and that is running this kubernetes version will be forcefully updated to the latest kubernetes patch version for the current minor version 2) Shoot\u0026rsquo;s with this kubernetes version cannot be created\n    MachineImage   (Appears on: AWSConstraints, AlicloudConstraints, AzureConstraints, GCPConstraints, OpenStackConstraints, PacketConstraints)  MachineImage defines the name and multiple versions of the machine image in any environment.\n   Field Description      name  string    Name is the name of the image.\n    version  string    (Optional) DEPRECATED: This field will be removed in a future version.\n    versions  []MachineImageVersion     (Optional) Versions contains versions and expiration dates of the machine image\n    MachineImageVersion   (Appears on: MachineImage)  MachineImageVersion contains a version and an expiration date of a machine image\n   Field Description      version  string    Version is the version of the image.\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which a shoot that opted out of automatic operating system updates and that is running this image version will be forcefully updated to the latest version specified in the referenced cloud profile.\n    MachineType   (Appears on: AWSConstraints, AlicloudMachineType, AzureConstraints, GCPConstraints, OpenStackMachineType, PacketConstraints)  MachineType contains certain properties of a machine type.\n   Field Description      name  string    Name is the name of the machine type.\n    usable  bool    (Optional) Usable defines if the machine type can be used for shoot clusters.\n    cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     CPU is the number of CPUs for this machine type.\n    gpu  k8s.io/apimachinery/pkg/api/resource.Quantity     GPU is the number of GPUs for this machine type.\n    storage  MachineTypeStorage     (Optional) Storage is the amount of storage associated with the root volume of this machine type.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     Memory is the amount of memory for this machine type.\n    MachineTypeStorage   (Appears on: MachineType)  MachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n   Field Description      class  string    Class is the class of the storage type.\n    size  k8s.io/apimachinery/pkg/api/resource.Quantity     Size is the storage size.\n    type  string    Type is the type of the storage.\n    Maintenance   (Appears on: ShootSpec)  Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n   Field Description      autoUpdate  MaintenanceAutoUpdate     (Optional) AutoUpdate contains information about which constraints should be automatically updated.\n    timeWindow  MaintenanceTimeWindow     (Optional) TimeWindow contains information about the time window for maintenance operations.\n    MaintenanceAutoUpdate   (Appears on: Maintenance)  MaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n   Field Description      kubernetesVersion  bool    KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated.\n    machineImageVersion  bool    (Optional) MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n    MaintenanceTimeWindow   (Appears on: Maintenance)  MaintenanceTimeWindow contains information about the time window for maintenance operations.\n   Field Description      begin  string    Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, a random value will be computed.\n    end  string    End is the end of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, the value will be computed based on the \u0026ldquo;Begin\u0026rdquo; value.\n    Monitoring   (Appears on: ShootSpec)  Monitoring contains information about the monitoring configuration for the shoot.\n   Field Description      alerting  Alerting     (Optional) Alerting contains information about the alerting configuration for the shoot cluster.\n    Monocular   (Appears on: Addons)  Monocular describes configuration values for the monocular addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     Networking   (Appears on: ShootSpec)  Networking defines networking parameters for the shoot cluster.\n   Field Description      K8SNetworks  K8SNetworks      (Members of K8SNetworks are embedded into this type.)     type  string    Type identifies the type of the networking plugin\n    providerConfig  github.com/gardener/gardener/pkg/apis/core/v1alpha1.ProviderConfig     (Optional) ProviderConfig is the configuration passed to network resource.\n    NginxIngress   (Appears on: Addons)  NginxIngress describes configuration values for the nginx-ingress addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     loadBalancerSourceRanges  []string    (Optional) LoadBalancerSourceRanges is list of whitelist IP sources for NginxIngress\n    config  map[string]string    (Optional) Config contains custom configuration for the nginx-ingress-controller configuration. See https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n    externalTrafficPolicy  Kubernetes core/v1.ServiceExternalTrafficPolicyType     (Optional) ExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service exposing the nginx-ingress. Defaults to Cluster.\n    OIDCConfig   (Appears on: KubeAPIServerConfig)  OIDCConfig contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientID  string    (Optional) The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    (Optional) The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n    requiredClaims  map[string]string    (Optional) ATTENTION: Only meaningful for Kubernetes \u0026gt;= 1.11 key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \u0026ldquo;sub\u0026rdquo;)\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    clientAuthentication  OpenIDConnectClientAuthentication     (Optional) ClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n    OpenIDConnectClientAuthentication   (Appears on: OIDCConfig)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      secret  string    (Optional) The client Secret for the OpenID Connect client.\n    extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    OpenStackCloud   (Appears on: Cloud)  OpenStackCloud contains the Shoot specification for OpenStack.\n   Field Description      floatingPoolName  string    FloatingPoolName is the name of the floating pool to get FIPs from.\n    loadBalancerProvider  string    LoadBalancerProvider is the name of the load balancer provider in the OpenStack environment.\n    loadBalancerClasses  []OpenStackLoadBalancerClass     (Optional) LoadBalancerClasses available for a dedicated Shoot.\n    machineImage  ShootMachineImage     (Optional) ShootMachineImage holds information about the machine image to use for all workers. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    networks  OpenStackNetworks     Networks holds information about the Kubernetes and infrastructure networks.\n    workers  []OpenStackWorker     Workers is a list of worker groups.\n    zones  []string    Zones is a list of availability zones to deploy the Shoot cluster to.\n    OpenStackConstraints   (Appears on: OpenStackProfile)  OpenStackConstraints is an object containing constraints for certain values in the Shoot specification.\n   Field Description      dnsProviders  []DNSProviderConstraint     (Optional) DNSProviders contains constraints regarding allowed values of the \u0026lsquo;dns.provider\u0026rsquo; block in the Shoot specification.\n    floatingPools  []OpenStackFloatingPool     FloatingPools contains constraints regarding allowed values of the \u0026lsquo;floatingPoolName\u0026rsquo; block in the Shoot specification.\n    kubernetes  KubernetesConstraints     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    loadBalancerProviders  []OpenStackLoadBalancerProvider     LoadBalancerProviders contains constraints regarding allowed values of the \u0026lsquo;loadBalancerProvider\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []OpenStackMachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    zones  []Zone     Zones contains constraints regarding allowed values for \u0026lsquo;zones\u0026rsquo; block in the Shoot specification.\n    OpenStackFloatingPool   (Appears on: OpenStackConstraints)  OpenStackFloatingPool contains constraints regarding allowed values of the \u0026lsquo;floatingPoolName\u0026rsquo; block in the Shoot specification.\n   Field Description      name  string    Name is the name of the floating pool.\n    loadBalancerClasses  []OpenStackLoadBalancerClass     (Optional) LoadBalancerClasses contains a list of supported labeled load balancer network settings.\n    OpenStackLoadBalancerClass   (Appears on: OpenStackCloud, OpenStackFloatingPool)  OpenStackLoadBalancerClass defines a restricted network setting for generic LoadBalancer classes usable in CloudProfiles.\n   Field Description      name  string    Name is the name of the LB class\n    floatingSubnetID  string    (Optional) FloatingSubnetID is the subnetwork ID of a dedicated subnet in floating network pool.\n    floatingNetworkID  string    (Optional) FloatingNetworkID is the network ID of the floating network pool.\n    subnetID  string    (Optional) SubnetID is the ID of a local subnet used for LoadBalancer provisioning. Only usable if no FloatingPool configuration is done.\n    OpenStackLoadBalancerProvider   (Appears on: OpenStackConstraints)  OpenStackLoadBalancerProvider contains constraints regarding allowed values of the \u0026lsquo;loadBalancerProvider\u0026rsquo; block in the Shoot specification.\n   Field Description      name  string    Name is the name of the load balancer provider.\n    OpenStackMachineType   (Appears on: OpenStackConstraints)  OpenStackMachineType contains certain properties of a machine type in OpenStack\n   Field Description      MachineType  MachineType      (Members of MachineType are embedded into this type.)     volumeType  string    VolumeType is the type of that volume.\n    volumeSize  k8s.io/apimachinery/pkg/api/resource.Quantity     VolumeSize is the amount of disk storage for this machine type.\n    OpenStackNetworks   (Appears on: OpenStackCloud)  OpenStackNetworks holds information about the Kubernetes and infrastructure networks.\n   Field Description      K8SNetworks  K8SNetworks      (Members of K8SNetworks are embedded into this type.)     router  OpenStackRouter     (Optional) Router indicates whether to use an existing router or create a new one.\n    workers  []string    Workers is a list of CIDRs of worker subnets (private) to create (used for the VMs).\n    OpenStackProfile   (Appears on: CloudProfileSpec)  OpenStackProfile defines certain constraints and definitions for the OpenStack cloud.\n   Field Description      constraints  OpenStackConstraints     Constraints is an object containing constraints for certain values in the Shoot specification.\n    keystoneURL  string    KeyStoneURL is the URL for auth{n,z} in OpenStack (pointing to KeyStone).\n    dnsServers  []string    (Optional) DNSServers is a list of IPs of DNS servers used while creating subnets.\n    dhcpDomain  string    (Optional) DHCPDomain is the dhcp domain of the OpenStack system configured in nova.conf. Only meaningful for Kubernetes 1.10.1+. See https://github.com/kubernetes/kubernetes/pull/61890 for details.\n    requestTimeout  string    (Optional) RequestTimeout specifies the HTTP timeout against the OpenStack API.\n    OpenStackRouter   (Appears on: OpenStackNetworks)  OpenStackRouter indicates whether to use an existing router or create a new one.\n   Field Description      id  string    ID is the router id of an existing OpenStack router.\n    OpenStackWorker   (Appears on: OpenStackCloud)  OpenStackWorker is the definition of a worker group.\n   Field Description      Worker  Worker      (Members of Worker are embedded into this type.)     PacketCloud   (Appears on: Cloud)  PacketCloud contains the Shoot specification for Packet cloud\n   Field Description      machineImage  ShootMachineImage     (Optional) ShootMachineImage holds information about the machine image to use for all workers. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    networks  PacketNetworks     Networks holds information about the Kubernetes and infrastructure networks.\n    workers  []PacketWorker     Workers is a list of worker groups.\n    zones  []string    Zones is a list of availability zones to deploy the Shoot cluster to, currently, only one is supported.\n    PacketConstraints   (Appears on: PacketProfile)  PacketConstraints is an object containing constraints for certain values in the Shoot specification\n   Field Description      dnsProviders  []DNSProviderConstraint     (Optional) DNSProviders contains constraints regarding allowed values of the \u0026lsquo;dns.provider\u0026rsquo; block in the Shoot specification.\n    kubernetes  KubernetesConstraints     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    volumeTypes  []VolumeType     VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    zones  []Zone     Zones contains constraints regarding allowed values for \u0026lsquo;zones\u0026rsquo; block in the Shoot specification.\n    PacketNetworks   (Appears on: PacketCloud)  PacketNetworks holds information about the Kubernetes and infrastructure networks.\n   Field Description      K8SNetworks  K8SNetworks      (Members of K8SNetworks are embedded into this type.)     PacketProfile   (Appears on: CloudProfileSpec)  PacketProfile defines constraints and definitions in Packet Cloud environment.\n   Field Description      constraints  PacketConstraints     Constraints is an object containing constraints for certain values in the Shoot specification.\n    PacketWorker   (Appears on: PacketCloud)  PacketWorker is the definition of a worker group.\n   Field Description      Worker  Worker      (Members of Worker are embedded into this type.)     volumeType  string    VolumeType is the type of the root volumes.\n    volumeSize  string    VolumeSize is the size of the root volume.\n    ProjectPhase (string alias)\n  (Appears on: ProjectStatus)  ProjectPhase is a label for the condition of a project at the current time.\nProjectSpec   (Appears on: Project)  ProjectSpec is the specification of a Project.\n   Field Description      createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []Kubernetes rbac/v1.Subject     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user that should be part of this project with full permissions to manage it.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    viewers  []Kubernetes rbac/v1.Subject     (Optional) Viewers is a list of subjects representing a user name, an email address, or any other identifier of a user that should be part of this project with limited permissions to only view some resources.\n    ProjectStatus   (Appears on: Project)  ProjectStatus holds the most recently observed status of the project.\n   Field Description      observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this project.\n    phase  ProjectPhase     Phase is the current phase of the project.\n    ProxyMode (string alias)\n  (Appears on: KubeProxyConfig)  ProxyMode available in Linux platform: \u0026lsquo;userspace\u0026rsquo; (older, going to be EOL), \u0026lsquo;iptables\u0026rsquo; (newer, faster), \u0026lsquo;ipvs\u0026rsquo;(newest, better in performance and scalability).\nAs of now only \u0026lsquo;iptables\u0026rsquo; and \u0026lsquo;ipvs\u0026rsquo; is supported by Gardener.\nIn Linux platform, if the iptables proxy is selected, regardless of how, but the system\u0026rsquo;s kernel or iptables versions are insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to \u0026lsquo;ipvs\u0026rsquo;, and the fall back path is firstly iptables and then userspace.\nQuotaScope (string alias)\n  (Appears on: QuotaSpec)  QuotaScope is a string alias.\nQuotaSpec   (Appears on: Quota)  QuotaSpec is the specification of a Quota.\n   Field Description      clusterLifetimeDays  int    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  QuotaScope     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n    SeedCloud   (Appears on: SeedSpec)  SeedCloud defines the cloud profile and the region this Seed cluster belongs to.\n   Field Description      profile  string    Profile is the name of a cloud profile.\n    region  string    Region is a name of a region.\n    SeedNetworks   (Appears on: SeedSpec)  SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network.\n    pods  string    Pods is the CIDR of the pod network.\n    services  string    Services is the CIDR of the service network.\n    shootDefaults  ShootNetworks     (Optional) ShootDefaults contains the default networks CIDRs for shoots.\n    SeedSpec   (Appears on: Seed)  SeedSpec is the specification of a Seed.\n   Field Description      cloud  SeedCloud     Cloud defines the cloud profile and the region this Seed cluster belongs to.\n    ingressDomain  string    IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    visible  bool    (Optional) Visible labels the Seed cluster as selectable for the seedfinder admission controller.\n    protected  bool    (Optional) Protected prevent that the Seed Cluster can be used for regular Shoot cluster control planes.\n    backup  BackupProfile     (Optional) Backup holds the object store configuration for the backups of shoot(currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for Shoots associated with this Seed. If backup field is present in Seed, then backups of the etcd from Shoot controlplane will be stored under the configured object store.\n    SeedStatus   (Appears on: Seed)  SeedStatus holds the most recently observed status of the Seed cluster.\n   Field Description      conditions  []github.com/gardener/gardener/pkg/apis/core/v1alpha1.Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    gardener  Gardener     (Optional) Gardener holds information about the Gardener which last acted on the Seed.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the Kubernetes version of the seed cluster.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the Seed\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    ServiceAccountConfig   (Appears on: KubeAPIServerConfig)  ServiceAccountConfig is the kube-apiserver configuration for service accounts.\n   Field Description      issuer  string    (Optional) Issuer is the identifier of the service account token issuer. The issuer will assert this identifier in \u0026ldquo;iss\u0026rdquo; claim of issued tokens. This value is a string or URI.\n    signingKeySecretName  Kubernetes core/v1.LocalObjectReference     (Optional) SigningKeySecret is a reference to a secret that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the \u0026lsquo;TokenRequest\u0026rsquo; feature gate.)\n    ShootMachineImage   (Appears on: AWSCloud, Alicloud, AzureCloud, GCPCloud, OpenStackCloud, PacketCloud, Worker)  MachineImage defines the name and the version of the shoot\u0026rsquo;s machine image in any environment. Has to be defined in the respective CloudProfile.\n   Field Description      name  string    Name is the name of the image.\n    version  string    Version is the version of the shoot\u0026rsquo;s image.\n    providerConfig  github.com/gardener/gardener/pkg/apis/core/v1alpha1.ProviderConfig     (Optional) ProviderConfig is the shoot\u0026rsquo;s individual configuration passed to an extension resource.\n    ShootNetworks   (Appears on: SeedNetworks)  ShootNetworks contains the default networks CIDRs for shoots.\n   Field Description      pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    ShootSpec   (Appears on: Shoot)  ShootSpec is the specification of a Shoot.\n   Field Description      addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloud  Cloud     Cloud contains information about the cloud environment and their specific settings.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    ShootStatus   (Appears on: Shoot)  ShootStatus holds the most recently observed status of the Shoot cluster.\n   Field Description      conditions  []github.com/gardener/gardener/pkg/apis/core/v1alpha1.Condition     (Optional) Conditions represents the latest available observations of a Shoots\u0026rsquo;s current state.\n    constraints  []github.com/gardener/gardener/pkg/apis/core/v1alpha1.Condition     (Optional) Constraints represents conditions of a Shoot\u0026rsquo;s current state that constraint some operations on it.\n    gardener  Gardener     Gardener holds information about the Gardener which last acted on the Shoot.\n    lastOperation  github.com/gardener/gardener/pkg/apis/core/v1alpha1.LastOperation     (Optional) LastOperation holds information about the last operation on the Shoot.\n    lastError  github.com/gardener/gardener/pkg/apis/core/v1alpha1.LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    lastErrors  []github.com/gardener/gardener/pkg/apis/core/v1alpha1.LastError     (Optional) LastErrors holds information about the last occurred error(s) during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the Shoot\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    retryCycleStartTime  Kubernetes meta/v1.Time     (Optional) RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation must be retried until we give up).\n    seed  string    Seed is the name of the seed cluster that runs the control plane of the Shoot. This value is only written after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n    hibernated  bool    (Optional) IsHibernated indicates whether the Shoot is currently hibernated.\n    technicalID  string    TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and basically everything that is related to this particular Shoot.\n    uid  k8s.io/apimachinery/pkg/types.UID     UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters. It is used to compute unique hashes.\n    VolumeType   (Appears on: AWSConstraints, AlicloudVolumeType, AzureConstraints, GCPConstraints, PacketConstraints)  VolumeType contains certain properties of a volume type.\n   Field Description      name  string    Name is the name of the volume type.\n    usable  bool    (Optional) Usable defines if the volume type can be used for shoot clusters.\n    class  string    Class is the class of the volume type.\n    Worker   (Appears on: AWSWorker, AlicloudWorker, AzureWorker, GCPWorker, OpenStackWorker, PacketWorker)  Worker is the base definition of a worker group.\n   Field Description      name  string    Name is the name of the worker group.\n    machineType  string    MachineType is the machine type of the worker group.\n    machineImage  ShootMachineImage     (Optional) ShootMachineImage holds information about the machine image to use for all workers. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    autoScalerMin  int    AutoScalerMin is the minimum number of VMs to create.\n    autoScalerMax  int    AutoScalerMin is the maximum number of VMs to create.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n    Zone   (Appears on: AWSConstraints, AlicloudConstraints, AzureConstraints, GCPConstraints, OpenStackConstraints, PacketConstraints)  Zone contains certain properties of an availability zone.\n   Field Description      region  string    Region is a region name.\n    names  []string    (Optional) Names is a list of availability zone names in this region.\n      Generated with gen-crd-api-reference-docs on git commit 79c676930. \n"
},
{
	"uri": "https://gardener.cloud/components/gardner/",
	"title": "Gardener",
	"tags": [],
	"description": "",
	"content": " Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides extensibility concepts which allow support for any cloud or infrastructure provider. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a certain quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds. This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nPlease find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog post on kubernetes.io.\nK8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     GCP TBD1          AWS TBD1          OpenStack TBD1          Azure TBD1          Alicloud TBD1     N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A    1 will be available in near future\nBesides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about out project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "
},
{
	"uri": "https://gardener.cloud/",
	"title": "Gardener",
	"tags": [],
	"description": "",
	"content": "Universal Kubernetes at Scale  \u0026#10003; Thousands of Clusters \u0026#10003; Fully Managed \u0026#10003; Multi-Cloud / IaaS \u0026#10003; Homogeneous \u0026#10003; Free \u0026#10003; All Regions  Generate Your Installer Right Now    Many Open Source tools exist which help in creating and updating single Kubernetes clusters. However, the more clusters you need the harder it becomes to operate, monitor, manage and keep all of them alive and up-to-date. That is exactly what project Gardener focuses on.       Our Mission  100% Kubernetes Inspired by the possibilities of Kubernetes and the ability to self-host, the foundation of Gardener is Kubernetes itself. Gardener applies a special pattern catering to the needs of operating a huge number of clusters with minimal total cost of ownership. Overall, reusing Kubernetes primitives in Gardeners core architecture simplifies deployment, scaling \u0026amp; patching/updating of all control planes under Gardener's management.  100% Fast \u0026amp; Simple We agree that cloud native will become the foundation and the de facto standard for shipping software fast, simple and reliable using Kubernetes as the new \"virtualisation\" \u0026amp; deployment fabric/underlay. We expect that Kubernetes will become a forcing function.       Create clusters via self service Simple and powerful  Despite requiring only the familiar kubectl command line tool for managing all of Gardener, we provide a central dashboard for comfortable interaction. It enables users to easily keep track of their clusters’ health, and operators to monitor, debug, and analyze the clusters they are responsible for.    More focused on the duties of developers and operators, the Gardener command line client gardenctl simplifies administrative tasks by introducing easy higher-level abstractions with simple commands that allow to condense and multiplex information \u0026 actions from/to a set of seed and shoot clusters.  The clusters are self-healing, auto-scaling - and if you choose to - also auto-updating. The Gardener will show you details on your cluster like the Kubernetes dashboard URL or the credentials you need to access it via kubectl.  For a high level discussion on the motivation for Gardener and its architecture, read the blog on kubernetes.io     consistency on any infrastructure Cloud Agnostic Kubernetes   Gardener Kubernetes Engine offers a fast and production-ready deployment of certified Kubernetes clusters at all major cloud providers. Deliver any infrastructure reliably, consistently and cost-effectively.      Full transparency of CNCF conformance test results 100% Kubernetes  Starting from Kubernetes release 1.10 the conformance test results of clusters provided by the Gardener are published on Testgrid.  One major goal of Gardener is to provide Kubernetes clusters which completely satisfy the requirements of the CNCF.     For this the CNCF launched in 2017 a certification program. As of the Kubernetes release 1.8 the clusters created by the Gardener are already officially certified by the CNCF. In addition, the CNCF offers the public dashboard Testgrid, where besides others the conformance test results are published. A typical use case is to add a publish step into an existing CI/CD pipelines as a target for the conformance test results. Testgrid offers an easy way to investigate historical conformance test results by visualizing the result of each test and by providing access to the test logs.     start contributing Project members  SAP is working on Gardener since mid 2017 and is focused on building up a project that can easily be evolved and extended. Consequently, we are looking for further partners and contributors to the project now. As outlined above, we completely rely on Kubernetes primitives, add-ons, and specifications and adapt its innovative cloud native approach. We are looking forward to aligning with and contributing to the Kubernetes community, especially with the upcoming cluster and machine specifications from SIG Cluster Lifecycle.     Feedback    The Gardener is fully Open Source and developed in the public on GitHub. Feedback is always welcome. Please report bugs or suggestions about our Kubernetes clusters as such at the gardener project and about the user interface at the dashboard project.      Start Contributing If you also see the potential of the Gardener project then please learn more about it on GitHub.   Github      "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/gardener_certificate_management/",
	"title": "Gardener Certificate Management",
	"tags": [],
	"description": "Configure Certificate Management For Shoot Clusters",
	"content": " Gardener Certificate Management Introduction Gardener comes with an extension that enables shoot owners to request X.509 compliant certificates for shoot domains.\nExtension Installation The Shoot-Cert-Service extension can be deployed and configured via Gardener\u0026rsquo;s native resource ControllerRegistration.\nPrerequisites To let the Shoot-Cert-Service operate properly, you need to have: - a DNS service in your seed - contact details and optionally a private key for a pre-existing Let\u0026rsquo;s Encrypt account\nControllerRegistration An example of a ControllerRegistration for the Shoot-Cert-Service can be found here: https://github.com/gardener/gardener-extensions/blob/master/controllers/extension-shoot-cert-service/example/controller-registration.yaml\nConfiguration The ControllerRegistration contains a Helm chart which eventually deploy the Shoot-Cert-Service to seed clusters. It offers some configuration options, mainly to set up a default issuer for shoot clusters. With a default issuer, pre-existing Let\u0026rsquo;s Encrypt accounts can be used and shared with shoot clusters (See \u0026ldquo;One Account or Many?\u0026rdquo; of the Integration Guide).\n Please keep the Let\u0026rsquo;s Encrypt Rate Limits in mind when using this shared account model. Depending on the amount of shoots and domains it is recommended to use an account with increased rate limits.\n apiVersion: core.gardener.cloud/v1alpha1 kind: ControllerRegistration ... values: certificateConfig: defaultIssuer: acme: email: foo@example.com privateKey: |- -----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- server: https://acme-v02.api.letsencrypt.org/directory name: default-issuer  If the Shoot-Cert-Service should be enabled for every shoot cluster in your Gardener managed environment, you need to globally enable it in the ControllerRegistration:\napiVersion: core.gardener.cloud/v1alpha1 kind: ControllerRegistration ... resources: - globallyEnabled: true kind: Extension type: shoot-cert-service  Alternatively, you\u0026rsquo;re given the option to only enable the service for certain shoots:\nkind: Shoot apiVersion: core.gardener.cloud/v1alpha1 ... spec: extensions: - type: shoot-cert-service ...   #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/gardener-cookies/",
	"title": "Gardener Cookies",
	"tags": [],
	"description": "Gardener Cookies",
	"content": " Green Tea Matcha Cookies For a team event during the Christmas season we decided to completely reinterpret the topic cookies. :-)\n            .sh__item img { object-fit: cover !important; }  Matcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies are perfect with tea. And of course they fit perfectly to our logo\nIngredients  1 stick butter, softened ⅞ cup of granulated sugar 1 cup + 2 tablespoons all-purpose flour 2 eggs 1¼ tablespoons culinary grade matcha powder 1 teaspoon baking powder Pinch of salt  Instructions  Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy. Gently incorporate the eggs to the butter mixture one at a time. In a separate bowl, sift together all the dry ingredients. Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you\u0026rsquo;ve incorporated all the remaining flour mixture. The dough should be a beautiful green color. Chill the dough for at least an hour - up to overnight. The longer the better! Preheat your oven to 325 F. Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet. Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely. Remove and let cool on a rack and enjoy!  Note Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/gardener_dns_management/",
	"title": "Gardener DNS Management for Shoots",
	"tags": [],
	"description": "Configure DNS Management For Shoot Clusters",
	"content": " Gardener DNS Management for Shoots Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. To support this the gardener must be installed with the shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\n #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  Configuration A general description for configuring the DNS management of the gardener can be found here.\nTo generally enable the DNS management for shoot objects the shoot-dns-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the registration must set the globallyEnabled flag to true.\nspec: resources: - kind: Extension type: shoot-dns-service globallyEnabled: true  Providing Base Domains usable for a Shoot So, far only the external DNS domain of a shoot already used for the kubernetes api server and ingress DNS names can be used for managed DNS names. This is either the shoot domain as subdomain of the default domain configured for the gardener installation or a dedicated domain with dedicated access credentials configured for a dedicated shoot via the shoot manifest.\nShoot Feature Gate If the shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster), it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must explicitly add the shoot-dns-service extension.\n... spec: extensions: - type: shoot-dns-service ...  "
},
{
	"uri": "https://gardener.cloud/about/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides extensibility concepts which allow support for any cloud or infrastructure provider. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a certain quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds. This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nPlease find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog post on kubernetes.io.\nK8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     GCP TBD1          AWS TBD1          OpenStack TBD1          Azure TBD1          Alicloud TBD1     N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A    1 will be available in near future\nBesides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about out project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/app/https/",
	"title": "HTTPS with self Signed Certificate",
	"tags": [],
	"description": "HTTPS with self Signed Certificate",
	"content": " Configuring ingress with front-end TLS It is alyways recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json  Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026quot;CN\u0026quot;: \u0026quot;Gardener Self Signed CA\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot;, \u0026quot;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;US\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;CA\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;San Francisco\u0026quot; } ] }  And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -  You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json  Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026quot;CN\u0026quot;: \u0026quot;Gardener Self Signed CA\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot;, \u0026quot;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;US\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;CA\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;San Francisco\u0026quot; } ] }  Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server  You\u0026rsquo;ll get following files: - server-key.pem - server.csr - server.pem\nConfigure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem  Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion: v1 kind: Service metadata: labels: app: node-server name: node-svc namespace: default spec: type: NodePort ports: - port: 8080 selector: app: node-server --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: node-ingress spec: tls: - hosts: - ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com secretName: tls-secret rules: - host: ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: node-svc servicePort: 8080  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/secure-setup/",
	"title": "Hardening the Gardener Community Setup",
	"tags": [],
	"description": "Hardening the Gardener Community Setup",
	"content": " Hardening the Gardener Community Setup Context Gardener stakeholders in the Open Source community usually use the Gardener Setup Scripts, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:\n Seed cluster Shoot cluster  As Alban Crequy from Kinvolk has recommended in his recent Gardener blog Auditing Kubernetes for Secure Setup the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.\nRecommendations Mitigation for Gardener CVE-2018-2475 The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.\n Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account Create a Shoot cluster in a different IaaS account As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster Register this newly created Shoot cluster as a Seed cluster in the Gardener End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).  A tutorial on how to create a shooted seed cluster can be found here.\nThe rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.\nWhen you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener CVE-2018-2475 anymore.\nMitigation for Kubernetes CVE-2018-1002105 In addition when you follow the recommendations in the recent Gardener Security Announcement you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.\nAlternative Approach For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his blog directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_50/",
	"title": "Hardening the Gardener Community Setup",
	"tags": [],
	"description": "",
	"content": "The Gardener project team has analyzed the impact of the Gardener CVE-2018-2475 and the Kubernetes CVE-2018-1002105 on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.\nRead more on Hardening the Gardener Community Setup.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/hibernate-cluster/",
	"title": "Hibernate a Cluster",
	"tags": [],
	"description": "Hibernate a Cluster to save money",
	"content": " Problem If you have built a customer scenario for demo purposes, you don\u0026rsquo;t want to run the cluster all the time. The costs would exceed here very fast. You can setup the cluster again for each demo, thanks to Helm this works relatively well, but takes a long time depending on the infrastructure. Furthermore not all 3rd party services are connected yet.\n Set a Gardener Cluster in Hibernate Mode Fortunately the Gardener offers the possibility to scale the Worker Nodes down to \u0026ldquo;Zero\u0026rdquo; without much effort. Follow the slide deck below to bring your Gardner Cluster in Hibernate Mode\n The mechanism to hibernate a cluster has changed. It is not necessary to scale down the workergroups to zero. You can now press the hibernate button or add the property hibernation to the shoot YAML.\n            "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_40/",
	"title": "Hibernate a Cluster to save money",
	"tags": [],
	"description": "",
	"content": "You want to experiment with Kubernetes or have set up a customer scenario, but you don\u0026rsquo;t want to run the cluster 24 / 7 for reasons of cost?\n The Gardener gives you the possibility to scale your cluster down to zero nodes.\n..read some more on Hibernate a Cluster.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/debug-a-pod/",
	"title": "How to debug a pod",
	"tags": [],
	"description": "Your pod doesn&#39;t run as expected. Are there any log files? Where? How could I debug a pod?",
	"content": " Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash:\n error during image pull caused by e.g. wrong/missing secrets or wrong/missing image the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets liveness probe failed too high resource consumption (memory and/or CPU) or too strict quota settings persistent volumes can\u0026rsquo;t be created/mounted the container image is not updated  Basically, the commands kubectl logs ... and kubectl describe ... with additional parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you\u0026rsquo;ll find some basic approaches to get some ideas what went wrong.\nRemarks:\n- Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems - There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities - The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u0026lt;your-namespace\u0026gt; to select the target namespace. They require Kubernetes release \u0026ge; 1.8.\nPrerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren\u0026rsquo;t running.\nError caused by wrong image name You run kubectl describe pod \u0026lt;your-pod\u0026gt; \u0026lt;your-namespace\u0026gt; to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nkubectl delete pod termination-demo  Next, create a resource based on the yaml content below\napiVersion: v1 kind: Pod metadata: name: termination-demo spec: containers: - name: termination-demo-container image: debiann command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026quot;]  kubectl describe pod termination-demo lists the following content in the Event section\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal 2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026quot;default-token-sgccm\u0026quot; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026quot;debiann\u0026quot; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \u0026quot;debiann\u0026quot;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found 2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod 2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \u0026quot;debiann\u0026quot;  The error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nApp runs in an error state caused by missing ConfigMaps or Secrets This example illustrates the behavior in case of the app expecting environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectl delete deployment termination-demo kubectl delete configmaps app-env  Next, deploy this manifest\napiVersion: apps/v1 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sed \\\u0026quot;s/foo/bar/\\\u0026quot; \u0026lt; $MYFILE\u0026quot;]  Now, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal 19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026quot;default-token-sgccm\u0026quot; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026quot;debian\u0026quot; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \u0026quot;debian\u0026quot; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container 19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container 19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod  The command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file  So you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion: v1 kind: ConfigMap metadata: name: app-env data: MYFILE: \u0026quot;/etc/profile\u0026quot; --- apiVersion: apps/v1 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sed \\\u0026quot;s/foo/bar/\\\u0026quot; \u0026lt; $MYFILE\u0026quot;] envFrom: - configMapRef: name: app-env  Note that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and runs to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion: v1 kind: ConfigMap metadata: name: app-env data: MYFILE: \u0026quot;/etc/profile\u0026quot; SLEEP: \u0026quot;5\u0026quot; --- apiVersion: apps/v1 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] # args: [\u0026quot;-c\u0026quot;, \u0026quot;sed \\\u0026quot;s/foo/bar/\\\u0026quot; \u0026lt; $MYFILE\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;while true; do sleep $SLEEP; echo sleeping; done;\u0026quot;] envFrom: - configMapRef: name: app-env  Too high resource consumption or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. Find more details in Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u0026lt;= requests \u0026lt;= limit. For both settings you need to consider the total amount of resources the available nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption of your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectl delete deployment termination-demo kubectl delete configmaps app-env  Next, adapt the cpu in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion: apps/v1 kind: Deployment metadata: name: termination-demo labels: app: termination-demo spec: replicas: 1 selector: matchLabels: app: termination-demo template: metadata: labels: app: termination-demo spec: containers: - name: termination-demo-container image: debian command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026quot;] resources: requests: cpu: \u0026quot;600m\u0026quot;  The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw Name: termination-demo-fdb7bb7d9-mzvfw Namespace: default ... Containers: termination-demo-container: Image: debian Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: /bin/sh Args: -c sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log Requests: cpu: 6 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro) Conditions: Type Status PodScheduled False Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu.  More details in - Managing Compute Resources for Containters - Resource Quality of Service in Kubernetes\nRemark:\n- This example works similarly when specifying a too high request for memory - In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn\u0026rsquo;t reach the maximum number of worker nodes - If your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output\nWhy was the container image not updated? You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn\u0026rsquo;t change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag whenever you changed anything in your image (see Configuration Best Practices).\nFind more details in FAQ Container Image not updating\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/app/",
	"title": "HowTos",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/",
	"title": "HowTos",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/content_trust/",
	"title": "Integrity and Immutability",
	"tags": [],
	"description": "Ensure that you get always the right image",
	"content": " Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML\u0026rsquo;s or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9  or\napiVersion: apps/v1 kind: Deployment metadata: name: rss-site spec: replicas: 1 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: front-end image: nginx:1.13.9 ports: - containerPort: 80  But Tags are mutable and humans are prone to error. Not a good combination. Here we’ll dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de  You can now make sure that the same image is always loaded at every deployment. It doesn\u0026rsquo;t matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. This solves the problem of trust.\nIn addition you should scan all images for known vulnerabilities, this can fill another book\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/bash_kubeconfig/",
	"title": "Kubeconfig context as bash prompt",
	"tags": [],
	"description": "Expose the active kubeconfig into the bash",
	"content": " Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, the kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copying the used configuration always to the right place?\nExport the KUBECONFIG enviroment variable bash$ export KUBECONFIG=\u0026lt;PATH-TO-M\u0026gt;-CONFIG\u0026gt;/kubeconfig-dev.yaml  How to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. bash$  Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it worth to be added here. Edit your ~/.bash_profile and add the following code snippet to show the current k8s context in the shell\u0026rsquo;s prompt.\nprompt_k8s(){ k8s_current_context=$(kubectl config current-context 2\u0026gt; /dev/null) if [[ $? -eq 0 ]] ; then echo -e \u0026quot;(${k8s_current_context}) \u0026quot;; fi } PS1+='$(prompt_k8s)'  After this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$  Note the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\Windows­PowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\nfunction prompt_k8s { $k8s_current_context = (kubectl config current-context) | Out-String if($?) { return $k8s_current_context }else { return \u0026quot;No K8S contenxt found\u0026quot; } } $host.ui.rawui.WindowTitle = prompt_k8s  If you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/antipattern/",
	"title": "Kubernetes Antipatterns",
	"tags": [],
	"description": "Common Antipatterns for Kubernetes and Docker",
	"content": " This HowTo covers common kubernetes antipatterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.\nWatch the very good presentation by Liz Rice at the KubeCon 2018 \nUse RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and add a user to it. Use the USER command to switch to this user. Note that you may also consider to provide an explicit UID/GID if required.\nFor Example:\nARG GF_UID=\u0026quot;500\u0026quot; ARG GF_GID=\u0026quot;500\u0026quot; # add group \u0026amp; user RUN groupadd -r -g $GF_GID appgroup \u0026amp;\u0026amp; \\ useradd appuser -r -u $GF_UID -g appgroup USER appuser  Store data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside of containers. Using an ELK stack is another good option for storing and processing logs.\nUsing pod IP addresses Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile. Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult. You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nCreating images in a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nSaving passwords in docker image 💀 Do not save passwords in a Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to provision passwords or inject them by mounting a persistent volume.\nUsing the \u0026lsquo;latest\u0026rsquo; tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case you don\u0026rsquo;t have complete control over your image - which is bad.\nDifferent images per environment Don\u0026rsquo;t create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nDepend on start order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.\nAdditional anti-patterns and patterns\u0026hellip; In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Refer to the following link for more information - Kubernetes Production Patterns\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_06/",
	"title": "Kubernetes is available in Docker for Mac 17.12 CE",
	"tags": [],
	"description": "",
	"content": "    Kubernetes is only available in Docker for Mac 17.12 CE and higher on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see general configuration.   \n Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes.\nThe Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n\u0026hellip;see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/landscape-setup/",
	"title": "Landscape Setup",
	"tags": [],
	"description": "",
	"content": " \u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR Kubectl Aliases Step 1: Clone the Repositories and get Dependencies Submodule Management Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster Kubify Shoot Cluster Using an Arbitrary Base Cluster  Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component  Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot  Step 9: Install Identity and Dashboard  Create CNAME Entry  Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard   Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh  Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces  Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape  After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh  This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;your cluster domain\u0026gt; --oidc-client-id=kube-kubectl --oidc-username-claim=email --oidc-groups-claim=groups  For a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh  After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before   The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt;  Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt;  Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys 'gardener' through 'dashboard' deploy all gardener: # deploys all components starting from 'gardener' deploy all :gardener # deploys all components up to 'gardener' # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to 'helm-tiller' undeploy all dashboard:cert # undeploys 'dashboard' through 'cert'  Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify  The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd  This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert  Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller  Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener  You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting...  while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found.  As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m  Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config  Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml  Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...]  Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname  The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager  After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found.  If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify  Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh  This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"
},
{
	"uri": "https://gardener.cloud/components/mcm/",
	"title": "Machine Controller Manager",
	"tags": [],
	"description": "",
	"content": " machine-controller-manager \nMachine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs. The current implementation supports AWS, GCP, Azure, Alicloud, Packet and Openstack. It can easily be extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1  Key terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. Node: Native kubernetes node objects. The objects you get to see when you do a \u0026ldquo;kubectl get nodes\u0026rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager See the design documentation in the /docs/design repository, please find the design doc here.\nTo start using or developing the Machine Controller Manager See the documentation in the /docs repository, please find the index here.\nCluster-api Implementation  cluster-api branch of machine-controller-manager implements the machine-api aspect of the cluster-api project. Link: https://github.com/gardener/machine-controller-manager/tree/cluster-api Once cluster-api project gets stable, we may make master branch of MCM as well cluster-api compliant, with well-defined migration notes.  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/maintain-shoot/",
	"title": "Maintain a Shoot cluster",
	"tags": [],
	"description": "Maintain a Shoot cluster",
	"content": " Maintain Shoot Cluster Day two operations like updating Kubernetes patch version (if the auto-update is enabled) and updating Operating system version happen in the maintenance time window of the Shoot cluster. The maintenance time window is part of the shoot spec (.spec.maintenance.timeWindow). If it is not specified during Shoot creation, Gardener will default to a randomized time window (to spread the load). The time interval cannot be less than 30 minutes and more than 6 hours.\nTo trigger the maintenance operation, you can annotate the Shoot with shoot.garden.sapcloud.io/operation: maintain.\nKubernetes Patch Version If a Shoot has .spec.maintenance.autoUpdate.kubernetesVersion: true in the manifest, and you update the .spec.kubernetes.versions field in the CloudProfile used in the Shoot, then the maintenance controller will apply Kubernetes patch releases updates automatically during the maintenance time window.\nSince Kubernetes follows Semantic Versioning, if indicated so, Gardener will automatically apply the patch release updates. But it will never auto update the Major or Minor releases since there is no effort to keep backward compatibility in those releases.\nMajor or Minor updates must be handled by updating the .spec.kubernetes.version field manually, these updates will be executed immediately and will not wait for maintenance time window. Before applying such update on Minor or Major releases, operators should check for all the breaking changes introduced in the target release Changelog.\nE.g. If you have a Shoot cluster with the field values below (only related fields are shown):\nspec: kubernetes: version: 1.10.0 maintenance: timeWindow: begin: 220000+0000 end: 230000+0000 autoUpdate: kubernetesVersion: true  If you update the CloudProfile used in the Shoot and add 1.10.5 and 1.11.0 to the .spec.kubernetes.versions list, the Shoot will be updated to 1.10.5 between 22:00-23:00 UTC. Your Shoot won\u0026rsquo;t be updated to 1.11.0 even though its the highest Kubernetes version in the CloudProfile. This is because that wouldn\u0026rsquo;t be a patch release update but a minor release update, and potentially have breaking changes that could impact your deployed resources.\nIn this example if the operator wants to update the Kubernetes version to 1.11.0, he/she must update the Shoot\u0026rsquo;s .spec.kubernetes.version to 1.11.0 manually.\nKubernetes Version Expiration Date Gardener administrators can also specify expiration dates for the Kubernetes versions in the CloudProfile. Kubernetes version expiration dates allow smoother transitions for Shoot owners giving them time for testing before the actual Kubernetes version update happens. Expiration date for the latest Kubernetes version in the CloudProfile is not allowed.\nWe can check the following scenarios for better understanding on Kubernetes version expiration dates:\n Automatic patch update from expired Kubernetes version.  Let\u0026rsquo;s assume the following CloudProfile spec (only related fields are shown):\nspec: kubernetes: versions: - version: 1.12.8 - version: 1.11.10 - version: 1.10.13 - version: 1.10.12 expirationDate: \u0026quot;2019-04-13T08:00:00Z\u0026quot;  And let\u0026rsquo;s the Shoot has the following spec:\nspec: kubernetes: version: 1.10.12 maintenance: timeWindow: begin: 220000+0100 end: 230000+0100 autoUpdate: kubernetesVersion: false  The Shoot refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-12 the Kubernetes version will stay the same as it is still not expired. But in the maintenance window on 2019-04-14 the Kubernetes version of the Shoot will be updated to 1.10.13 (no matter the value of .spec.maintenance.autoUpdate.kubernetesVersion).\n Automatic patch update from dropped Kubernetes version:  Let\u0026rsquo;s assume the following CloudProfile spec (only related fields are shown):\nspec: kubernetes: versions: - version: 1.12.8 - version: 1.11.10 - version: 1.10.13  And let\u0026rsquo;s the Shoot has the following spec:\nspec: kubernetes: version: 1.10.12 maintenance: timeWindow: begin: 220000+0100 end: 230000+0100 autoUpdate: kubernetesVersion: true  The Shoot refers a Kubernetes version that was dropped from the CloudProfile. In the upcoming maintenance window the Kubernetes version of the Shoot will be updated to the next patch version - 1.10.13. .spec.maintenance.autoUpdate.kubernetesVersion needs to be true, otherwise no version update will happen.\nOperating System Version If a Shoot has .spec.maintenance.autoUpdate.machineImageVersion: true in the manifest, and you update the .spec.machineImages field in the CloudProfile used in the Shoot, then the maintenance controller will apply the new machine image to the Shoot spec (and will mark the Shoot to be reconciled) during the maintenance time window. During the reconciliation the corresponding \u0026lt;Provider\u0026gt;MachineClass resource in the Shoot namespace in the Seed will be updated and the machine controller manager will take care of the actual state to match the desired one.\nMachine Image Expiration Date  Automatic update from expired machine image version.  Let\u0026rsquo;s assume the following CloudProfile spec (only related fields are shown):\nspec: machineImages: - name: coreos versions: - version: 2191.5.0 - version: 2191.4.1 - version: 2135.6.0 expirationDate: \u0026quot;2019-04-13T08:00:00Z\u0026quot;  And let\u0026rsquo;s the Shoot has the following spec:\nspec: cloud: aws: workers: - name: name autoScalerMax: 1 autoScalerMin: 1 machineImage: name: coreos version: 2135.6.0 machineType: m5.large maxSurge: 1 maxUnavailable: 0 volumeSize: 20Gi volumeType: gp2 maintenance: timeWindow: begin: 220000+0100 end: 230000+0100 autoUpdate: machineImageVersion: false  The Shoot refers a machine image version that has an expirationDate. In the maintenance window on 2019-04-12 the machine image version will stay the same as it is still not expired. But in the maintenance window on 2019-04-14 the machine image version of the Shoot will be updated to 2191.5.0 (no matter the value of .spec.maintenance.autoUpdate.machineImageVersion) as version 2135.6.0 will be expired.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/add-node-to-cluster/",
	"title": "Manually adding a node to an existing cluster",
	"tags": [],
	"description": "This document describes steps on how to add a node to an existing cluster without the support of Gardener",
	"content": " Manually adding a node to an existing cluster Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\nDisclaimer  Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener. Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be responsible to replace it.\n How  Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.  To ssh into a machine which is already in the cluster, use the steps defined here.\nAttach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node\u0026rsquo;s name.\n On the new machine, create file /var/lib/kubelet/kubeconfig-bootstrap with the following content:  apiVersion: v1 kind: Config current-context: kubelet-bootstrap@default clusters: - cluster: certificate-authority-data: \u0026lt;CA Certificate\u0026gt; server: \u0026lt;Server\u0026gt; name: default contexts: - context: cluster: default user: kubelet-bootstrap name: kubelet-bootstrap@default users: - name: kubelet-bootstrap user: as-user-extra: {} token: \u0026lt;Token\u0026gt;   ssh into an existing node, and run these commands to get the values of and  to be replaced in above file: \u0026lt;Servr\u0026gt; bash /opt/bin/hyperkube kubectl \\ --kubeconfig /var/lib/kubelet/kubeconfig-real \\ config view \\ -o go-template='{{index .clusters 0 \u0026quot;cluster\u0026quot; \u0026quot;server\u0026quot;}}' \\ --raw  \u0026lt;CA Certificate\u0026gt;\n/opt/bin/hyperkube kubectl \\ --kubeconfig /var/lib/kubelet/kubeconfig-real \\ config view \\ -o go-template='{{index .clusters 0 \u0026quot;cluster\u0026quot; \u0026quot;certificate-authority-data\u0026quot;}}' \\ --raw  \u0026lt;Token\u0026gt;\nThe kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the kube-system namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its .data.expiration field. The name of this secret is of the format bootstrap-token-*. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets. To get an unexpired token, find the secrets with the name format bootstrap-token-* in the kube-system namespace in the cluster, and pick the one with minimum age. Eg. bootstrap-token-abcdef.\nRun these commands to get the token:\ntokenid=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template='{{index .data \u0026quot;token-id\u0026quot;}}' | base64 --decode) tokensecret=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template='{{index .data \u0026quot;token-secret\u0026quot;}}' | base64 --decode) echo $tokenid.$tokensecret  The value of $TOKEN will be tokenid.tokensecret. Replace $TOKEN in above file with this value\n Copy contents of the files - /var/lib/kubelet/config/kubelet, /var/lib/kubelet/ca.crt and /etc/systemd/system/kubelet.service - from an existing node to the new node\n Run the following command in the new node to start the kubelet:\nsystemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet   The new node should be added to the existing cluster within a couple of minutes.\n"
},
{
	"uri": "https://gardener.cloud/blog/2019_week_06/",
	"title": "Manually adding a node to an existing cluster",
	"tags": [],
	"description": "",
	"content": "Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\n This tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\n..read some more on Adding Nodes to a Cluster.\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/25_markup/",
	"title": "Markdown",
	"tags": [],
	"description": "",
	"content": "Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesn’t support well. You could use pure HTML to expand possibilities.\nBut this happens to be a bad idea. Everyone uses Markdown because it\u0026rsquo;s pure and simple to read even non-rendered. You should avoid HTML to keep it as simple as possible.\nTo avoid this limitations, Hugo created shortcodes. A shortcode is a simple snippet inside a page.\nGardener provides multiple shortcodes on top of existing ones.\n Attachments  The Attachments shortcode displays a list of files attached to a page.\n Button  Nice buttons on your page.\n Expand  Displays an expandable/collapsible section of text on your page\n Mermaid  Generation of diagram and flowchart from text in a similar manner as markdown\n Notice  Disclaimers to help you structure your page\n "
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/25_markup/mermaid/",
	"title": "Mermaid",
	"tags": [],
	"description": "Generation of diagram and flowchart from text in a similar manner as markdown",
	"content": " Mermaid is a library helping you to generate diagram and flowcharts from text, in a similar manner as Markdown.\nJust insert your mermaid code in the mermaid shortcode and that\u0026rsquo;s it.\nFlowchart example {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] {{\u0026lt; /mermaid \u0026gt;}}  renders as\ngraph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two]  Sequence example {{\u0026lt;mermaid\u0026gt;}} sequenceDiagram participant Alice participant Bob Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! {{\u0026lt; /mermaid \u0026gt;}}  renders as\nsequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good!  GANTT Example {{\u0026lt;mermaid\u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d {{\u0026lt; /mermaid \u0026gt;}}  render as\ngantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d  "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/network-isolation/",
	"title": "Namespace Isolation",
	"tags": [],
	"description": "",
	"content": " \u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.\nThere are many reasons why you may chose to employ Kubernetes network policies: - Isolate multi-tenant deployments - Regulatory compliance - Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other\nKubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDRs or IP addresses used for matching source or destination IP’s. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and select subsets of objects.\nExample We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose kubectl create ns customer1 kubectl create ns customer2 # create a standard HTTP web server kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1 kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2 # expose the port 80 for external access kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1 kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2  Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \u0026quot;bash\u0026quot; pod in one namespace kubectl run -i --tty client --image=tutum/curl -n=customer1  try to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \u0026quot;customer1\u0026quot; =\u0026gt; success curl http://nginx.customer1 # get the index.html from the nginx of the namespace \u0026quot;customer2\u0026quot; =\u0026gt; success curl http://nginx.customer2  Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\nTest with NP Install the NetworkPolicy from your shell\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-from-other-namespaces spec: podSelector: matchLabels: ingress: - from: - podSelector: {}   it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1 kubectl apply -f ./network-policy.yaml -n=customer2  after this curl http://nginx.customer2 shouldn\u0026rsquo;t work anymore if you are a service inside the namespace customer1 and vice versa\nNote: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type LoadBalancer in namespace customer1 that match the nginx pod. When you request the service by its \u0026lt;EXTERNAL_IP\u0026gt;:\u0026lt;PORT\u0026gt;, then the network policy will deny the ingress traffic from the service and the request will time out.\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_09/",
	"title": "Namespace Isolation",
	"tags": [],
	"description": "",
	"content": "\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may chose to configure Kubernetes network policies: - Isolate multi-tenant deployments - Regulatory compliance - Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another\n..read on Namespace Isolation how to configure it.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_08_2/",
	"title": "Namespace Scope",
	"tags": [],
	"description": "",
	"content": "Should I use: ❌ one namespace per user/developer? ❌ one namespace per team? ❌ one per service type? ❌ one namespace per application type? 😄 one namespace per running instance of your application? \nApply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don\u0026rsquo;t provide: - Network isolation - Access Control - Audit Logging on user level\n"
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/25_markup/notice/",
	"title": "Notice",
	"tags": [],
	"description": "Disclaimers to help you structure your page",
	"content": " The notice shortcode shows 4 types of disclaimers to help you structure your page.\nNote {{% notice note %}} A notice disclaimer {{% /notice %}}  renders as\nA notice disclaimer\n Info {{% notice info %}} An information disclaimer {{% /notice %}}  renders as\nAn information disclaimer\n Tip {{% notice tip %}} A tip disclaimer {{% /notice %}}  renders as\nA tip disclaimer\n Warning {{% notice warning %}} An warning disclaimer {{% /notice %}}  renders as\nA warning disclaimer\n "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/container-startup/",
	"title": "Orchestration of container startup",
	"tags": [],
	"description": "How to orchestrate startup sequence of multiple containers",
	"content": " Disclaimer If an application depends on other services deployed separately do not rely on a certain start sequence of containers but ensure that the application can cope with unavailability of the services it depends on.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod\u0026rsquo;s initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n time=\u0026quot;2018-06-12T11:02:42Z\u0026quot; level=info msg=\u0026quot;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\u0026quot; time=\u0026quot;2018-06-12T11:02:42Z\u0026quot; level=fatal msg=\u0026quot;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\u0026quot; $ kubectl get po -w NAME READY STATUS RESTARTS AGE webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 1s webapp-958cf5567-h247n 0/1 Error 0 2s webapp-958cf5567-h247n 0/1 Error 1 3s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s webapp-958cf5567-h247n 0/1 Error 2 18s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s webapp-958cf5567-h247n 0/1 Error 3 43s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s  If the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers can be defined which are executed prior to the application container. If one InitContainers fails, the application container won\u0026rsquo;t be triggered.\napiVersion: apps/v1 kind: Deployment metadata: name: webapp spec: selector: matchLabels: app: webapp template: metadata: labels: app: webapp spec: initContainers: # check if DB is ready, and only continue when true - name: check-db-ready image: postgres:9.6.5 command: ['sh', '-c', 'until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;'] containers: - image: xcoulon/go-url-shortener:0.1.0 name: go-url-shortener env: - name: POSTGRES_HOST value: postgres - name: POSTGRES_PORT value: \u0026quot;5432\u0026quot; - name: POSTGRES_DATABASE value: url_shortener_db - name: POSTGRES_USER value: user - name: POSTGRES_PASSWORD value: mysecretpassword ports: - containerPort: 8080  In above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w NAME READY STATUS RESTARTS AGE nginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d privileged-pod 1/1 Running 0 4d webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s $ kubectl logs webapp-fdcb49cbc-4gs4n Error from server (BadRequest): container \u0026quot;go-url-shortener\u0026quot; in pod \u0026quot;webapp-fdcb49cbc-4gs4n\u0026quot; is waiting to start: PodInitializing  "
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/10_organisation/",
	"title": "Organisation",
	"tags": [],
	"description": "",
	"content": " Content Organisation This site uses Hugo. In Hugo, content organization is a core concept.\nHugo Tip: Start Hugo with hugo server --navigateToChanged for content edit-sessions.\n Page Lists Page Order The documentation side menu, the documentation page browser etc. are listed using Hugo\u0026rsquo;s default sort order, which sorts by weight (from 1), date (newest first) and finally by the link title.\nGiven that, if you want to move a page or a section up, set a weight in the page\u0026rsquo;s front matter:\ntitle: My Page weight: 10  For page weights, it can be smart not to use 1, 2, 3 \u0026hellip;, but some other interval, say 10, 20, 30\u0026hellip; This allows you to insert pages where you want later.\n "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/working-with-kubeconfig/",
	"title": "Organizing Access Using kubeconfig Files",
	"tags": [],
	"description": " Organizing Access Using kubeconfig Files",
	"content": " Organizing Access Using kubeconfig Files The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\nProblem If you\u0026rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials in case anything was leaked. However, this is not possible with the initial or master kubeconfig from your cluster.\nPitfall Never distribute the kubeconfig, which you can download directly within the Gardener dashboard, for a productive cluster.\nCreate custom kubeconfig file for each user Create a separate kubeconfig for each user. One of the big advantages is, that you can revoke them and control the permissions better. A limitation to single namespaces is also possible here.\nThe script creates a new ServiceAccount with read privileges in the whole cluster (Secretes are excluded). To run the script jq, a lightweight and flexible command-line JSON processor, must be installed.\n#!/bin/bash if [[ -z \u0026quot;$1\u0026quot; ]] ;then echo \u0026quot;usage: $0 \u0026lt;username\u0026gt;\u0026quot; exit 1 fi user=$1 kubectl create sa ${user} secret=$(kubectl get sa ${user} -o json | jq -r .secrets[].name) kubectl get secret ${secret} -o json | jq -r '.data[\u0026quot;ca.crt\u0026quot;]' | base64 -D \u0026gt; ca.crt user_token=$(kubectl get secret ${secret} -o json | jq -r '.data[\u0026quot;token\u0026quot;]' | base64 -D) c=`kubectl config current-context` cluster_name=`kubectl config get-contexts $c | awk '{print $3}' | tail -n 1` endpoint=`kubectl config view -o jsonpath=\u0026quot;{.clusters[?(@.name == \\\u0026quot;${cluster_name}\\\u0026quot;)].cluster.server}\u0026quot;` # Set up the config KUBECONFIG=k8s-${user}-conf kubectl config set-cluster ${cluster_name} \\ --embed-certs=true \\ --server=${endpoint} \\ --certificate-authority=./ca.crt KUBECONFIG=k8s-${user}-conf kubectl config set-credentials ${user}-${cluster_name#cluster-} --token=${user_token} KUBECONFIG=k8s-${user}-conf kubectl config set-context ${user}-${cluster_name#cluster-} \\ --cluster=${cluster_name} \\ --user=${user}-${cluster_name#cluster-} KUBECONFIG=k8s-${user}-conf kubectl config use-context ${user}-${cluster_name#cluster-} cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: view-${user}-global subjects: - kind: ServiceAccount name: ${user} namespace: default roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io EOF echo \u0026quot;done! Test with: \u0026quot; echo \u0026quot;export KUBECONFIG=k8s-${user}-conf\u0026quot; echo \u0026quot;kubectl get pods\u0026quot;  If edit or admin rights are to be assigned, the ClusterRoleBinding must be adapted in the roleRef section with the roles listed below.\nFurthermore, you can restrict this to a single namespace by not creating a ClusterRoleBinding but only a RoleBinding within the desired namespace.\n   Default ClusterRole Default ClusterRoleBinding Description     cluster-admin system:masters group Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding\u0026rsquo;s namespace, including the namespace itself.   admin None Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.   edit None Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.   view None Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.    "
},
{
	"uri": "https://gardener.cloud/blog/2019_week_02/",
	"title": "Organizing Access Using kubeconfig Files",
	"tags": [],
	"description": "",
	"content": "The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n  What happens if your kubeconfig file of your production cluster is leaked or published by accident?\n Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if it is has leaked - delete the cluster.\n..learn more on Work with kubeconfig files.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/service-cache-control/",
	"title": "Out-Dated HTML and JS files delivered",
	"tags": [],
	"description": "Why is my application always outdated?",
	"content": " Problem After updating your HTML and JavaScript sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: vuejs-ingress spec: rules: - host: test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: vuejs-svc servicePort: 8080  where: - \u0026lt;GARDENER-CLUSTER\u0026gt;: The cluster name in the Gardener - \u0026lt;GARDENER-PROJECT\u0026gt;: You project name in the Gardener\nWhat is the underlying problem? The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below:\n use a cache buster + HTTP-Cache-Control(prefered) use HTTP-Cache-Control with a lower retention period disable the caching in the ingress (just for dev purpose)  Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise for your web framework (e.g. Express/NodeJS, SpringBoot,\u0026hellip;)\nHere an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (during development).\n--- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/cache-enable: \u0026quot;false\u0026quot; name: vuejs-ingress spec: rules: - host: test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.com http: paths: - backend: serviceName: vuejs-svc servicePort: 8080  "
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/37_process/",
	"title": "Process",
	"tags": [],
	"description": "",
	"content": " Creating a new Feature If you want to contribute to the Gardener, please do that always on a dedicated branch on your own fork named after the purpose of the code changes, for example feature/helm-integration. Please do not forget to rebase your branch regularly.\n If you have finished your work, please create a pull request based on master. It will be reviewed and merged if no further changes are requested from you.  :warning: Please ensure that your modifications pass the lint checks, formatting checks, static code checks, and unit tests by executing\nmake verify  :rotating_light: Please run make generate whenever you modify the any API within pkg/apis.\nPlease do not file your pull request unless you receive a successful response from here!\nCreating a new Release Please refer to the Gardener contributor guide.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_27/",
	"title": "ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS",
	"tags": [],
	"description": "",
	"content": " The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  When you have application running on multiple nodes which require shared access to a file system When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS supports encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about vendor lock-in and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (roughly twice the price of EBS storage) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don’t use EFS, use it for the files which can\u0026rsquo;t be stored in a CDN. Don’t use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.   ..read some more on ReadWriteMany.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/app/read-write-many/",
	"title": "ReadWriteMany with AWS",
	"tags": [],
	"description": "Dynamically Provisioned PV’s Using Amazon EFS",
	"content": ""
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/40_repositories/",
	"title": "Repositories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/dns_names/",
	"title": "Request DNS Names",
	"tags": [],
	"description": "Requesting DNS Names for Ingresses and Services in Shoot Clusters",
	"content": " Request DNS Names in Shoot Clusters Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. Therefore the gardener must be installed with the shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\nShoot Feature Gate The shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster). Therefore it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must add the shoot-dns-service extension.\n... spec: extensions: - type: shoot-dns-service ...  Configuration In Shoot Cluster To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class garden and an annotation denoting the desired DNS names.\nFor a Service (it must have the type LoadBalancer) this looks like this:\napiVersion: v1 kind: Service metadata: annotations: dns.gardener.cloud/class: garden dns.gardener.cloud/dnsnames: my.subdomain.for.shootsomain.cloud name: my-service namespace: default spec: ports: - port: 80 protocol: TCP targetPort: 80 type: LoadBalancer  The dnsnames annotation accepts a comma-separated list of DNS names, if multiple names are required.\nFor an Ingress, the dns names are already declared in the specification. Nevertheless the dnsnames annotation must be present. Here a subset of the dns names of the ingress can be specified. If DNS names for all names are desired, the value all can be used.\nIf one of the accepted dns names is a direct subname of the shoot\u0026rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the dnsnames list in the annotation. If only this dns name is configured in the ingress, no explicit dns entry is required, and the dns annotations should be omitted at all.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/x509_certificates/",
	"title": "Request X.509 Certificates",
	"tags": [],
	"description": "X.509 Certificates For TLS Communication",
	"content": " Request X.509 Certificates Introduction Dealing with applications on Kubernetes which offer service endpoints (e.g. HTTP) may also require you to enable a secured communication via SSL/TLS. Gardener let\u0026rsquo;s you request a commonly trusted X.509 certificate for your application endpoint. Furthermore, Gardener takes care about the renewal process for your requested certificate.\nRestrictions Domains Certificates may be received for any subdomain of your shoot\u0026rsquo;s domain (see .spec.dns.domain of your shoot resource).\nCharacter Restrictions Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).\nFor example, the following request is invalid:\napiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata: name: cert-invalid namespace: default spec: commonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud  But it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:\napiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata: name: cert-example namespace: default spec: commonName: short.ingress.shoot.project.default-domain.gardener.cloud dnsNames: - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud  Certificate Resources Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener\u0026rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can either be requested by creating Certificate resources in the Kubernetes cluster or by configuring Ingress or Service (type LoadBalancer) resources. If the latter is used, a Certificate resource will automatically be created by Gardener\u0026rsquo;s certificate service.\nIf you\u0026rsquo;re interested in the current progress of your request, you\u0026rsquo;re advised to consult the Certificate\u0026rsquo;s status subresource. You\u0026rsquo;ll also find error descriptions in the status in case the issuance failed.\nCertificate status example:\napiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate ... status: commonName: short.ingress.shoot.project.default-domain.gardener.cloud expirationDate: \u0026quot;2020-02-27T15:39:10Z\u0026quot; issuerRef: name: garden namespace: shoot--foo--bar lastPendingTimestamp: \u0026quot;2019-11-29T16:38:40Z\u0026quot; observedGeneration: 11 state: Ready  Examples Request a certificate via Certificate apiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata: name: cert-example namespace: default spec: commonName: short.ingress.shoot.project.default-domain.gardener.cloud dnsNames: - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud secretRef: name: cert-example namespace: default   spec.commonName (required) specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.\nspec.dnsName additional domains the certificate should be valid for. Entries in this list can be longer than 64 characters.\nspec.secretRef specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.\n Request a wildcard certificate via Certificate apiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata: name: cert-wildcard namespace: default spec: commonName: '*.ingress.shoot.project.default-domain.gardener.cloud' secretRef: name: cert-wildcard namespace: default   spec.commonName (required) specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.\nPlease note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\nspec.secretRef specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.\n Request a certificate via Ingress apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: vuejs-ingress annotations: cert.gardener.cloud/purpose: managed spec: tls: # Must not exceed 64 characters. - hosts: - short.ingress.shoot.project.default-domain.gardener.cloud - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud # Certificate and private key reside in this secret. secretName: testsecret-tls rules: - host: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud http: paths: - backend: serviceName: vuejs-svc servicePort: 8080   metadata.annotations must contain cert.gardener.cloud/purpose: managed to activate the certificate service on this resource.\nspec.tls[].hosts specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.\nspec.tls[].secretName specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.\n Request a wildcard certificate via Ingress apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: vuejs-ingress annotations: cert.gardener.cloud/purpose: managed spec: tls: # Must not exceed 64 characters. - hosts: - \u0026quot;*.ingress.shoot.project.default-domain.gardener.cloud\u0026quot; # Certificate and private key reside in this secret. secretName: testsecret-tls rules: - host: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud http: paths: - backend: serviceName: vuejs-svc servicePort: 8080   metadata.annotations must contain cert.gardener.cloud/purpose: managed to activate the certificate service on this resource.\nspec.tls[].hosts please make sure the wildcard domain complies with the 64 character limit.\nPlease note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n Request a certificate via Service apiVersion: v1 kind: Service metadata: annotations: cert.gardener.cloud/secretname: test-service-secret dns.gardener.cloud/dnsnames: \u0026quot;service.shoot.project.default-domain.gardener.cloud, morethan64characters.svc.shoot.project.default-domain.gardener.cloud\u0026quot; dns.gardener.cloud/ttl: \u0026quot;600\u0026quot; name: test-service namespace: default spec: ports: - name: http port: 80 protocol: TCP targetPort: 8080 type: LoadBalancer   metadata.annotations[cert.gardener.cloud/secretname] specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.\nmetadata.annotations[dns.gardener.cloud/dnsnames] specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.\n Request a wildcard certificate via Service apiVersion: v1 kind: Service metadata: annotations: cert.gardener.cloud/secretname: test-service-secret dns.gardener.cloud/dnsnames: \u0026quot;*.shoot.project.default-domain.gardener.cloud\u0026quot; dns.gardener.cloud/ttl: \u0026quot;600\u0026quot; name: test-service namespace: default spec: ports: - name: http port: 80 protocol: TCP targetPort: 8080 type: LoadBalancer   metadata.annotations[cert.gardener.cloud/secretname] specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.\nmetadata.annotations[dns.gardener.cloud/dnsnames] please make sure the wildcard domain complies with the 64 character limit.\nPlease note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n  #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  "
},
{
	"uri": "https://gardener.cloud/045_contribute/10_code/12-security_guide/",
	"title": "Security Release Process",
	"tags": [],
	"description": "",
	"content": " Gardener Security Release Process Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.\nGardener Security Team Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits. The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:\n Olaf Beier, (@olafbeier) Vasu Chandrasekhara, (@vasu1124) Alban Crequy, (@alban) Norbert Hamann, (@norberthamann) Claudia H\u0026ouml;lters, (@hoeltcl) Oliver Kling, (@oliverkling) Vedran Lerenc, (@vlerenc) Dirk Marwinski, (@marwinski) Michael Schubert, (@schu) Matthias Sohn, (@msohn) Frederik Thormaehlen, (@ThormaehlenFred) Christian Cwienk (@ccwienk)  Disclosures Private Disclosure Processes The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you\u0026rsquo;ve found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to secure@sap.com. We\u0026rsquo;ll send a confirmation e-mail to acknowledge your report, and we\u0026rsquo;ll send an additional e-mail when we\u0026rsquo;ve identified the issue positively or negatively.\nPublic Disclosure Processes If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to secure@sap.com to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.\nIf possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a private disclosure process (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn\u0026rsquo;t necessary and is unlikely to make a public disclosure less damaging.\nPatch, Release, and Public Communication For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the \u0026ldquo;Fix Team\u0026rdquo; and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the \u0026ldquo;Fix Lead.\u0026rdquo; The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the \u0026ldquo;Fix team.\u0026rdquo; (I.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score \u0026gt;= 7; see below). If the fix relies on another upstream project\u0026rsquo;s disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.\nFix Team Organization The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team. The Fix Lead will give the Fix Team access to a private security repository to develop the fix.\nFix Development Process The Fix Lead and the Fix Team will create a CVSS using the CVSS Calculator. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect. The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers. If the CVSS score is under 7.0 (a medium severity score) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private Gardener Security mailing list.\nFix Disclosure Process With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the Gardener mailing list that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.\nFix Release Day The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date. The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue. The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will LGTM and merge. The Release Managers will merge these PRs as quickly as possible. Changes shouldn\u0026rsquo;t be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches. The Fix Lead will request a CVE from the SAP Product Security Response Team via email to cna@sap.com with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the Gardener mailing list and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.\nAs much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia. The Fix Lead will remove the Fix Team from the private security repository.\nRetrospective These steps should be completed after the Release Date. The retrospective process should be blameless.\nThe Fix Lead will send a retrospective of the process to the Gardener mailing list including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process. The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the Gardener mailing list. Honest critique is the only way we are going to get good at this as a community.\nCommunication Channel The private or public disclosure process should be triggered exclusively by writing an e-mail to secure@sap.com.\nGardener security announcements will be communicated by the Fix Lead sending an e-mail to the Gardener mailing list (reachable via gardener@googlegroups.com) as well as posting a link in the Gardener Slack channel. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (how to find and join a group)\nThe members of the Gardener Security Team are subscribed to the private Gardener Security mailing list (reachable via gardener-security@googlegroups.com).\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/setup-seed/",
	"title": "Setting up a Seed Cluster",
	"tags": [],
	"description": "How to configure a Kubernetes cluster as a Gardener seed",
	"content": " The Seed Cluster The landscape-setup-template is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don\u0026rsquo;t have network policies, for example. See Hardening the Gardener Community Setup for more information.\nTo have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.\nSetting up the Shoot The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won\u0026rsquo;t work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but kubectl apply and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.\nSo, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest here. You could, for example, change the CIDRs to this:\n... networks: internal: - 10.254.112.0/22 nodes: 10.254.0.0/19 pods: 10.255.0.0/17 public: - 10.254.96.0/22 services: 10.255.128.0/17 vpc: cidr: 10.254.0.0/16 workers: - 10.254.0.0/19 ...  Also make sure that your new seed cluster has enough resources for the expected number of shoots.\nRegistering the Shoot as Seed The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the seed-config component of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the state/seed-config/ directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.\n1. Seed Namespace First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called seed-test.\n2. Cloud Provider Secret The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).\napiVersion: v1 kind: Secret metadata: name: test-seed-secret namespace: seed-test labels: cloudprofile.garden.sapcloud.io/name: aws type: Opaque data: accessKeyID: \u0026lt;base64-encoded AWS access key\u0026gt; secretAccessKey: \u0026lt;base64-encoded AWS secret key\u0026gt; kubeconfig: \u0026lt;base64-encoded kubeconfig\u0026gt;  Deploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.\n3. Secretbinding for Cloud Provider Secret Create a secretbinding for your cloud provider secret:\napiVersion: core.gardener.cloud/v1alpha1 kind: SecretBinding metadata: name: test-seed-secret namespace: seed-test labels: cloudprofile.garden.sapcloud.io/name: aws secretRef: name: test-seed-secret # namespace: only required if in different namespace than referenced secret quotas: []  You can give it the same name as the referenced secret.\n4. Cloudprofile The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don\u0026rsquo;t want to change anything.\n5. Seed Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.\napiVersion: core.gardener.cloud/v1alpha1 kind: Seed metadata: name: aws-secure spec: provider: type: aws region: eu-west-1 secretRef: name: test-seed-secret namespace: seed-test dns: ingressDomain: ingress.\u0026lt;your cluster domain\u0026gt; networks: nodes: 10.254.0.0/19 pods: 10.255.0.0/17 services: 10.255.128.0/17  6. Hide Original Seed In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.\nTo solve this problem, edit the original seed and set its spec.visible field to false. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.\n"
},
{
	"uri": "https://gardener.cloud/api-reference/settings/",
	"title": "Settings",
	"tags": [],
	"description": "",
	"content": "Packages:\n  settings.gardener.cloud/v1alpha1   settings.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  ClusterOpenIDConnectPreset  OpenIDConnectPreset  ClusterOpenIDConnectPreset   ClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot objects cluster-wide.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  ClusterOpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterOpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project mathching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n       OpenIDConnectPreset   OpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot in a namespace.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  OpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  OpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n       ClusterOpenIDConnectPresetSpec   (Appears on: ClusterOpenIDConnectPreset)  ClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and project selector matching Shoots in Projects.\n   Field Description      OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project mathching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    KubeAPIServerOpenIDConnect   (Appears on: OpenIDConnectPresetSpec)  KubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientID  string    The client ID for the OpenID Connect client. Required.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT). Required.\n    requiredClaims  map[string]string    (Optional) key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value. Only applied when the Kubernetes version of the Shoot is \u0026gt;= 1.11\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1 Defaults to [RS256]\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details. Defaults to \u0026ldquo;sub\u0026rdquo;.\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OpenIDConnectPresetSpec)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      secret  string    (Optional) The client Secret for the OpenID Connect client.\n    extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    OpenIDConnectPresetSpec   (Appears on: OpenIDConnectPreset, ClusterOpenIDConnectPresetSpec)  OpenIDConnectPresetSpec contains the Shoot selector for which a specific OpenID Connect configuration is applied.\n   Field Description      server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n      Generated with gen-crd-api-reference-docs on git commit 79c676930. \n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/app/s3/",
	"title": "Shared storage with S3 backend",
	"tags": [],
	"description": "Shared storage with S3 backend",
	"content": " Shared storage with S3 backend The storage is definitely the most complex and important part of an application setup, once this part is completed, 80% of the tasks are completed.\nMounting an S3 bucket into a pod using FUSE allows you to access the data as if it were on the local disk. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\nOverview Limitations Generally S3 cannot offer the same performance or semantics as a local file system. More specifically:\n random writes or appends to files require rewriting the entire file metadata operations such as listing directories have poor performance due to network latency eventual consistency can temporarily yield stale data(Amazon S3 Data Consistency Model) no atomic renames of files or directories no coordination between multiple clients mounting the same bucket no hard links  Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using the Gardener.\nEnsure that you have create the \u0026ldquo;imagePullSecret\u0026rdquo; in your cluster.\nkubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt;  Setup The first step is to clone this repository. Next is the Secret for the AWS API credentials of the user that has full access to our S3 bucket. Copy the configmap_secrets_template.yaml to configmap_secrets.yaml and place your secrets at the right place\napiVersion: v1 kind: ConfigMap metadata: name: s3-config data: S3_BUCKET: \u0026lt;YOUR-S3-BUCKET-NAME\u0026gt; AWS_KEY: \u0026lt;YOUR-AWS-TECH-USER-ACCESS-KEY\u0026gt; AWS_SECRET_KEY: \u0026lt;YOUR-AWS-TECH-USER-SECRET\u0026gt;  Build and deploy Change the settings in the build.sh file with your docker registry settings.\n#!/usr/bin/env bash ######################################################################################################################## # PREREQUISTITS ######################################################################################################################## # # - ensure that you have a valid Artifactory or other Docker registry account # - Create your image pull secret in your namespace # kubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; # - change the settings below arcording your settings # # usage: # Call this script with the version to build and push to the registry. After build/push the # yaml/* files are deployed into your cluster # # ./build.sh 1.0 # VERSION=$1 PROJECT=kube-s3 REPOSITORY=cp-enablement.docker.repositories.sap.ondemand.com # causes the shell to exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x . . . .  Create the S3Fuse Pod and check the status:\n# build and push the image to your docker registry ./build.sh 1.0 # check that the pods are up and running kubectl get pods  Check success Create a demo Pod and check the status:\nkubectl apply -f ./yaml/example_pod.yaml # wait some second to get the pod up and running... kubectl get pods # go into the pd and check that the /var/s3 is mounted with your S3 bucket content inside kubectl exec -ti test-pd sh # inside the pod ls -la /var/s3  Why does this work? Docker engine 1.10 added a new feature which allows containers to share the host mount namespace. This feature makes it possible to mount a s3fs container file system to a host file system through a shared mount, providing a persistent network storage with S3 backend.\nThe key part is mountPath: /var/s3:shared which enables the volume to be mounted as shared inside the pod. When the container starts it will mount the S3 bucket onto /var/s3 and consequently the data will be available under /mnt/data-s3fs on the host and thus to any other container/pod running on it (and has /mnt/data-s3fs mounted too).\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_10/",
	"title": "Shared storage with S3 backend",
	"tags": [],
	"description": "",
	"content": "The storage is definitely the most complex and important part of an application setup, once this part is completed, one of the most problematic parts could be solved.\nMounting a S3 bucket into a pod using FUSE allows to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/commit_secret_fail/",
	"title": "Storing secrets in git 💀",
	"tags": [],
	"description": "Never ever commit a kubeconfig.yaml into github",
	"content": " Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository\u0026rsquo;s history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository\u0026rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository\u0026rsquo;s history  Warning: If you run git filter-branch after stashing changes, you won\u0026rsquo;t be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you\u0026rsquo;ve made. To unstash the last set of changes you\u0026rsquo;ve stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we\u0026rsquo;ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository\u0026rsquo;s working directory.\ncd YOUR-REPOSITORY  Run the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will: * Force Git to process, but not check out, the entire history of every branch and tag * Remove the specified file, as well as any empty commits generated as a result * Overwrite your existing tags\ngit filter-branch --force --index-filter \\ 'git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA' \\ --prune-empty --tag-name-filter cat -- --all  Add your file with sensitive data to .gitignore to ensure that you don\u0026rsquo;t accidentally commit it again.\necho \u0026quot;YOUR-FILE-WITH-SENSITIVE-DATA\u0026quot; \u0026gt;\u0026gt; .gitignore  Double-check that you\u0026rsquo;ve removed everything you wanted to from your repository\u0026rsquo;s history, and that all of your branches are checked out.\nOnce you\u0026rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you\u0026rsquo;ve pushed up:\ngit push origin --force --all  In order to remove the sensitive file from your tagged releases, you\u0026rsquo;ll also need to force-push against your Git tags:\ngit push origin --force --tags   Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n References:\n https://help.github.com/articles/removing-sensitive-data-from-a-repository/   blockquote { border:1px solid red; padding:10px; margin-top:40px; margin-bottom:40px; } blockquote p { font-size: 1.5rem; color: black; }  "
},
{
	"uri": "https://gardener.cloud/045_contribute/20_documentation/20_style/",
	"title": "Style Guide",
	"tags": [],
	"description": "",
	"content": " This page gives writing style guidelines for the Gardener documentation. These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\nLanguage Gardener documentation uses US English.\nDocumentation formatting standards Use camel case for API objects When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name. Typically, the names of API objects use camel case.\nDon\u0026rsquo;t split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying \u0026ldquo;object,\u0026rdquo; unless omitting \u0026ldquo;object\u0026rdquo; leads to an awkward construction.\n DoDon't The Pod has two containers.The pod has two containers. The Deployment is responsible for ...The Deployment object is responsible for ... A PodList is a list of Pods.A Pod List is a list of pods. The two ContainerPorts ...The two ContainerPort objects ... The two ContainerStateTerminated objects ...The two ContainerStateTerminateds ...  Use angle brackets for placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents.\n Display information about a pod:\nkubectl describe pod \nwhere \u0026lt;pod-name\u0026gt; is the name of one of your pods.\n  Use bold for user interface elements  DoDon't Click Fork.Click \"Fork\". Select Other.Select 'Other'.  Use italics to define or introduce new terms  DoDon't A cluster is a set of nodes ...A \"cluster\" is a set of nodes ... These components form the control plane.These components form the control plane.  Use code style for filenames, directories, and paths  DoDon't Open the envars.yaml file.Open the envars.yaml file. Go to the /docs/tutorials directory.Go to the /docs/tutorials directory. Open the /_data/concepts.yaml file.Open the /_data/concepts.yaml file.  Use the international standard for punctuation inside quotes  DoDon't events are recorded with an associated \"stage\".events are recorded with an associated \"stage.\" The copy is called a \"fork\".The copy is called a \"fork.\"  Inline code formatting Use code style for inline code and commands For inline code in an HTML document, use the \u0026lt;code\u0026gt; tag. In a Markdown document, use the backtick (`).\n DoDon't The kubectl run command creates a Deployment.The \"kubectl run\" command creates a Deployment. For declarative management, use kubectl apply.For declarative management, use \"kubectl apply\".  Use code style for object field names  DoDon't Set the value of the replicas field in the configuration file.Set the value of the \"replicas\" field in the configuration file. The value of the exec field is an ExecAction object.The value of the \"exec\" field is an ExecAction object.  Use normal style for string and integer field values For field values of type string or integer, use normal style without quotation marks.\n DoDon't Set the value of imagePullPolicy to Always.Set the value of imagePullPolicy to \"Always\". Set the value of image to nginx:1.8.Set the value of image to nginx:1.8. Set the value of the replicas field to 2.Set the value of the replicas field to 2.  Code snippet formatting Don\u0026rsquo;t include the command prompt  DoDon't kubectl get pods$ kubectl get pods  Separate commands from output Verify that the pod is running on your chosen node:\nkubectl get pods --output=wide  The output is similar to this:\nNAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0  Versioning Kubernetes examples Code examples and configuration examples that include version information should be consistent with the accompanying text. Identify the Kubernetes version in the Before you begin section.\nTo specify the Kubernetes version for a task or tutorial page, include min-kubernetes-server-version in the front matter of the page.\nIf the example YAML is in a standalone file, find and review the topics that include it as a reference. Verify that any topics using the standalone YAML have the appropriate version information defined. If a stand-alone YAML file is not referenced from any topics, consider deleting it instead of updating it.\nFor example, if you are writing a tutorial that is relevant to Kubernetes version 1.8, the front-matter of your markdown file should look something like:\n--- title: \u0026lt;your tutorial title here\u0026gt; min-kubernetes-server-version: v1.8 ---  In code and configuration examples, do not include comments about alternative versions. Be careful to not include incorrect statements in your examples as comments, such as:\napiVersion: v1 # earlier versions use... kind: Pod ...  "
},
{
	"uri": "https://gardener.cloud/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/trigger-shoot-operations/",
	"title": "Trigger Shoot operations",
	"tags": [],
	"description": "Trigger Shoot operations",
	"content": " Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nImmediate reconciliation Annotate the shoot with shoot.garden.sapcloud.io/operation=reconcile to make the gardener-controller-manager start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; shoot.garden.sapcloud.io/operation=reconcile  Immediate maintenance Annotate the shoot with shoot.garden.sapcloud.io/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; shoot.garden.sapcloud.io/operation=maintain  Retry failed operation Annotate the shoot with shoot.garden.sapcloud.io/operation=retry to make the gardener-controller-manager start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; shoot.garden.sapcloud.io/operation=retry  Rotate kubeconfig credentials Annotate the shoot with shoot.garden.sapcloud.io/operation=rotate-kubeconfig-credentials to make the gardener-controller-manager exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; shoot.garden.sapcloud.io/operation=rotate-kubeconfig-credentials  "
},
{
	"uri": "https://gardener.cloud/tutorials/",
	"title": "Tutorials",
	"tags": [],
	"description": "",
	"content": "Initial Consideration There is a big difference between installing Kubernetes and using Kubernetes as a developer     Administrator The admin section is for anyone setup or administering a Gardener Landscape. It assumes some familiarity with concepts of IaaS   Developer You don’t have to understand all the internals of Kubernetes; however, basic knowledge of the architecture is helpful for understanding how to deploy and debug your applications. In this section we offer best practices for service and application development on Kubernetes in the context of Gardener.      "
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/helm/",
	"title": "Use a Helm chart to deploy some application or service",
	"tags": [],
	"description": "Use a Helm chart to deploy some application or service",
	"content": " Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default hence Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: helm namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm namespace: kube-system EOF  Initialize Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \u0026quot;system:serviceaccount:kube-system:default\u0026quot; cannot list configmaps in the namespace \u0026quot;kube-system\u0026quot;. (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system kubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/  Now follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/prometheus/",
	"title": "Using Prometheus and Grafana to monitor K8s",
	"tags": [],
	"description": "How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics",
	"content": " Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments. Such advanced details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.\nPrometheus graduates within CNCF second hosted project.\nThe following characteristics make Prometheus a good match for monitoring Kubernetes clusters:\n Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.\n Labels Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.\nLabels are used to identify time series and sets of label matchers can be used in the query language ( PromQL ) to select the time series to be aggregated..\n Exporters\nThere are many exporters available which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n Powerful query language\nThe Prometheus query language PromQL lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the HTTP API.\n  Find query examples on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is a metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses data via Data Sources. The continuously growing list of supported backends includes Prometheus.\nDashboards are created by combining panels, e.g. Graph and Dashlist.\nIn this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.\nIf you miss elements on the Prometheus web page when accessing it via its service URL https://\u0026lt;your K8s FQN\u0026gt;/api/v1/namespaces/\u0026lt;your-prometheus-namespace\u0026gt;/services/prometheus-prometheus-server:80/proxy this is probably caused by Prometheus issue #1583 To workaround this issue setup a port forward kubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;prometheus-pod\u0026gt; 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana is based on Helm charts.\nMake sure to implement the Helm settings before deploying the Helm charts.\nThe Kubernetes clusters provided by Gardener use role based access control (RBAC). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster\u0026rsquo;s worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the garden.sapcloud.io:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\nContent of crbinding.yaml\napiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: \u0026lt;your-prometheus-name\u0026gt;-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: garden.sapcloud.io:monitoring:prometheus subjects: - kind: ServiceAccount name: \u0026lt;your-prometheus-name\u0026gt;-server namespace: \u0026lt;your-prometheus-namespace\u0026gt;  Deployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nCopy the following configuration into a file called values.yaml and deploy Prometheus: helm install --name \u0026lt;your-prometheus-name\u0026gt; --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nContent of values.yaml for Prometheus:\nrbac: create: false # Already created in Preparation step nodeExporter: enabled: false # The node-exporter is already deployed by default server: global: scrape_interval: 30s scrape_timeout: 30s serverFiles: prometheus.yml: rule_files: - /etc/config/rules - /etc/config/alerts scrape_configs: - job_name: 'kube-kubelet' honor_labels: false scheme: https tls_config: # This is needed because the kubelets' certificates are not generated # for a specific pod IP insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - target_label: __metrics_path__ replacement: /metrics - source_labels: [__meta_kubernetes_node_address_InternalIP] target_label: instance - action: labelmap regex: __meta_kubernetes_node_label_(.+) - job_name: 'kube-kubelet-cadvisor' honor_labels: false scheme: https tls_config: # This is needed because the kubelets' certificates are not generated # for a specific pod IP insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - target_label: __metrics_path__ replacement: /metrics/cadvisor - source_labels: [__meta_kubernetes_node_address_InternalIP] target_label: instance - action: labelmap regex: __meta_kubernetes_node_label_(.+) # Example scrape config for probing services via the Blackbox Exporter. # # Relabelling allows to configure the actual service scrape endpoint using the following annotations: # # * `prometheus.io/probe`: Only probe services that have a value of `true` - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name # Example scrape config for pods # # Relabelling allows to configure the actual service scrape endpoint using the following annotations: # # * `prometheus.io/scrape`: Only scrape pods that have a value of `true` # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`. - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: (.+):(?:\\d+);(\\d+) replacement: ${1}:${2} target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # Scrape config for service endpoints. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/scrape`: Only scrape services that have a value of `true` # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: If the metrics are exposed on a different port to the # service then set this appropriately. - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+)(?::\\d+);(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # Add your additional configuration here...  Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. Deploy Grafana via helm install --name grafana --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nContent of values.yaml for Grafana:\nserver: ingress: enabled: false service: type: ClusterIP  Check the running state of the pods on the Kubernetes Dashboard or by running kubectl get pods -n \u0026lt;your-prometheus-namespace\u0026gt;. In case of errors check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u0026lt;your-prometheus-namespace\u0026gt; and could be decoded via kubectl get secret --namespace \u0026lt;my-grafana-namespace\u0026gt; grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo.\nBasic functional tests To access the web UI of both applications use port forwarding of port 9090.\nSetup port forwarding for port 9090:\nkubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;your-prometheus-server-pod\u0026gt; 9090:9090  Open http://localhost:9090 in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))  This should show some data in a graph.\nTo show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser. Enter the credentials of the admin user.\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.\nRun\nhelm status \u0026lt;your-prometheus-name\u0026gt;  to find this name. Below this server name is referenced by \u0026lt;your-prometheus-server-name\u0026gt;.\nFirst, you need to add your Prometheus server as data source.\n select Dashboards \u0026rightarrow; Data Sources select Add data source enter Name: \u0026lt;your-prometheus-datasource-name\u0026gt;\nType: Prometheus\nURL: http://\u0026lt;your-prometheus-server-name\u0026gt;\n_Access: proxy\n select Save \u0026amp; Test  In case of failure check the Prometheus URL in the Kubernetes Dashboard.\nTo add a Graph follow these steps:\n in the left corner, select Dashboards \u0026rightarrow; New to create a new dashboard select Graph to create a new graph next, select the Panel Title \u0026rightarrow; Edit select your Prometheus Data Source in the drop down list enter the expression 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A select the floppy disk symbol (Save) on top  Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.\nAs a next step you can implement monitoring for your applications by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_08/",
	"title": "Watching logs of several pods",
	"tags": [],
	"description": "",
	"content": "One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment and you don\u0026rsquo;t have setup a log viewer service like Kibana.\nkubetail comes to the rescue, it is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at https://github.com/johanhaleby/kubetail.\n"
},
{
	"uri": "https://gardener.cloud/components/gardenctl/",
	"title": "gardenctl",
	"tags": [],
	"description": "",
	"content": " Gardenctl  \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nInstallation gardenctl is shipped for mac and linux in a binary format.\n Download the latest release: bash curl -LO https://github.com/gardener/gardenctl/releases/download/$(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST)/gardenctl-darwin-amd64   To download a specific version, replace the $(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST) portion of the command with the specific version.\nFor example, to download version 0.13.0 on macOS, type:\ncurl -LO https://github.com/gardener/gardenctl/releases/download/0.13.0/gardenctl-darwin-amd64   Make the gardenctl binary executable.\nchmod +x ./gardenctl-darwin-amd64  Move the binary in to your PATH.\nsudo mv ./gardenctl-darwin-amd64 /usr/local/bin/gardenctl   How to build it If no binary builds are available for your platform or architecture, you can build it from source, go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to clone the repository and build gardenctl.\ngit clone /030-architecture/15_gardenctl/ cd gardenctl make build  After successfully building gardenctl the executables are in the directory ~/go/src/github.com/gardener/gardenctl/bin/. Next, move the executable for your architecture to /usr/local/bin. In this case for darwin-amd64.\nsudo mv bin/darwin-amd64/gardenctl-darwin-amd64 /usr/local/bin/gardenctl  gardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\necho \u0026quot;source \u0026lt;(gardenctl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  Via Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\nemail: john.doe@example.com githubURL: https://github.location.company.corp gardenClusters: - name: dev kubeConfig: ~/clusters/dev/kubeconfig.yaml - name: prod kubeConfig: ~/clusters/prod/kubeconfig.yaml  The path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs. - aliyun - aws - az - gcloud - openstack\nMoreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion bash  print on the standard output a completion script which can be sourced via\nsource \u0026lt;(gardenctl completion bash)  Please keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots uses the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster\ngardenctl ls seeds List all projects with shoot cluster\ngardenctl ls projects Target a seed cluster\ngardenctl target seed-gce-dev Target a project\ngardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster\ngardenctl show prometheus Execute an aws command on a targeted aws shoot cluster\ngardenctl aws ec2 describe-instances or\ngardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace\ngardenctl target myshoot\ngardenctl kubectl get pods -- -n kube-system -l k8s-app=kube-dns List all cluster with an issue\ngardenctl ls issues Drop an element from target stack\ngardenctl drop Open a shell to a cluster node\ngardenctl shell nodename Show logs from elasticsearch\ngardenctl logs etcd-main --elasticsearch Show last 100 logs from elasticsearch from the last 2 hours\ngardenctl logs etcd-main --elasticsearch --since=2h --tail=100  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues bash gardenctl ls issues -o json | jq '.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }'  Print all issues of a single project e.g. garden-myproject bash gardenctl ls issues -o json | jq '.issues[] | if (.project==\u0026quot;garden-myproject\u0026quot;) then . else empty end'  Print all issues with error state \u0026ldquo;Error\u0026rdquo; bash gardenctl ls issues -o json | jq '.issues[] | if (.status.lastOperation.state==\u0026quot;Error\u0026quot;) then . else empty end'  Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo; bash gardenctl ls issues -o json | jq '.issues[] | if (.status.lastOperation.state!=\u0026quot;Succeeded\u0026quot;) then . else empty end'  Print createdBy information (typically email addresses) of all shoots bash gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026quot;.items[].metadata | {email: .annotations.\\\u0026quot;garden.sapcloud.io/createdBy\\\u0026quot;, name: .name, namespace: .namespace}\u0026quot;   Here a few on cluster analysis:\n Which states are there and how many clusters are in this state?\ngardenctl ls issues -o json | jq '.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}'  Get all clusters in state Failed\ngardenctl ls issues -o json | jq '.issues[] | if (.status.lastOperation.state==\u0026quot;Failed\u0026quot;) then . else empty end'   "
},
{
	"uri": "https://gardener.cloud/components/kubify/",
	"title": "kubify",
	"tags": [],
	"description": "",
	"content": " Kubify Kubify is a Terraform based provisioning project for setting up production ready Kubernetes clusters on public and private Cloud infrastructures. Kubify currently supports:\n OpenStack AWS Azure  Key features of Kubify are:\n Kubernetes v1.10.12 Etcd v3.3.10 multi master node setup Etcd backup and restore Supports rolling updates  To start using or developing Kubify locally See our documentation in the /docs repository or find the main documentation here.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Kubify itself as GitHub issues or join our Slack channel #gardener (Invite yourself to the Kubernetes Slack workspace here).\n"
},
{
	"uri": "https://gardener.cloud/050-tutorials/content/howto/tail-logfile/",
	"title": "tail -f /var/log/my-application.log",
	"tags": [],
	"description": "Aggregate log files from different pods",
	"content": " Problem One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don\u0026rsquo;t have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at https://github.com/johanhaleby/kubetail.\n"
}]