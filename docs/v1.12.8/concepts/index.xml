<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Concepts on Gardener</title><link>https://gardener.cloud/v1.12.8/concepts/</link><description>Recent content in Concepts on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/v1.12.8/concepts/index.xml" rel="self" type="application/rss+xml"/><item><title>Backup and Restore</title><link>https://gardener.cloud/v1.12.8/concepts/backup-restore/backup-restore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/backup-restore/backup-restore/</guid><description>Backup and restore Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.
Gardener uses etcd-backup-restore component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via etcd-druid. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components.</description></item><item><title>BackupBucket resource</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/backupbucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/backupbucket/</guid><description>Contract: BackupBucket resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The BackupBucket resource takes this responsibility in Gardener.
Before introducing the BackupBucket extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here).</description></item><item><title>BackupEntry resource</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/backupentry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/backupentry/</guid><description>Contract: BackupEntry resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The BackupEntry resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component. Said that, the core motivation for introducing this resource was to support retention of backups post deletion of Shoot.</description></item><item><title>ControlPlane resource</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/controlplane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/controlplane/</guid><description>Contract: ControlPlane resource Most Kubernetes clusters require a cloud-controller-manager or CSI drivers in order to work properly. Before introducing the ControlPlane extension resource Gardener was having several different Helm charts for the cloud-controller-manager deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.
Which control plane resources are required? As mentioned in the controlplane customization webhooks document Gardener shall not deploy any cloud-controller-manager or any other provider-specific component.</description></item><item><title>ControlPlane resource with purpose exposure</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/controlplane-exposure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/controlplane-exposure/</guid><description>Contract: ControlPlane resource with purpose exposure Some Kubernetes clusters require an additional deployments required by the seed cloud provider in order to work properly, e.g. AWS Load Balancer Readvertiser. Before using ControlPlane resources with purpose exposure Gardener was having different Helm charts for the deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.
Which control plane resources are required? As mentioned in the controlplane document Gardener shall not deploy any other provider-specific component.</description></item><item><title>Deploy a Gardenlet Manually</title><link>https://gardener.cloud/v1.12.8/concepts/deployment/deploy_gardenlet_manually/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/deployment/deploy_gardenlet_manually/</guid><description>Deploy a Gardenlet Manually Manually deploying a gardenlet is required in the following cases:
The Kubernetes cluster to be registered as a seed cluster has no public endpoint, because it is behind a firewall. The gardenlet must then be deployed into the cluster itself.
The Kubernetes cluster to be registered as a seed cluster is managed externally (the Kubernetes cluster is not a shoot cluster, so Automatic Deployment of Gardenlets cannot be used).</description></item><item><title>Deploying Gardenlets</title><link>https://gardener.cloud/v1.12.8/concepts/deployment/deploy_gardenlet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/deployment/deploy_gardenlet/</guid><description>Deploying Gardenlets Gardenlets act as decentral &amp;ldquo;agents&amp;rdquo; to manage shoot clusters of a seed cluster.
To support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.</description></item><item><title>Deploying the Gardener into a Kubernetes cluster</title><link>https://gardener.cloud/v1.12.8/concepts/deployment/setup_gardener/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/deployment/setup_gardener/</guid><description>Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.</description></item><item><title>Deploying the previous Gardener versions and a Seed into an AKS cluster</title><link>https://gardener.cloud/v1.12.8/concepts/deployment/aks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/deployment/aks/</guid><description>Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We&amp;rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .
Please note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.</description></item><item><title>DNSProvider and DNSEntry resources</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/dns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/dns/</guid><description>Contract: DNSProvider and DNSEntry resources Every shoot cluster requires external DNS records that are publicly resolvable. The management of these DNS records requires provider-specific knowledge which is to be developed outside of the Gardener&amp;rsquo;s core repository.
What does Gardener create DNS records for? Internal domain name Every shoot cluster&amp;rsquo;s kube-apiserver running in the seed is exposed via a load balancer that has a public endpoint (IP or hostname). This endpoint is used by end-users and also by system components (that are running in another network, e.</description></item><item><title>Extension resource</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/extension/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/extension/</guid><description>Contract: Extension resource Gardener defines common procedures which must be passed to create a functioning shoot cluster. Well known steps are represented by special resources like Infrastructure, OperatingSystemConfig or DNS. These resources are typically reconciled by dedicated controllers setting up the infrastructure on the hyperscaler or managing DNS entries, etc..
But, some requirements don&amp;rsquo;t match with those special resources or don&amp;rsquo;t depend on being proceeded at a specific step in the creation / deletion flow of the shoot.</description></item><item><title>General conventions</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/conventions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/conventions/</guid><description>General conventions All the extensions that are registered to Gardener are deployed to the seed clusters (at the moment, every extension is installed to every seed cluster, however, in the future Gardener will be more smart to determine which extensions needs to be placed into which seed).
Some of these extensions might need to create global resources in the seed (e.g., ClusterRoles), i.e., it&amp;rsquo;s important to have a naming scheme to avoid conflicts as it cannot be checked or validated upfront that two extensions don&amp;rsquo;t use the same names.</description></item><item><title>Health Check Library</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/healthcheck-library/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/healthcheck-library/</guid><description>Health Check Library Goal Typically an extension reconciles a specific resource (Custom Resource Definitions (CRDs)) and creates/modifies resources in the cluster (via helm, managed resources, kubectl, &amp;hellip;). We call these API Objects &amp;lsquo;dependent objects&amp;rsquo; - as they are bound to the lifecycle of the extension.
The goal of this library is to enable extensions to setup health checks for their &amp;lsquo;dependent objects&amp;rsquo; with minimal effort.
Usage The library provides a generic controller with the ability to register any resource that satisfies the extension object interface.</description></item><item><title>Image Vector</title><link>https://gardener.cloud/v1.12.8/concepts/deployment/image_vector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/deployment/image_vector/</guid><description>Image Vector The Gardenlet is deploying several different container images into the seed and the shoot clusters. The image repositories and tags are defined in a central image vector file. Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).
Example images:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:&amp;#34;3.0&amp;#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:&amp;#34;3.1&amp;#34;version:&amp;#34;&amp;gt;= 1.12&amp;#34;...That means that the Gardenlet will use the pause-container in with tag 3.0 for all seed/shoot clusters with Kubernetes version 1.</description></item><item><title>Infrastructure resource</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/infrastructure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/infrastructure/</guid><description>Contract: Infrastructure resource Every Kubernetes cluster requires some low-level infrastructure to be setup in order to work properly. Examples for that are networks, routing entries, security groups, IAM roles, etc. Before introducing the Infrastructure extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task.
Which infrastructure resources are required? Unfortunately, there is no general answer to this question as it is highly provider specific.</description></item><item><title>Logging and Monitoring for Extensions</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/logging-and-monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/logging-and-monitoring/</guid><description>Logging and Monitoring for Extensions Gardener provides an integrated logging and monitoring stack for alerting, monitoring and troubleshooting of its managed components by operators or end users. For further information how to make use of it in these roles, refer to the corresponding guides for exploring logs and for monitoring with Grafana.
The components that constitute the logging and monitoring stack are managed by Gardener. By default, it deploys Prometheus, Alertmanager and Grafana into the garden namespace of all seed clusters.</description></item><item><title>Machine Controller Manager</title><link>https://gardener.cloud/v1.12.8/concepts/mcm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/mcm/</guid><description>machine-controller-manager
:warning: We are in the progress of migrating and deprecating all the in-tree providers to OOT. Please avoid making any new feature enhancements to the intree providers. Kindly make it on the OOT providers available here. More details on adding new OOT providers can be found here.
Machine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs.
MCM supports following providers:</description></item><item><title>Registering Extension Controllers</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/controllerregistration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/controllerregistration/</guid><description>Registering Extension Controllers Extensions are registered in the garden cluster via ControllerRegistration resources. Gardener is evaluating the registrations and creates ControllerInstallation resources which describe the request &amp;ldquo;please install this controller X to this seed Y&amp;rdquo;.
Similar to how CloudProfile or Seed resources get into the system, the Gardener administrator must deploy the ControllerRegistration resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).</description></item><item><title>Shoot maintenance</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/shoot-maintenance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/shoot-maintenance/</guid><description>Shoot maintenance There is a general document about shoot maintenance that you might want to read. Here, we describe how you can influence certain operations that happen during a shoot maintenance.
Restart Control Plane Controllers As outlined in above linked document, Gardener offers to restart certain control plane controllers running in the seed during a shoot maintenance.
Extension controllers can extend the amount of pods being affected by these restarts. If your Gardener extension manages pods of a shoot&amp;rsquo;s control plane (shoot namespace in seed) and it could potentially profit from a regular restart please consider labeling it with maintenance.</description></item><item><title>Shoot resource customization webhooks</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/shoot-webhooks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/shoot-webhooks/</guid><description>Shoot resource customization webhooks Gardener deploys several components/resources into the shoot cluster. Some of these resources are essential (like the kube-proxy), others are optional addons (like the kubernetes-dashboard or the nginx-ingress-controller). In either case, some provider extensions might need to mutate these resources and inject provider-specific bits into it.
What&amp;rsquo;s the approach to implement such mutations? Similar to how control plane components in the seed are modified we are using MutatingWebhookConfigurations to achieve the same for resources in the shoot.</description></item><item><title>Worker resource</title><link>https://gardener.cloud/v1.12.8/concepts/extensions/worker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/v1.12.8/concepts/extensions/worker/</guid><description>Contract: Worker resource While the control plane of a shoot cluster is living in the seed and deployed as native Kubernetes workload, the worker nodes of the shoot clusters are normal virtual machines (VMs) in the end-users infrastructure account. The Gardener project features a sub-project called machine-controller-manager. This controller is extending the Kubernetes API using custom resource definitions to represent actual VMs as Machine objects inside a Kubernetes system. This approach unlocks the possibility to manage virtual machines in the Kubernetes style and benefit from all its design principles.</description></item></channel></rss>